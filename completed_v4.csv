,Unnamed: 0,0,1,topics
0,0,artificial intelligence ai has the potential to change the way we live and work,0,4
1,1,embedding ai across all sectors has the potential to create thousands of jobs and drive economic growth,0,0
2,2,by one estimate ais contribution to the united kingdom could be as large as  of gdp by,0,2
3,3,a number of public sector organisations are already successfully using ai for tasks ranging from fraud detection to answering customer queries,0,0
4,4,the potential uses for ai in the public sector are significant but have to be balanced with ethical fairness and safety considerations,0,16
5,5,national grid has turned to ai to help it maintain the wires and pylons that transmit electricity from power stations to homes and businesses across the uk,0,4
6,6,the firm has been using six drones for the past two years to help inspect its  miles of overhead lines around england and wales,0,25
7,7,equipped with high res still video and infrared cameras the drones are deployed to assess the steelwork wear and corrosion and faults such as damaged conductors,0,25
8,8,ai and drones turn an eye towards uks energy infrastructure,0,25
9,9,the government has set up two funds to support the development and uptake of ai systems,0,2
10,10,govtech catalyst to help public sector bodies take advantage of emerging technologies,0,17
11,11,regulators pioneer fund to help regulators promote cuttingedge regulatory practices when developing emerging technologies,0,8
12,12,recognising ais potential the governments industrial strategy white paper placed ai and data as one of four grand challenges supported by up to m in the ai sector deal,0,2
13,13,the government has set up three new bodies to support the use of ai build the right infrastructure and facilitate public and private sector adoption of these technologies,0,2
14,14,these three new bodies are the ai council an expert committee of independent members providing highlevel leadership on implementing the ai sector deal,0,19
15,15,office for ai which works with industry academia and the third sector to coordinate and oversee the implementation of the uks ai strategy,0,2
16,16,centre for data ethics and innovation which identifies the measures needed to make sure the development of ai is safe ethical and innovative,0,16
17,17,at its core ai is a research field spanning philosophy logic statistics computer science mathematics neuroscience linguistics cognitive psychology and economics,0,4
18,18,ai can be defined as the use of digital technology to create systems capable of performing tasks commonly thought to require intelligence,0,4
19,19,ai is constantly evolving but generally it involves machines using statistics to find patterns in large amounts of data,0,4
20,20,is the ability to perform repetitive tasks with data without the need for constant human guidance,0,9
21,21,there are many new concepts used in the field of ai and you may find it useful to refer to a glossary of ai terms,0,4
22,22,this guidance mostly discusses machine learning,0,13
23,23,machine learning is a subset of ai and refers to the development of digital systems that improve their performance on a given task over time through experience,0,13
24,24,machine learning is the most widely used form of ai and has contributed to innovations like selfdriving cars speech recognition and machine translation,0,13
25,25,recent advances in machine learning are the result of improvements to algorithms increases in funding huge growth in the amount of data created and stored by digital systems increased access to computational power and the expansion of cloud computing,0,13
26,26,machine learning can be supervised learning which allows an ai model to learn from labelled training data for example training a model to help tag content on gov uk,0,13
27,27,unsupervised learning which is training an ai algorithm to use unlabelled and unclassified information,0,13
28,28,reinforcement learning which allows an ai model to learn as it performs a task,0,13
29,29,each year  testers conduct  million mot tests in  garages across great britain,0,25
30,30,the driver and vehicle standards agency dvsa developed an approach that applies a clustering model to analyse vast amount of testing data which it then combines with day to day operations to develop a continually evolving risk score for garages and their testers,0,3
31,31,from this the dvsa is able to direct its enforcement officers attention to garages or mot testers who may be either underperforming or committing fraud,0,12
32,32,by identifying areas of concern in advance the examiners preparation time for enforcement visits has fallen by,0,12
33,33,the department for international development partnered with the university of southampton columbia university and the united nations population fund to apply a random forest machine learning algorithm to satellite image and micro census data,0,13
34,34,the algorithm then used this information to predict the population density of an area,0,13
35,35,the model also used data from micro censuses to validate its outputs and provide valuable training data for the model,0,13
36,36,ai can benefit the public sector in a number of ways,0,17
37,37,for example it can provide more accurate information forecasts and predictions leading to better outcomes for example more accurate medical diagnoses,0,13
38,38,produce a positive social impact by using ai to provide solutions for some of the worlds most challenging social problems,0,4
39,39,simulate complex systems that allow policy makers to experiment with different policy options and spot unintended consequences before committing to a measure,0,17
40,40,improve public services for example providing service providers moreaccurate and detailed information of citizens with similar needs or interests to provide personalised public services tailored to individual circumstances,0,17
41,41,automate repetitive and time consuming tasks which frees up valuable time of frontline staff,0,22
42,42,ai is not a general purpose solution which can solve every problem,0,4
43,43,current applications of ai focus on performing narrowly defined tasks,0,4
44,44,ai generally cannot be imaginative perform well without a large quantity of relevant high quality data infer additional context if the information is not present in the data,0,4
45,45,even if ai can help you meet some user needs simpler solutions may be more effective and less expensive,0,4
46,46,for example optical character recognition technology can extract information from scans of passports,0,12
47,47,however a digital form requiring manual input might be more accurate quicker to build and cheaper,0,25
48,48,youll need to investigate alternative mature technology solutions thoroughly to check if this is the case,0,25
49,49,in supervised learning the objective is to make predictions using a set of data,0,13
50,50,to do this the ai model is trained against a dataset a training set a subset to train the model and a test set a subset to test the trained model,0,13
51,51,the data has been tagged with one or more labels,0,13
52,52,n unsupervised learning the objective is to make predictions using data where there are no labels for example pictures,0,13
53,53,often this involves looking for patterns in the dataset and grouping related data points together a common example of grouping data is clustering,0,13
54,54,in reinforcement learning the objective is to make predictions which accomplish a specific goal,0,13
55,55,the ai model uses a trial and error approach when making its decisions starting from totally random trials and finishing with sophisticated tactics,0,21
56,56,a familiar example is chess where the goal of the ai model is to checkmate the opponent after having taught itself how to play,0,4
57,57,with an ai project you should consider a number of factors including ai ethics and safety,0,16
58,58,these factors span safety ethical legal and administrative concerns and include but are not limited to data quality the success of your ai project depends on the quality of your data,0,3
59,59,fairness are the models trained and tested on relevant accurate and generalisable datasets and is the ai system deployed by users trained to implement them responsibly and without bias,0,6
60,60,accountability consider who is responsible for each element of the models output and how the designers and implementers of ai systems will be held accountable,0,16
61,61,privacy complying with appropriate data policies for example the general data protection regulations gdpr and the data protection act  explainability and transparency so the affected stakeholders can know how the ai model reached its decision,0,11
62,62,costs consider how much it will cost to build run and maintain an ai infrastructure train and educate staff and if the work to install ai may outweigh any potential savings,0,4
63,63,youll need to make sure your ai system is compliant with gdpr and the data protection act  dpa  including the points which relate to automated decision making,0,21
64,64,we recommend discussing this with legal advisors,0,10
65,65,automated decisions in this context are decisions made without human intervention which have legal or similarly significant effects on data subjects,0,21
66,66,for example an online decision to award a business grant,0,21
67,67,if you want to use automated processes to make decisions with legal or similarly significant effects on individuals you must follow the safeguards laid out in the gdpr and dpa,0,21
68,68,this includes making sure you provide users with specific and easily accessible information about the automated decision making process,0,21
69,69,a simple way to obtain human intervention to review and potentially change the decision,0,21
70,70,remember to make sure your use of automated decision making does not conflict with any other laws or regulations,0,21
71,71,you should consider both the final decision and any automated decisions which significantly affected the decision making process,0,21
72,72,read the working party guidance on automated individual decision making and profiling for more information,0,21
73,73,ai is just another technology tool to help deliver services,0,4
74,74,designing any service starts with identifying user needs,0,27
75,75,if you think ai may be an appropriate technology choice to help you meet user needs you will need to consider your data and the specific technology you want to use,0,21
76,76,your data scientists will then use your data to build and train an ai model,0,13
77,77,when assessing if ai could help you meet users needs consider if theres data containing the information you need even if disguised or buried,0,9
78,78,consider if its ethical and safe to use the data refer to the data ethics framework,0,9
79,79,consider if you have a the right sort of data for the ai model to learn from,0,21
80,80,consider if the task is large scale and repetitive enough that a human would struggle to carry it out,0,4
81,81,consider if it would provide information a team could use to achieve outcomes in the real world,0,0
82,82,its important to remember that ai is not an all purpose solution,0,4
83,83,unlike a human ai cannot infer and can only produce an output based on the data a team inputs to the model,0,4
84,84,when identifying whether ai is the right solution its important that you work with specialists who have a good knowledge of your data and the problem youre trying to solve such as data scientists,0,4
85,85,at least one domain knowledge expert who knows the environment where you will be deploying the ai model results,0,20
86,86,because of its experimental and iterative nature it can be difficult to specify the precise benefits which could come from an ai project,0,4
87,87,to explore this uncertainty and provide the right level of information around the potential benefits you can carry out some initial analysis on your data to help you understand how hard the problem is and how likely the projects success would be,0,0
88,88,build your business case around a small scale proof of concept poc and use its results to prove your hypothesis,0,0
89,89,once you have secured budget youll need to allow enough time and resources to conduct a substantial discovery to show feasibility,0,25
90,90,discovery for projects using ai can often takes longer for similar projects that do not use ai,0,4
91,91,if your organisation is a central government department you may have to get approval from the gds to spend money on ai,0,25
92,92,at this point most ai projects are classified as novel which requires a high level of scrutiny,0,18
93,93,you should contact the gds standards assurance team for help on the spend controls process,0,22
94,94,or your ai model to work it often needs access to a large quantity of data and more importantly the right kind of data,0,4
95,95,work with specialists who have the knowledge of your data such as data scientists to assess your data state,0,24
96,96,you can assess whether your data is high enough quality for ai using a combination of accuracy completeness uniqueness timeliness validity sufficiency relevancy representativeness consistency,0,18
97,97,if your problem involves supporting an ongoing business decision process you will need to plan to establish ongoing up to date access to data,0,9
98,98,remember to follow data protection laws,0,11
99,99,when assessing if ai could help you meet user needs consider how you will procure the technology,0,4
100,100,you should define your purchasing strategy in the same way as you would for any other technology,0,0
101,101,whether you build buy or reuse or combine these approaches will depend on a number of considerations including whether the needs youre trying to meet are unique to your organisation or you could fulfil users needs with generic components,0,20
102,102,the maturity of commercially available products that meet those needs,0,0
103,103,how your product needs to integrate with your existing infrastructure,0,0
104,104,it is also important to address ethical concerns about the use of ai from the start of the procurement process,0,16
105,105,your team can build or adapt offthe shelf ai models or open source algorithms in house,0,25
106,106,when making this decision you should work with data scientists to consider whether your team has the skills to build an ai project in house,0,25
107,107,your operations team can run and maintain an in house ai solution,0,25
108,108,you may be able to buy your ai technology as an off the shelf product,0,25
109,109,this is most suitable if you are looking for a common application of ai for example optical character recognition,0,4
110,110,however buying your ai technology may not always be suitable as the specifics of your data and needs could mean the supplier would have to build from scratch or significantly customise an existing ai model,0,25
111,111,your ai solution will still need to be integrated into an end to end service for your users even if you are able to buy significant components off the shelf,0,25
112,112,there is no one ai technology,0,25
113,113,currently widely available ai technologies are mostly either supervised unsupervised or reinforcement machine learning,0,13
114,114,the machine learning techniques that can provide you with the best insight depends on the problem youre trying to solve,0,13
115,115,machine learning techniqueclassification regression clustering dimensionality reduction or manifold learning ranking,0,13
116,116,learns the characteristics of a given category allowing the ai model to classify unknown data points into existing categories,0,13
117,117,predicts a value for an unknown data point,0,13
118,118,identifies groups of similar data points in a datasetnarrows down the data to the most relevant variables to make models more accurate or make it possible to visualise the data,0,13
119,119,trains an ai model to rank new data based on previously seen lists,0,22
120,120,deciding if a consignment of goods undergoes border inspection,0,25
121,121,deciding if an email is spam or not,0,21
122,122,predicting the market value of a house from information such as its size location or age,0,24
123,123,forecasting the concentrations of air pollutants in cities,0,24
124,124,grouping retail customers to find subgroups with specific spending habits,0,24
125,125,clustering smart meter data to identify groups of electrical appliances and generate itemised electricity bills,0,24
126,126,used by data scientists when evaluating and developing other types of machine learning algorithms,0,13
127,127,returning pages by order of relevance when a user searches a website,0,22
128,128,there are certain types of problems for which machine learning is commonly used,0,13
129,129,for some of these you will be able to buy or adapt commercially available products,0,25
130,130,machine learning application natural language processing nlp computer vision anomaly detection time series analysis recommender systems,0,13
131,131,processes and analyses natural language recognising words their meaning context and the narrative,0,14
132,132,the ability of a machine or program to emulate human vision,0,12
133,133,finds anomalous data points within a dataset,0,24
134,134,understanding how data varies over time to conduct forecasting and monitoring,0,22
135,135,predicts how a user will rate a given item to make new recommendations,0,21
136,136,converting speech into text for automatic subtitles generation  automatically generating a reply to a customers email,0,0
137,137,identification of road signs for self driving vehicles,0,12
138,138,face recognition for automated passport controls,0,12
139,139,identifying fraudulent activity in a users bank account,0,0
140,140,conducting budget analyses  forecasting economic indicators,0,24
141,141,suggesting relevant pages on a website given the articles a user has previously viewed,0,24
142,142,when using ai its important to understand who is responsible if the system fails as the problem may lie in a number of areas,0,4
143,143,for example failures with the data chosen to train the ai model design of the model coding of the software or deployment,0,20
144,144,allocating responsibility and governance for ai projects,0,16
145,145,you should establish a responsibility record which sets out who is responsible for different areas of the ai system,0,16
146,146,it would be useful to consider whether the models are achieving their purpose and business objectives,0,20
147,147,there is a clear accountability framework for models in production,0,16
148,148,there is a clear testing and monitoring framework in place,0,18
149,149,your team has reviewed and validated the code,0,18
150,150,the algorithms are robust unbiased fair and explainable,0,13
151,151,the project fits with how citizens and users expect their data to be used,0,9
152,152,depending on your organisations maturity it may be useful to set up a dedicated board committee or forum to handle ai training data and model governance,0,16
153,153,it can be useful to keep a central record of all ai technologies you use listing where an ai model is in use what the ai model is used for whos involved how its assessed or checked what other teams rely on the technology,0,4
154,154,national grid and the alan turing institute improve solar forecastingthe national grid electricity system operator eso balances the electricity system in real time ensuring the nations supply always meets demand,0,22
155,155,this balancing act becomes more challenging as wind and solar power become a larger part of the overall energy mix as their generation output is hard to predict,0,22
156,156,an innovation project between eso and the alan turing institute used a mix of machine learning prediction methods and computational statistics to achieve a big improvement in forecast accuracy,0,13
157,157,one result found the solar forecasting system  more accurate at dayahead forecasts,0,22
158,158,improved foresting helps eso run the grid more efficiently which ultimately means lower bills for households,0,22
159,159,planning and preparing for ai systems implementation,0,5
160,160,as with all projects you need to make sure youre hypothesis led and can constantly iterate to best help your users and their needs,0,25
161,161,you should integrate your ai systems development with your wider project phases,0,25
162,162,discovery consider your current data state decide whether to build buy or collaborate allocate responsibility for ai models assess your existing data build your ai team get your data ready for ai and plan your ai modelling phase,0,20
163,163,alpha build and evaluate your machine learning model,0,20
164,164,beta deploy and maintain your model,0,20
165,165,you should consider ai ethics and safety throughout all phases,0,16
166,166,significant time is needed to understand the feasibility of using your data in a new way,0,18
167,167,this means the discovery phase tends to be longer and more expensive than for services without ai,0,4
168,168,your data scientists may be familiar with a lifecycle called crisp dm and may wish to integrate parts of it into your project,0,24
169,169,discovery can help you understand the problem that needs to be solved,0,14
170,170,assess your user needs and data sources,0,24
171,171,thoroughly understand the problem and the needs of different users,0,14
172,172,assess whether an ai system is the right tool to address the user needs,0,4
173,173,understand the processes and how the ai model will connect with the wider service,0,20
174,174,consider the location and condition of the data you will use,0,24
175,175,assess your existing data,0,24
176,176,to prepare for your ai project you should assess your existing data,0,18
177,177,training an ai system on errorstrewn data can result in poor results due to the dataset not containing clear patterns for the model to explore when making a prediction,0,13
178,178,the dataset containing clear but accidental patterns resulting in the model learning biases,0,13
179,179,you can use a combination of accuracy completeness uniqueness timeliness validity relevancy representativeness sufficiency or consistency to see if the data is high enough quality for an ai system to make predictions from,0,18
180,180,when assessing your ai data its useful to collaborate with someone who has deep knowledge of your data such as a data scientist,0,18
181,181,they will be familiar with the best practice for measuring cleaning and maintaining good data standards for ongoing projects,0,18
182,182,make your data proportionate to user needs and understand the limitations of the data to help you assess your data readiness,0,24
183,183,questions for you to consider with data scientists are do you have enough data for the model to learn from,0,20
184,184,do you understand the onward effects of using data in this way,0,9
185,185,is the data accurate and complete and how frequently is the data updated,0,18
186,186,is the data representative of the users the models results will impact,0,20
187,187,was the data gathered using suitable reliable and impartial sources of measurement,0,18
188,188,is the data secure and do you have permission to use it,0,9
189,189,what modelling approaches could be suitable for the data available,0,24
190,190,do you have access to the data and how quickly can you access it,0,22
191,191,where is the data located,0,24
192,192,what format is the data in and does it require significant preparation to be ready for modelling,0,24
193,193,is your data structured for example can you store it in a table or unstructured such as emails or webpages,0,24
194,194,are there any constraints on the data for example does it contain sensitive information such as home addresses,0,9
195,195,can you link key variables within and between datasets,0,24
196,196,if youre unsure about your use of data consult the data ethics framework guidance to check your project is a safe application and deployment of ai models,0,9
197,197,build your team for ai implementation,0,16
198,198,as with other projects your team should be multidisciplinary with a diverse combination of roles and skills to reduce bias and make sure your results are as accurate as possible,0,16
199,199,when working with ai you may need specialist roles such as a data architect to set the vision for the organisations use of data through data design to meet business needs,0,4
200,200,data scientist to identify complex business problems while leveraging data value often having at least two data scientists working on a project allows them to better collaborate and validate ai experiments,0,24
201,201,data engineer to develop the delivery of data products and services into systems and business processes,0,23
202,202,ethicist to provide ethical judgements and assessments on the ai models inputs,0,16
203,203,domain expert who knows the environment where you will be deploying the ai model results for example if the model will be investigating social care collaborate with a social worker,0,20
204,204,you may not need all of these roles from the very beginning but this may change as the work progresses,0,27
205,205,you may want to break up your discovery into smaller phases so you can evaluate what you are learning,0,13
206,206,experience of solving an ai problem similar to the one youre solving,0,4
207,207,commercial experience of ai understanding of machine learning techniques and algorithms including production deployments at scale,0,13
208,208,an understanding of cloud architecture security scalable deployment and open source tools and technologies,0,20
209,209,hands on experience of major cloud platforms,0,7
210,210,experience with containers and container orchestrations for example docker and kubernetes,0,24
211,211,experience in or strong understanding of the fundamentals of computer science and statistics,0,24
212,212,experience in software development for example python r or scala,0,24
213,213,experience building large scale backend systems,0,24
214,214,hands on experience with a cluster computing framework for example hadoop or spark,0,24
215,215,hands on experience with data stores for example sql and no sql,0,24
216,216,technical understanding of streaming data architectures,0,11
217,217,experience of working to minimise bias from data,0,6
218,218,managing infrastructure and suppliers,0,24
219,219,when preparing for ai implementation you should identify how you can best integrate ai with your existing technology and services,0,4
220,220,its useful to consider how youll manage data collection pipelines to support reliable model performance and a clean input for modelling such as batch upload or continuous upload,0,20
221,221,storing your data in databases and how the type of database you choose will change depending on the complexity of the project and the different data sources required,0,24
222,222,data mining and data analysis of the results,0,24
223,223,any platforms your team will use to collate the technology used across the ai project to help speed up ai deployment,0,25
224,224,when choosing your ai tools you should bring in specialists such as data scientists or technical architects to assess what tools you currently have to support ai,0,4
225,225,use cloud first when setting up your infrastructure,0,25
226,226,consider the benefits of ai platforms,0,4
227,227,a data science platform is a type of software tool which helps teams connect all of the technology they require across their project workflow speeding up ai deployment and increasing the transparency and oversight over ai models,0,26
228,228,when deciding on whether to use a data science platform its useful to consider how the platform can provide access to flexible computation which allows teams to have secure access to the power needed to process large amounts of data,0,11
229,229,help your team build workflows for accessing and preparing datasets and allow for easy maintenance of the data,0,24
230,230,provide common environments for sharing data and code so the team can work collaboratively,0,25
231,231,let your teams clearly share their output through dashboards and applications,0,1
232,232,provide a reproducible environment for your teams to work from,0,25
233,233,help control and monitor project specific or sensitive permissions,0,25
234,234,preparing your data for an ai model,0,20
235,235,after youve assessed your current data quality you should prepare your data to make sure it is secure and unbiased,0,18
236,236,you may find it useful to create a data factsheet during discovery to keep a record of your data quality,0,24
237,237,ensuring diversity in your data,0,24
238,238,in the same way you should have diversity in your team your data should also be diverse and reflective of the population you are trying to model,0,6
239,239,this will reduce conscious or unconscious bias,0,6
240,240,alongside this a lack of diverse input could mean certain groups are disadvantaged as the ai model may not cater for a diverse set of needs,0,6
241,241,you should read the data ethics framework guidance to understand the limitations of your data and how to recognise any bias present,0,6
242,242,you should also evaluate the accuracy of your data how it was collected and consider alternative sources,0,24
243,243,consider the social context of where when and how the system is being deployed,0,15
244,244,consider if any particular groups might be at an advantage or disadvantage in the context in which the system is being deployed,0,6
245,245,keeping your data secure,0,11
246,246,make sure you design your system to keep data secure,0,11
247,247,to help keep data safe,0,11
248,248,follow the national cyber security centres guidance wwwncscgovuk on using data with ai models,0,13
249,249,make sure your system is compliant with gdpr and dpa,0,2
250,250,as with any other software you should design and build modular loosely coupled systems which can be easily iterated and adapted,0,25
251,251,writing and training algorithms can take a lot of time and computational power,0,13
252,252,in addition to ongoing cost youll need to think about the network and memory resources your team will need to train your model,0,20
253,253,using historic data,0,24
254,254,most of the data in government available to train our models is within legacy systems which might contain bias and might have poor controls around it,0,8
255,255,for legacy systems to be compatible with ai technology you will often need to invest a lot of work to bring your legacy systems up to modern standards,0,25
256,256,youll also need to carefully consider the ethical and legal implications of working with historic data and whether you need to seek permission to use this information,0,24
257,257,evaluate your data preparation phase,0,24
258,258,when you complete your data preparation phase you should have a dataset ready for modelling in a technical environment,0,24
259,259,a set of features measurable properties generated from the raw dataset,0,13
260,260,a data quality assessment using a combination of accuracy bias completeness uniqueness timelinesscurrency validity or consistency,0,18
261,261,researching the end to end service,0,24
262,262,during the discovery phase you should explore the needs of the users of the end to end service,0,0
263,263,like other digital services youll use this phase to determine whether theres a viable service you could build that would solve user needs and that its cost effective to pursue the problem,0,23
264,264,youll be able to check guidance on how to know when your discovery is finished before moving on to alpha,0,22
265,265,moving to the alpha phase,0,22
266,266,plan and prototype your ai model build and service,0,20
267,267,if you have decided to build your ai model in house you should follow these steps split the data create a baseline model build a prototype of the model and service,0,20
268,268,test the model and service,0,20
269,269,evaluate the model,0,20
270,270,assess and refine performance,0,18
271,271,split the data,0,24
272,272,your team will need to train the models they build on data,0,20
273,273,your team should split your data into a training set to train algorithms during the modelling phase validation set for assessing the performance of your models test set for a final check on the performance of your best model,0,20
274,274,create a baseline model,0,20
275,275,your team should build a simple baseline version model before they build any more complex models,0,20
276,276,this provides a benchmark that your team can later compare more complex models against and will help your team identify problems in your data,0,18
277,277,build a prototype of the model and service,0,20
278,278,once you have a baseline model your team can start prototyping more complex models,0,20
279,279,this is a highly iterative process requiring substantial amounts of data and will see your team probably build a number of ai models before deciding on the most effective and appropriate algorithm for your problem,0,21
280,280,keeping your teams first model simple and setting up the right end to end infrastructure will help smooth the transition from alpha to beta,0,20
281,281,you can action this by focusing on the infrastructure requirements for your ai pipelines as the same time your team is developing the model,0,20
282,282,your simple model will provide baseline metrics and information on the models behaviour that you can use to test more complex models,0,20
283,283,test the model and service,0,20
284,284,your team will need to test your models throughout the process to mitigate against issues such as overfitting or underfitting that could undermine your models effectiveness once deployed,0,20
285,285,your team should only use the test set on your best model,0,20
286,286,keep this data separate from your models until this final test,0,20
287,287,this test will provide you with the most accurate impression of how your model will perform once deployed,0,20
288,288,evaluate the model,0,20
289,289,your team will need to evaluate your model to assess how it is performing against unseen data,0,20
290,290,this will give you an indication of how your model will perform in the real world,0,20
291,291,the best evaluation metric will depend on the problem you are trying to solve and your chosen model,0,18
292,292,while you should select the evaluation metric with data scientists you should also consider the ethical economical and societal implications,0,18
293,293,these considerations make the fine tuning of ai systems relevant to both data scientists and delivery leads,0,4
294,294,choose the final model,0,20
295,295,when choosing your final model you will need to consider what level of performance your problem needs,0,20
296,296,how interpretable you need your model to be,0,20
297,297,how frequently you need predictions or retraining,0,22
298,298,the cost of maintaining the model,0,20
299,299,assess and refine performance,0,18
300,300,once you select a final model your team will need to assess its performance and refine it to make sure it performs as well as you need it to,0,20
301,301,when assessing your models performance consider,0,20
302,302,how it performs compared to simpler models,0,20
303,303,what level of performance you need before deploying the model,0,20
304,304,what level of performance you can justify to the public your stakeholders and regulators,0,16
305,305,what level of performance similar applications deliver in other organisations,0,0
306,306,whether the model shows any signs of bias,0,20
307,307,if a model does not outperform human performance it still may be useful,0,13
308,308,for example a text classification algorithm might not be as accurate as a human when classifying documents however they can perform at a far higher scale and speed than a human,0,13
309,309,evaluate your alpha phase,0,22
310,310,when you complete building your ai prototyping phase you should have a final model or set of predictive models and a summary of their performance and characteristics,0,20
311,311,a decision on whether or not to progress to the beta phase,0,22
312,312,a plan for your beta phase,0,22
313,313,moving from alpha to beta involves integrating the model into the services decision making process and using live data for the model to make predictions on,0,20
314,314,using your model in your service has three stages,0,20
315,315,integrating your model performance test the model with live data and integrate it within the decision making workflow,0,20
316,316,integration can happen in a number of ways from a local deployment to the creation of a custom application for staff or customers,0,20
317,317,this decision is dependent on your infrastructure and user requirements,0,25
318,318,evaluating your model undertake continuous evaluation to make sure the model still meets business objectives and the model is performing at the level required,0,20
319,319,this will make sure the model performance is in line with the modelling phase and to help you identify when to retrain the model,0,20
320,320,helping users make sure users feel confident in using interpreting and challenging any outputs or insights generated by the model,0,14
321,321,you should continue to collect user needs so your team can use the models outputs in the real world,0,20
322,322,when moving from alpha to beta there are some best practice guidelines to smooth the transition,0,22
323,323,iterate and deploy improved models,0,20
324,324,after creating a beta version you team can use automated testing to create some high level tests before moving to more thorough testing,0,20
325,325,working in this way means you can launch new improvements without worrying about functionality once deployed,0,25
326,326,maintain a cross functional team,0,25
327,327,during alpha you will have relied mostly on data scientists to assess the opportunity and your data state,0,24
328,328,moving to beta needs specialists with a strong knowledge of devops servers networking data stores data management data governance containers cloud infrastructure and security design,0,27
329,329,this skillset is likely to be better suited to an engineer rather than a data scientist so maintaining a cross functional team will help smooth the transition from alpha to beta,0,27
330,330,when you complete your beta phase you should have ai running on top of your data learning and improving its performance and informing decisions,0,4
331,331,a monitoring framework to evaluate the models performance and rapidly identify incidents,0,20
332,332,launched a private beta followed by a public end to end beta prototype which users can use in full,0,20
333,333,found a way to measure your services success using new data youve got during the beta phase,0,22
334,334,evidence that your service meets government accessibility requirements,0,15
335,335,tested the way youve designed assisted digital support for your service,0,23
336,336,managing your ai systems implementation project,0,16
337,337,governance in safety is important to make sure the model shows no signs of bias or discrimination,0,16
338,338,you can consider whether the algorithm is performing in line with safety and ethical considerations,0,21
339,339,the model is explainable,0,14
340,340,there is an agreed definition of fairness implemented in the model,0,6
341,341,the data use aligns with the data ethics framework,0,9
342,342,the algorithms use of data complies with privacy and data processing legislation,0,11
343,343,governance in purpose makes sure the model is achieving its purpose business objectives,0,16
344,344,you can consider whether the model solves the problem identified,0,20
345,345,how and when you will evaluate the model,0,20
346,346,the user experience aligns with existing government guidance,0,22
347,347,testing and monitoring,0,24
348,348,governance in accountability provides a clear accountability framework for the model,0,16
349,349,you can consider whether there is a clear and accountable owner of the model,0,20
350,350,who will maintain the model,0,20
351,351,who has the ability to change and modify the code,0,15
352,352,governance in testing and monitoring makes sure a robust testing framework is in place,0,15
353,353,you can consider how you will monitor the models performance,0,20
354,354,who will monitor the models performance how often you will assess the model,0,20
355,355,public narrative,0,14
356,356,governance in public narrative protects against reputational risks arising from the application of the model,0,16
357,357,you can consider whether the project fits with the government organisations use of ai systems,0,21
358,358,the model fits with the government organisations policy on data use,0,9
359,359,the project fits with how citizens users expect their data to be used,0,9
360,360,quality assurance,0,18
361,361,governance in quality assurance makes sure the code has been reviewed and validated,0,16
362,362,you can consider whether the team has validated the code,0,25
363,363,the code is open source,0,25
364,364,project shows signs of bias or discrimination,0,6
365,365,data use is not compliant with legislation guidance or the government organisations public narrative,0,9
366,366,security protocols are not in place to make sure you maintain confidentiality and uphold data integrity,0,11
367,367,you cannot access data or it is of poor quality,0,24
368,368,you cannot integrate the model,0,20
369,369,there is no accountability framework for the model,0,16
370,370,make sure your model is fair explainable and you have a process for monitoring unexpected or biased outputs,0,20
371,371,consult guidance on preparing your data for ai,0,24
372,372,build a data catalogue to define the security protocols required,0,11
373,373,map the datasets you will use at an early stage both within and outside your government organisation,0,24
374,374,its then useful to assess the data against criteria for a combination of accuracy completeness uniqueness relevancy sufficiency timeliness representativeness validity or consistency,0,18
375,375,include engineers early in the building of the ai model to make sure any code developed is production ready,0,20
376,376,establish a clear responsibility record to define who has accountability for the different areas of the ai model,0,16
377,377,understanding ai ethics and safety,0,16
378,378,ai has the potential to make a substantial impact for individuals communities and society,0,4
379,379,to make sure the impact of your ai project is positive and does not unintentionally harm those affected by it you and your team should make considerations of ai ethics and safety a high priority,0,16
380,380,this section introduces ai ethics and provides a high level overview of the ethical building blocks needed for the responsible delivery of an ai project,0,16
381,381,the following guidance is designed to complement and supplement the data ethics framework,0,16
382,382,the framework is a tool that should be used in any project,0,26
383,383,ethical considerations will arise at every stage of your ai project,0,16
384,384,you should use the expertise and active cooperation of all your team members to address them,0,1
385,385,ai ethics is a set of values principles and techniques that employ widely accepted standards to guide moral conduct in the development and use of ai systems,0,16
386,386,the field of ai ethics emerged from the need to address the individual and societal harms ai systems might cause,0,16
387,387,these harms rarely arise as a result of a deliberate choice most ai developers do not want to build biased or discriminatory applications or applications which invade users privacy,0,25
388,388,the main ways ai systems can cause involuntary harm are misuse systems are used for purposes other than those for which they were designed and intended,0,4
389,389,questionable design creators have not thoroughly considered technical issues related to algorithmic bias and safety risks,0,25
390,390,unintended negative consequences creators have not thoroughly considered the potential negative impacts their systems may have on the individuals and communities they affect,0,17
391,391,the field of ai ethics mitigates these harms by providing project teams with the values principles and techniques needed to produce ethical fair and safe ai applications,0,16
392,392,varying your governance for projects using ai,0,8
393,393,the guidance summarised in this chapter and presented at length in the alan turing institutes further guidance on ai ethics and safety is as comprehensive as possible,0,16
394,394,however not all issues discussed will apply equally to each project using ai,0,25
395,395,an ai model which filters out spam emails for example will present fewer ethical challenges than one which identifies vulnerable children,0,0
396,396,you and your team should formulate governance procedures and protocols for each project using ai following a careful evaluation of social and ethical impacts,0,16
397,397,establish ethical building blocks for your ai project,0,16
398,398,you should establish ethical building blocks for the responsible delivery of your ai project,0,16
399,399,this involves building a culture of responsible innovation as well as a governance architecture to bring the values and principles of ethical fair and safe ai to life,0,16
400,400,building a culture of responsible innovation,0,16
401,401,to build and maintain a culture of responsibility you and your team should prioritise four goals as you design develop and deploy your ai project,0,16
402,402,in particular you should make sure your ai project is ethically permissible consider the impacts it may have on the wellbeing of affected stakeholders and communities,0,16
403,403,fair and non discriminatory consider its potential to have discriminatory effects on individuals and social groups mitigate biases which may influence your models outcome and be aware of fairness issues throughout the design and implementation lifecycle,0,6
404,404,worthy of public trust guarantee as much as possible the safety accuracy reliability security and robustness of its product,0,18
405,405,justifiable prioritise the transparency of how you design and implement your model and the justification and interpretability of its decisions and behaviours,0,20
406,406,prioritising these goals will help build a culture of responsible innovation,0,16
407,407,to make sure they are fully incorporated into your project you should establish a governance architecture consisting of a framework of ethical values,0,16
408,408,set of actionable principles,0,26
409,409,process based governance framework,0,26
410,410,you should understand the framework of ethical values which support underwrite and motivate the responsible design and use of ai,0,16
411,411,the alan turing institute calls these the sum values,0,26
412,412,respect the dignity of individuals,0,16
413,413,connect with each other sincerely openly and inclusively,0,27
414,414,care for the wellbeing of all,0,16
415,415,protect the priorities of social values justice and public interest,0,16
416,416,these values provide you with an accessible framework to enable you and your team members to explore and discuss the ethical aspects of ai,0,16
417,417,these values establish well defined criteria which allow you and your team to evaluate the ethical permissibility of your ai project,0,16
418,418,you can read further guidance on sum values in the alan turing institutes comprehensive guidance on ai ethics and safety,0,16
419,419,establish a set of actionable principles,0,16
420,420,while the sum values can help you consider the ethical permissibility of your ai project they are not specifically catered to the particularities of designing developing and implementing an ai system,0,16
421,421,ai systems increasingly perform tasks previously done by humans,0,4
422,422,for example ai systems can screen cvs as part of a recruitment process,0,4
423,423,however unlike human recruiters you cannot hold an ai system directly responsible or accountable for denying applicants a job,0,9
424,424,this lack of accountability of the ai system itself creates a need for a set of actionable principles tailored to the design and use of ai systems,0,16
425,425,the alan turing institute calls these the fast track principles fairness accountability sustainability transparency,0,16
426,426,carefully reviewing the fast track principles helps you ensure your project is fair and prevent bias or discrimination,0,6
427,427,safeguard public trust in your projects capacity to deliver safe and reliable ai,0,16
428,428,if your ai system processes social or demographic data you should design it to meet a minimum level of discriminatory non harm,0,6
429,429,to do this you should use only fair and equitable datasets data fairness,0,6
430,430,include reasonable features processes and analytical structures in your model architecture design fairness,0,20
431,431,prevent the system from having any discriminatory impact outcome fairness,0,6
432,432,implement the system in an unbiased way implementation fairness,0,6
433,433,you should design your ai system to be fully answerable and auditable,0,0
434,434,to do this you should establish a continuous chain of responsibility for all roles involved in the design and implementation lifecycle of the project,0,16
435,435,implement activity monitoring to allow for oversight and review throughout the entire projectsustainability,0,25
436,436,the technical sustainability of these systems ultimately depends on their safety including their accuracy reliability security and robustness,0,15
437,437,you should make sure designers and users remain aware of the transformative effects ai systems can have on individuals and society,0,4
438,438,your ai systems real world impact,0,4
439,439,transparency,0,14
440,440,designers and implementers of ai systems should be able to explain to affected stakeholders how and why a model performed the way it did in a specific context,0,14
441,441,justify the ethical permissibility the discriminatory non harm and the public trustworthiness of its outcome and of the processes behind its design and use,0,16
442,442,to assess these criteria in depth you should consult the alan turing institutes guidance on ai ethics and safety,0,21
443,443,build a process based governance framework,0,26
444,444,the final method to make sure you use ai ethically fairly and safely is building a process based governance framework,0,16
445,445,the alan turing institute calls it a pbg framework,0,26
446,446,its primary purpose is to integrate the sum values and the fast track principles across the implementation of ai models within a service,0,4
447,447,building a good pbg framework for your ai project will provide your team with an overview of the relevant team members and roles involved in each governance action,0,16
448,448,the relevant stages of the workflow in which intervention and targeted consideration are necessary to meet governance goals,0,17
449,449,explicit time frames for any evaluations follow up actions re assessments and continuous monitoring,0,18
450,450,clear and well defined protocols for logging activity and for implementing mechanisms to support end to end auditability,0,11
451,451,you may find it useful to consider further guidance on allocating responsibility and governance for ai projects,0,16
452,452,privacy statementby responding to this discussion paper you provide personal data to the bank of england the bank including the prudential regulation authority pra and to the financial conduct authority fca we or us,1,0
453,453,this may include your name contact details including if provided details of the organisation you work for and opinions or details offered in the response itself,1,24
454,454,we will need to assess your response to inform our work as a regulator and central bank both in the public interest and in the exercise of our official authority,1,17
455,455,we may also use your details if we need to contact you to clarify any aspects of your response,1,27
456,456,the discussion paper will explain if responses will be shared with other organisations,1,14
457,457,if this is the case the other organisation will also review the responses and may also contact you to clarify aspects of your response,1,23
458,458,we will retain all responses for the period that is relevant to supporting ongoing regulatory policy developments and reviews,1,22
459,459,the bank and pra will redact all personal information from responses within five years of receipt,1,0
460,460,the fca will retain all personal information in line with their retention schedule linked here,1,22
461,461,to find out more about how we manage your personal data your rights or to get in touch please visit the banks privacy page and the fcas privacy page,1,0
462,462,information provided in response to this paper including personal information may be subject to publication or disclosure to other parties in accordance with access to information regimes including under the freedom of information act  or data protection legislation or as otherwise required by law or in discharge of the pras the banks or the fcas functions,1,0
463,463,we may choose to publish certain responses to this discussion paper to help inform the public debate on artificial intelligence,1,14
464,464,in doing so we will not publish your name or contact details or any information which may personally identify you,1,25
465,465,please indicate if you regard all or some of the information you provide as confidential,1,14
466,466,if we receive a request for disclosure of this information we will take your indications into account but cannot give an assurance that confidentiality can be maintained in all circumstances,1,14
467,467,an automatic confidentiality disclaimer generated by your it system on emails will not of itself be regarded as binding on us,1,1
468,468,responses are requested by friday  february,1,22
469,469,we prefer responses to be sent via email to dpbankofenglandcouk,1,0
470,470,alternatively please address any comments or enquiries toreporting disclosure data strategy and ai teamprudential regulation authoritythreadneedle streetlondonecr ahforewordthe use of artificial intelligence ai and machine learning ml in financial services may enable firms to offer better products and services to consumers improve operational efficiency increase revenue and drive innovation,1,0
471,471,all of which may lead to better outcomes for consumers firms financial markets and the wider economy,1,0
472,472,as our recent survey indicates ai adoption within financial services is likely to continue to increase due to increased availability of data improvements in computational power and wider availability of ai skills and resources,1,0
473,473,similarly the bank of england the bank the prudential regulation authority pra and the financial conduct authority fca collectively the supervisory authorities also endeavour to leverage ai and benefit from this technology to help meet our respective statutory objectives and other functions,1,0
474,474,although the use of ai may bring a range of benefits it can also pose novel challenges for firms and regulators as well as amplify existing risks to consumers the safety and soundness of firms market integrity and financial stability,1,0
475,475,one of the most significant questions is whether ai can be managed through clarifications of the existing regulatory framework or whether a new approach is needed,1,8
476,476,how to regulate ai to ensure it delivers in the best interests of consumers firms and markets is the subject of a wide ranging debate both here in the uk and in other jurisdictions around the world,1,8
477,477,this discussion paper dp sits within the context of this wider debate and focuses on the regulation of ai in uk financial services,1,0
478,478,the bank pra and the fca seek to encourage a broad based and structured discussion with stakeholders on the challenges associated with the use and regulation of ai,1,0
479,479,we are keen to explore how best to address these issues in a way that is aligned with our statutory objectives provides clarity is actionable and makes a practical difference for consumers firms and markets,1,0
480,480,beginning with our current regulatory framework we have considered how key existing sectoral legal requirements and guidance in uk financial services apply to ai,1,8
481,481,this evaluation will allow us to consider which ones are most relevant explore whether they are sufficient and identify gaps,1,24
482,482,the dp considers how such legal requirements and guidance apply to the use of ai in uk financial services to support consumer protection competition the safety and soundness of individual firms market integrity and financial stability,1,0
483,483,how can policy mitigate ai risks while facilitating beneficial innovation,1,16
484,484,is there a role for technical and indeed for global standards,1,18
485,485,if so what,1,22
486,486,given the extent of overlaps within the existing sectoral rules policies and principles in uk financial services that apply to ai the supervisory authorities approach is largely limited to clarifying how the existing regulatory framework applies to ai and addressing any identified gaps in the regulatory framework,1,8
487,487,in particular the supervisory authorities are interested in the additional challenges and risks that ai brings to firms decision making and governance processes and how those may be addressed through the senior managers and certification regime smcr and other existing regulatory tools,1,8
488,488,given the wide ranging implications of ai we are keen to hear from a broad range of stakeholders,1,2
489,489,this includes firms regulated by the bank pra andor fca as well as non regulated financial services firms professional services firms such as accounting and auditing firms law firms third parties such as technology companies trade associations and industry bodies standard setting organisations academics and civil society organisations,1,0
490,490,we note the importance of building maintaining and reinforcing the trust of all stakeholders including consumers in ai,1,16
491,491,engagement between the public and private sectors will facilitate the creation of a regulatory framework that enables innovation and mitigates potential risks,1,0
492,492,we hope that this dp contributes to this process and look forward to hearing from you,1,27
493,493,victoria saportaexecutive directorprudential policy directoratebank of englandsheldon millsexecutive directorconsumers and competitionfinancial conduct authoritycidimagepngddafcjessica rusuchief data information and intelligence officer cdiiofinancial conduct authorityexecutive summaryartificial intelligence ai and machine learning ml are rapidly developing technologies that have the potential to transform financial services,1,0
494,494,the promise of this technology is to make financial services and markets more efficient accessible and tailored to consumer needs,1,0
495,495,this may bring important benefits to consumers financial services firms financial markets and the wider economy,1,0
496,496,however ai can pose novel challenges as well as create new regulatory risks or amplify existing ones,1,8
497,497,the bank of england the bank the prudential regulation authority pra and the financial conduct authority fca therefore have a close interest in the safe and responsible adoption of ai in uk financial services including considering how policy and regulation can best support this,1,0
498,498,the supervisory authorities are publishing this dp to further our understanding and to deepen dialogue on how ai may affect our respective objectives,1,8
499,499,this is part of the supervisory authorities wider programme of work related to ai including the ai public private forum the final report of which was published in february,1,2
500,500,this dp should also be considered within the context of the evolving wider national and international policy debate on ai including the uk governments policy paper establishing a pro innovation approach to regulating aiopens in a new window joint working between uk regulators through the digital regulation cooperation forum drcf and international developments from other regulators and authorities such as the proposed ai regulation for the eu,1,8
501,501,benefits and risks related to the use of ai in financial servicesai offers potential benefits for consumers businesses and markets,1,0
502,502,however ai also has the potential to create new or increased risks and challenges,1,4
503,503,the benefits risks and harms discussed in this dp are neither exhaustive nor applicable to every ai use case,1,25
504,504,the primary drivers of ai risk in financial services relate to three key stages of the ai lifecycle  data ii models and iii governance,1,0
505,505,interconnected risks at the data level can feed into the model level and then raise broader challenges at the level of the firm and its overall governance of ai systems,1,3
506,506,depending on how ai is used in financial services issues at each of the three stages data models and governance can result in a range of outcomes and risks that are relevant to the supervisory authorities remits,1,0
507,507,consumersai may benefit consumers in important ways  from improved outcomes through more effective matching to products and services to an enhanced ability to identify and support consumers with characteristics of vulnerability as well as increasing financial access,1,0
508,508,however if misused these technologies may potentially lead to harmful targeting of consumers behavioural biases or characteristics of vulnerability discriminatory decisions financial exclusion and reduced trust,1,0
509,509,competitionthere may be substantial benefits to competition from the use of ai in financial services where these technologies may enable consumers to access assess and act on information more effectively,1,0
510,510,but risks to competition may also arise where ai is used to implement or facilitate further harmful strategic behaviour such as collusion or creating or exacerbating market features that hinder competition such as barriers to entry or to leverage a dominant position,1,0
511,511,firmsthere are also many potential benefits for financial services firms including enhanced data and analytical insights increased revenue generation increased operational efficiency and productivity enhanced risk management and controls and better combatting of fraud and money laundering,1,0
512,512,equally the use of ai can translate into a range of prudential risks to the safety and soundness of firms which may differ depending on how the technology is used by firms,1,0
513,513,financial marketsai may benefit the broader financial system and markets in general through more responsive pricing and more accurate decision making which can in turn lead to increased allocative efficiency,1,0
514,514,however ai may also lead to risks to system resilience and efficiency,1,3
515,515,for example models may become correlated in subtle ways and add to risks of herding or procyclical behaviour at times of market stress,1,20
516,516,how existing legal requirements and guidance apply to the use of aiin line with their statutory objectives and to support the safe and responsible adoption of ai in uk financial services the supervisory authorities may need to intervene further to manage and mitigate the potential risks and harms ai may have on consumers firms and the stability and integrity of the uk financial system and markets,1,0
517,517,it is important that the regulatory environment is proportionate and conducive to facilitating safe and responsible adoption of ai so as not to act as a barrier to beneficial innovation,1,8
518,518,a first step towards this ambition is clarifying how existing legal requirements and guidance apply to the use of ai,1,8
519,519,in addition to legal requirements and guidance targeted at particular risks such as risks to consumers or effective competition the supervisory authorities have identified sets of cross cutting legal requirements and guidance that encompasses multiple areas of risk,1,8
520,520,cross cutting legal requirements and guidance are relevant primarily to the three key stages of the ai lifecycle  data models and governance,1,8
521,521,data related legal requirements and guidance are targeted at data quality data privacy data infrastructure and data governance,1,11
522,522,model related legal requirements and guidance for managing capital risks may provide safeguards surrounding the model development validation and review processes for the firms to which these apply,1,20
523,523,governance related legal requirements and guidance including notably the smcr are focused on proper procedures clear accountability and effective risk management across the ai lifecycle at various levels of operations,1,8
524,524,ai industry standards or codes of conduct may potentially complement the regulatory system by helping firms build trust amongst users that their systems meet widely accepted industry norms which may extend beyond the minimum requirements for regulatory compliance,1,8
525,525,the supervisory authorities encourage all relevant stakeholders to respond to this dp,1,23
526,526,this includes financial services firms regulated by one or more of the supervisory authorities including both dualand solo regulated firms non regulated financial services firms professional services firms such as accounting and auditing firms law firms third parties such as technology companies trade associations and industry bodies standard setting organisations academics and representatives from civil society,1,0
527,527,discussion questions for stakeholder inputthis dp seeks to explore whether stakeholders consider the existing sectoral legal requirements and guidance to be sufficient to address the risks and harms associated with ai where there may be gaps in existing legal requirements and guidance andor how any additional intervention may support the safe and responsible adoption of ai in uk financial markets,1,8
528,528,to help us in this aim we are inviting responses to the questions listed in chapter,1,27
529,529,the questions fall under three main categoriessupervisory authorities objectives and remits exploring the best approach to defining andor scoping the characteristics of ai for the purposes of legal requirements and guidance,1,8
530,530,benefits and risks of ai identifying the areas of benefits risks and harms in relation to which the supervisory authorities should prioritise action,1,3
531,531,regulation exploring whether the current set of legal requirements and guidance is sufficient to address the risks and harms associated with ai and how additional intervention may support the safe and responsible adoption of ai in uk financial services,1,0
532,532,this includes understanding which areas of the current regulatory framework  would benefit from further clarification with respect to ai ii could be extended to better encompass ai and iii could act as a regulatory barrier to the safe and responsible adoption of ai in uk financial services,1,8
533,533,introduction the bank and the fca established the ai public private forum aippf to further dialogue on ai innovation and safe adoption within financial services,1,0
534,534,the aippf launched in october  and ran for one year bringing together a diverse group of experts from across financial services the tech sector and academia,1,0
535,535,the aippf published its final report in february,1,2
536,536,the report explores the various barriers to adoption challenges and risks related to the use of ai in financial services,1,0
537,537,the report reflects the views of its members as individual experts rather than the views of the bank or the fca,1,0
538,538,the bank and the fca are publishing this dp in response to the aippf final report which made it clear that the private sector wants regulators to have a role in supporting the safe adoption of ai in uk financial services as well as a wider background of domestic and international developments regarding ai regulation,1,0
539,539,the purpose of this dp is to share and obtain feedback onthe potential benefits risks and harms related to the use of ai in financial serviceshow the current regulatory framework could apply to aiwhether additional clarification may be helpful andhow policy can best support further safe ai adoption,1,0
540,540,background in  the supervisory authorities conducted a joint survey to better understand the use of ai and ml in uk financial services,1,0
541,541,the survey identified that financial services firms were increasingly using ai and needed effective and evolving risk management controls if they were to use it safely and harness the benefits,1,0
542,542,the bank in its response to the future of finance report announced in june  that it would establish with the fca and firms a public private working group the ai public private forum to further the dialogue on ai innovation and to explore whether principles and guidance could support the safe adoption of these technologies within financial services,1,0
543,543,the aippf launched in october  and ran for one year with four quarterly meetings and a number of workshops,1,2
544,544,it brought together a diverse group of experts from across financial services the tech sector and academia along with public sector observers from other uk regulators and government,1,0
545,545,the aippf published its final report in february,1,2
546,546,the report explores the various barriers challenges and risks related to the use of ai in financial services and potential ways to address them,1,0
547,547,the report reflects the views of aippf members in a personal capacity rather than their institutions the bank or the fca,1,0
548,548,the aippf has made clear to the supervisory authorities that the private sector wants regulators to have a role in supporting the safe adoption of ai in uk financial services,1,0
549,549,at the same time the uk government has sought to develop a national position on regulating ai across all sectors with the publication in july  of a policy paper establishing a pro innovation approach to regulating ai july  policy paper,1,8
550,550,similarly other governments and regulators around the world are developing their thinking and approaches to ai,1,8
551,551,for example the european commissions proposal for ai regulation the principles to promote fairness ethics accountability and transparency in the use of ai and data analytics in singapores financial sector feat and the veritas initiative from the monetary authority of singapore the organisation for economic co operation and development oecd ai principles and the internet information service algorithmic recommendation management provisions that came in force in march  in the peoples republic of china,1,2
552,552,the supervisory authorities are publishing this dp in response to the aippf final report following their previous work on ai including through the digital regulation co operation forum and in the wider domestic and international context of emerging ai regulation policies and principles in order to facilitate a public debate on the safe and responsible adoption of ai in uk financial services,1,0
553,553,box  uk government  establishing a pro innovation approach to regulating aithe uk government published the july  policy paper which set out its emerging thinking on its approach to regulating ai,1,8
554,554,the july  policy paper highlighted several key challenges that cut across the regulatory landscape including a lack of clarity overlaps inconsistencies and gaps in the current approach,1,8
555,555,many of these challenges may apply to the regulation of the uk financial services sector,1,0
556,556,the supervisory authorities hope the dp stimulates debate on how best to overcome these challenges within uk financial services,1,0
557,557,the uk government aims to address these challenges by creating a framework for regulating ai that is context specific pro innovation and risk based coherent proportionate and adaptable,1,8
558,558,moreover the uk governments proposed approach to regulating ai is underpinned by a set of cross sectoral principles tailored to the specific characteristics of aiensure that ai is used safelyensure that ai is technically secure and functions as designedmake sure that ai is appropriately transparent and explainableembed considerations of fairness into aidefine legal persons responsibility for ai governanceclarify routes to redress or contestabilitythe uk government stated that it will be considering the roles powers remits and capabilities of regulators the need for coordination and how this should be delivered across the range of regulators statutory and non statutory involved in ai regulation as part of its next steps,1,8
559,559,see table a in appendix  for a mapping of the uk government ai regulation principles against the dp chapters and sections,1,8
560,560,discussion paper structure chapter  explains how the use of ai in uk financial services could impact the supervisory authorities respective objectives,1,0
561,561,the chapter then explores the potential merits of providing a regulatory definition for ai and how regulators in other jurisdictions are approaching this,1,8
562,562,the chapter also provides a brief overview of how ai is used in financial services,1,0
563,563,chapter  sets out some of the benefits and risks related to the use of ai in uk financial services,1,0
564,564,these are divided into different categories based on each of the supervisory authorities objectives eg consumer protection competition safety and soundness of firms etc,1,0
565,565,more information on some of the potential risks and benefits associated with the use of ai in uk financial markets can be found in a report by the alan turing institute which was commissioned by the fca,1,0
566,566,chapter  provides an overview of how current key legal requirements and guidance apply to the use of ai in uk financial services,1,0
567,567,this includes how domestic regulation links to international regulation and how financial services regulations sit alongside a body of cross sectoral legislation and regulation,1,0
568,568,the supervisory authorities are keen to understand stakeholders views as to whether the current regulatory framework is sufficient to address the potential risks and harms associated with ai andor how any additional intervention may support the safe and responsible adoption of ai in uk financial services,1,0
569,569,annex  sets out how the dp sits within the wider context of domestic developments that are relevant to the use and regulation of ai in uk financial services,1,0
570,570,this includes the uk governments national ai strategy and the july  policy paper,1,2
571,571,annex  highlights the different international regulatory responses to ai in financial services and examines emerging approaches,1,0
572,572,the supervisory authorities are keen to explore if any of the international approaches or elements of any approaches may be helpful to support the safe and responsible adoption of ai in uk financial services,1,0
573,573,annex  sets out an overview of certain key existing legal requirements and guidance relevant to data and ai,1,8
574,574,annex  summarises existing pra supervisory statements ss and certain other standards relevant to various aspects of model risk management for pra authorised firms,1,0
575,575,annex  provides a list of selected relevant publications on the use of ai in financial services,1,0
576,576,responses and next steps this dp closes on friday  february,1,22
577,577,the supervisory authorities invite feedback on the topics discussed in this dp,1,23
578,578,please address any comments or enquiries to dpbankofenglandcouk,1,0
579,579,the responses to this dp will help inform both the supervisory authorities thinking and any potential future policy proposals which will sit within the wider domestic and international context of emerging ai regulation,1,8
580,580,as with other dps the supervisory authorities may choose to publish the anonymised and aggregated responses to this dp to help inform the public debate on ai,1,8
581,581,therefore the supervisory authorities would encourage all relevant stakeholders to respond to this dp and engage in the discussion,1,23
582,582,this includes financial services firms regulated by one or more of the supervisory authorities including both dualand solo regulated firms non regulated financial services firms professional services firms such as accounting and auditing firms law firms third parties such as technology companies trade associations and industry bodies standard setting organisations academics and civil society organisations,1,0
583,583,supervisory authorities objectives and remits the supervisory authorities consider the use of ai in financial services to be relevant to their respective objectives notably the below,1,0
584,584,the banks mission is to maintain monetary and financial stability in the uk and subject to maintaining price stability the bank is to support the uk governments economic policy including its objectives for growth and employment,1,0
585,585,the bank is also responsible for supervising certain financial market infrastructure firms fmis in the uk including central securities depositories csds central counterparties ccps recognised payment systems operators rpsos and specified service providers sps to rpsos,1,0
586,586,the pra has two primary objectives  a general objective to promote the safety and soundness of pra authorised firms and ii an objective specific to insurance firms to contribute to securing an appropriate degree of protection for those who are or who may become policyholders,1,0
587,587,the pra also has a secondary objective to facilitate effective competition in the markets for services provided by pra authorised firms in carrying on regulated activities,1,0
588,588,the fcas strategic objective is to ensure that the relevant markets including financial markets function well and its operational objectives are to  secure an appropriate degree of protection for consumers ii protect and enhance the integrity of the uk financial system and iii promote effective competition in the interests of consumers in the markets for regulated financial services,1,0
589,589,the fca must so far as is compatible with its consumer protection and market integrity objectives also exercise its functions in a way that promotes effective competition in the interests of consumers,1,0
590,590,effective regulation relies on consent trust and confidence from the public including consumers and regulated firms and ensures that powers are used consistently transparently and proportionately,1,0
591,591,the fca mission sets out a framework for the way the fca takes these decisions and serves the public interest,1,26
592,592,why the supervisory authorities have an interest in ai ai may bring important benefits to consumers financial services firms financial markets and the wider economy,1,0
593,593,the promise of this technology is to make financial services and markets more cost effective efficient accessible and tailored to consumer needs,1,0
594,594,however ai can pose novel challenges as well as create new risks or amplify existing ones,1,4
595,595,therefore the supervisory authorities have a close interest in the safe and responsible adoption of ai in uk financial services,1,0
596,596,in line with their statutory objectives and to support the safe and responsible adoption of ai technologies in uk financial services the supervisory authorities may need to intervene further to manage and mitigate the potential risks and harms related to ai applications,1,0
597,597,while the supervisory authorities generally take a technology neutral approach to regulation see box  for more details they are aware that risks may relate to the use of specific technologies,1,8
598,598,the supervisory authorities recognise the importance to consumers financial services firms and markets of a regulatory environment conducive to facilitating safe and beneficial innovation and competition,1,0
599,599,the pra and the fca have competition related statutory objectives and are required to have regard to the principle of proportionality when exercising their general functions including when making rules,1,26
600,600,for instance the supervisory authorities would wish to avoid introducing unnecessarily complex or costly rules that can act as a barrier to entry especially for non systemic firms or unintentionally favour larger firms with more resource to manage compliance with complex rulebooks,1,8
601,601,a proportionate approach is critical to supporting the safe and responsible adoption of ai and other technologies across uk financial services,1,0
602,602,it is also important to acknowledge that financial services firms are themselves subject to compliance obligations to other regulators when using ai,1,0
603,603,for examplewhere financial services firms use ai to process personal data firms may have regulatory obligations under the uk general data protection regulation uk gdpr and the data protection act,1,8
604,604,the information commissioners office ico has responsibility for enforcing compliance with data protection requirements andfinancial services firms also need to ensure compliance with the equality act  equality act in respect of which the equality and human rights commission ehrc has enforcement powers,1,11
605,605,where firms use ai systems in their decision making processes they will need to ensure that it does not result in unlawful discrimination based on protected characteristics,1,6
606,606,box  what do we mean by technology neutral,1,25
607,607,the supervisory authorities recognise that the use of new technologies such as ai in financial services may benefit consumers through new and better products and services enhance the safety and soundness of firms through improved risk management informed by advanced analytics and enhanced decision making and deliver a more efficient and effective financial system to the benefit of the wider economy,1,0
608,608,generally the supervisory authorities adopt a technology neutral approach in their supervisory and regulatory approaches,1,8
609,609,as such the core principles rules and regulations do not usually mandate or prohibit specific technologies,1,1
610,610,however the supervisory authorities monitor and mitigate technology risks as they can equally have adverse implications for their objectives including consumer protection competition the safety and soundness of firms market integrity and financial stability,1,0
611,611,the use of a certain technology may have an impact on the risks associated with firms activities,1,0
612,612,the supervisory authorities may therefore issue technology specific rules and guidance where appropriate,1,8
613,613,certain technologies may also raise novel challenges for firms and regulators which may mean it is difficult for firms to understand how existing rules apply to that technology,1,0
614,614,in those cases the supervisory authorities may issue guidance or use other policy tools to clarify how the existing rules and relevant regulatory expectations apply to those technologies,1,8
615,615,for example fg guidance for firms outsourcing to the cloud and other third party it services and paragraph  in pra ss outsourcing and third party risk management,1,0
616,616,firms could for example also consider outcomes and risk management as covered in the fcas implementing technology change paper,1,3
617,617,this dp reflects the supervisory authorities desire to understand how they may best support the safe and responsible adoption of ai in uk financial services in line with their statutory objectives,1,0
618,618,what is ai,1,4
619,619,while there is no consensus on a single definition it is generally accepted that ai is the simulation of human intelligence by machines including the use of computer systems which have the ability to perform tasks that demonstrate learning decision making problem solving and other tasks which previously required human intelligence,1,4
620,620,machine learning is a sub branch of ai,1,13
621,621,ai a branch of computer science is complex and evolving in terms of its precise definition,1,4
622,622,it is broadly seen as part of a spectrum of computational and mathematical methodologies that include innovative data analytics and data modelling techniques,1,26
623,623,the bank and fca previously defined ai in non legal terms as the theory and development of computer systems able to perform tasks which previously required human intelligence,1,0
624,624,the term ai system refers to the set of integrated computational elements and microservices that input output process and store data and information,1,4
625,625,therefore one ai system may include multiple ai algorithms models and datasets,1,4
626,626,it is important to distinguish between the terms algorithm and model as these have specific meanings within financial services regulation,1,26
627,627,for example the competition and markets authority cma defines algorithmic systems as shorthand for an automated system at the intersection of algorithms data models processes objectives and people,1,26
628,628,although the supervisory authorities do not define algorithms they do define algorithmic trading in the context of the markets in financial instruments directive mifid implementation and members of the digital regulation cooperation forum drcf also defines algorithmic processing,1,0
629,629,the pra defines model in its recent cp model risk management principles for banks,1,0
630,630,with all of these definitions it is important to note that while both algorithms and models may be components of an ai system they may not in themselves necessarily be deemed to be ai,1,26
631,631,many of the issues related to ai are not new and apply to other data analytics modelling techniques and technologies,1,4
632,632,for example many traditional financial models such as generalized linear models glm used to classify risk in insurance and internal ratings based irb approaches to modelling regulatory capital requirements in banking that do not use ai may be as complex and difficult to understand or explain,1,0
633,633,however there are three key areas in the ai lifecycle see figure  below that can introduce novel challenges and increase existing risks,1,4
634,634,figure  stages of ai lifecyclefootnotessource aippf data  ai can analyse traditional data sources as well as unstructured and alternative data from new sources such as image and text data,1,4
635,635,the increase in volume of data and data formats within the context of ai places added emphasis on data quality,1,18
636,636,also ai may pick up bias within datasets and may not perform as intended when exposed to issues excluded from the trainingtesting data so new data quality metrics like representativeness and completeness may be needed,1,18
637,637,model  whereas traditional financial models are usually rules based with explicit fixed parameterisation ai models are able to learn the rules and alter model parameterisation iteratively,1,0
638,638,the use of ai models also represents a step change for three other reasons firstly the speed and frequency at which the models update with some ai models able to learn continuously secondly the scale in terms of the volume of data needed to train the models and the number of features that are used as inputs and thirdly the complexity of certain techniques such as convolutional neural networksfootnote which can make them more opaque the so called black box problem,1,4
639,639,governance  ai may also pose some novel challenges for governance especially where the technology is used to facilitate autonomous decision making and may limit or even potentially eliminate human judgement and oversight from decisions,1,8
640,640,some of the data and model issues can also have implications for governance,1,8
641,641,for example a lack of explainability or transparency in some ai models may mean extra care or actions are needed to ensure full accountability and sufficient oversight,1,14
642,642,given the kind of issues outlined above regulators and authorities have generally found it useful to distinguish between ai and non ai by eitherproviding a more precise legal definition of what ai is for example the definition of artificial intelligence system in the proposed european ai regulation and proposed canadian ai and data act and thereby what ai is not orviewing ai as part of a wider spectrum of analytical techniques with a range of elements and characteristics,1,4
643,643,such approaches may include a classification scheme to encompass different methodologies or a mapping of the characteristics of ai such as the german financial services regulators consultation paper on machine learning in risk models the oecd classification system and the uk governments proposed framework for regulating ai,1,13
644,644,each approach aims to provide clarity of what constitutes ai within the context of a specific regulatory regime and therefore what restrictions and requirements may apply to the use of the technology,1,8
645,645,this depending on the particular requirements applied may help  consumers to understand where and when ai is used in products and services they use ii firms to use the technology responsibly and manage the relevant trade offs and iii regulators to assess whether firms are using the technology in a safe and responsible manner and establishing appropriate controls and procedures to mitigate against potential risks as well as adhering to any regulatory requirements,1,0
646,646,for the supervisory authorities there may be a number of benefits to providing a more precise definition of ai in their respective rulebooks,1,8
647,647,the benefits may include  creating a common language for firms and regulators which may ease uncertainty ii assisting in a uniform and harmonised response from regulators towards ai and iii providing a basis for identifying whether or not specific use cases might be captured under particular rules and principles,1,0
648,648,at the same time there are various challenges to the supervisory authorities providing a more precise definition as the basis for clarification or additional or alternative regulatory requirements,1,8
649,649,these include  the difficulty to create a robust definition that would remain relevant in what is undoubtedly a rapidly developing field ii challenges of a definition that is too broad as this could cause regulation to lack specificity precision and proportionality iii potential for certain use cases products and services to fall outside of scope when they should not and iv creating the ability and incentives for firms to misclassify ai to reduce regulatory oversight,1,8
650,650,the alternative approaches as described above may address some of these challenges and could potentially be more suitable for the regulation of ai in uk financial services,1,0
651,651,how ai is used in financial services financial services firms are increasingly using ai across a range of business areas and the covid  pandemic has accelerated the pace of adoption see annex  for a list of relevant publications,1,0
652,652,in this rapidly evolving space the following high level trends in financial services may have implications for consumers firms markets and the supervisory authoritiesgreater ai adoption is occurring both within established financial services firms and also newer fintech and insurtech companies,1,0
653,653,this is being powered by improved software and hardware including greater use of cloud computing which is often provided via third parties as well as via partnerships and investment with smaller ai specific vendors,1,0
654,654,firms are using ai for more material business areas and use cases,1,0
655,655,from anti money laundering aml functions and credit and regulatory capital modelling in banking claims management and product pricing to capital reserve modelling in insurance and order routing and execution to generating trading signals in investment management,1,0
656,656,firms are also using more complex ai techniques,1,0
657,657,this is because complexity often corresponds to improved performance and use of ever greater volumes of data,1,26
658,658,as firms begin to use more complex methods and data sets and apply ai to more complex use cases they will likely become more comfortable with the technology and adoption of even more complex techniques will increase further,1,0
659,659,for more information and detail please see the supervisory authorities recent report on machine learning in uk financial services   ml survey,1,0
660,660,box  the supervisory authorities use of aithe supervisory authorities utilise ai where appropriate to support and enhance their capabilities,1,8
661,661,this can include regtech which refers to the use of technology by firms to support them to fulfil their regulatory obligations and suptech which refers to the use of technology by regulators to facilitate their supervisory duties,1,1
662,662,there is also the potential to use ai to support central bank functions such as economic analysis,1,0
663,663,the bank uses ai for predictive analytics the study of non linear interactions between variables and analysis of larger and richer datasets which can potentially help forecast gdp growth bank distress and financial crises prediction,1,0
664,664,the bank is also exploring how ai enabled text analysis of newspapers can help improve economic forecasting and how ai can create faster indicators which may enable real time economic analysis,1,0
665,665,the pra successfully introduced a cognitive search tool with ai capabilities that helps supervisors gain more insights from firm management information by extracting key patterns from unstructured and complex datasets,1,0
666,666,the pra is also developing other ai tools for its staff to assist in their work,1,4
667,667,the fca has been exploring how ai can provide additional levels of insight across the organisation,1,4
668,668,examples include using natural language processing to generate insights from unstructured text documentation predictive analytics to forecast where risks may lie across our supervisory waterfront and simulation techniques to generate realistic synthetic data sets,1,13
669,669,the potential for ai innovation to assist the supervisory authorities and regulatory activity depends significantly on access to data and technology so may have implications for the data that firms are required to share,1,8
670,670,questions supervisory authorities objectives and remitsq would a sectoral regulatory definition of ai included in the supervisory authorities rulebooks to underpin specific rules and regulatory requirements help uk financial services firms adopt ai safely and responsibly,1,0
671,671,if so what should the definition be,1,26
672,672,q are there equally effective approaches to support the safe and responsible adoption of ai that do not rely on a definition,1,16
673,673,if so what are they and which approaches are most suitable for uk financial services,1,0
674,674,potential benefits and risks this chapter briefly summarises the potential benefits and risks of the use of ai in financial services,1,0
675,675,it also describes how the drivers of ai benefits and risks in financial services can occur at different levels within ai systems data models and governance and how these drivers of risk can result in a range of outcomes depending on how the ai system is used within financial services,1,0
676,676,the benefits and risks are divided into different categories based on each of the supervisory authorities objectives namely consumer protection competition safety and soundness of firms insurance policyholder protection financial stability and market integrity,1,0
677,677,the benefits risks and harms discussed in this chapter are neither exhaustive nor applicable to every ai use case,1,25
678,678,moreover while many of the benefits and harms presented in this chapter are neither new nor unique to ai the use of ai in financial services may amplify existing risks and introduce novel challenges,1,0
679,679,previous work by the supervisory authorities has shown that the drivers of ai risk in financial services can occur at different levels within ai systems see figure  starting with the risks associated with the use of data to train test and run ai models which can feed into risks arising from the design and use of ai models themselves which can in turn lead to challenges for the governance structures necessary to manage those risks,1,0
680,680,data  given that ai relies significantly on large volumes of data in its development training and testing and implementation data related risks can be amplified and have significant implications for ai systems,1,3
681,681,drivers of data risk can include errors in the training data incomplete or unrepresentative data significant outliers or noise historical data biases insufficient data and more,1,3
682,682,poor data preparation validation and management can also be drivers of risk,1,3
683,683,models  poor ai model performance may result from data related risks but also from a range of model related risks,1,3
684,684,these could include inappropriate model choices errors in the model design or construction lack of explainability unexpected behaviour unintended consequences degradation in model performance model or concept drift and more,1,14
685,685,as with data poor model risk management including validation change management and monitoring can also be a driver of risk,1,3
686,686,the extent to which poor model performance leads to poor outcomes can depend on the degree of autonomy of a model,1,20
687,687,governance  at the governance level the drivers of risk can include the absence of clearly defined roles and responsibilities for ai insufficient skillsets governance functions that do not include the relevant business areas or consider the relevant risks such as ethical risks a lack of challenge at the board and executive level and general lack of accountability,1,3
688,688,it is important to note that if the drivers of risks are mitigated then ai may deliver a number of benefits,1,3
689,689,for example the combination of high quality data appropriate model choices and good governance can result in a well performing ai system and accurate outputs,1,16
690,690,moreover both the benefits and risks that ai deliver are dependent on the context in which it is used and the purpose for which the technology is deployed,1,4
691,691,for example issues relating to data quality within a consumer facing ai system are likely to result in benefits andor risks related to consumer protection,1,0
692,692,whereas data quality issues within an ai trading system are likely to result in benefits andor risks related to the safety and soundness of individual firms and potentially even financial stability,1,0
693,693,therefore the following sections highlight some of the potential ai benefits and risks as they relate to the supervisory authorities objectives and remits,1,4
694,694,consumer protection  fca ai can harness the power of large volumes of data to identify characteristics about consumers and their preferences,1,0
695,695,this can be put to many uses from providing access to financial services to consumers with non standard histories to identifying demographics with specific needs or characteristics of vulnerability and better product matching for consumers,1,27
696,696,at the same time with a more granular understanding of individual consumers characteristics there may be a greater potential to identify and exploit consumer behavioural biases and characteristics of vulnerability from exploiting inertia to harmful price discrimination to exploiting actual characteristics of vulnerability,1,0
697,697,whether the use of ai is beneficial to consumers will depend on how it is used and for what purpose,1,0
698,698,there is a risk that the use of ai could be associated with discriminatory decisions,1,6
699,699,bias related to protected characteristics such as race or sex could arise inadvertently during model development,1,6
700,700,this is because even if these variables are excluded from the model they could still be correlated from other data points and identify protected characteristics,1,20
701,701,biased or discriminatory decisions can arise from bias in the underlying data on which the ai model is trained,1,6
702,702,data may reflect historical biases in society for instance where certain groups have been less able to access credit,1,6
703,703,or the overall dataset may be unrepresentative,1,9
704,704,ai models can amplify the inherent historical biases in input data potentially leading to biased decisions for consumers,1,6
705,705,ai applications could be used in a way that excludes certain consumers,1,25
706,706,for example ai based insurance screening or credit provision could enable greater segmentation between low risk and high risk consumer groups,1,0
707,707,this may have implications for risk pooling and could lead providers to exclude or offer unaffordable premiums to high risk consumers,1,0
708,708,personalisation eg pricing specific to individual consumers could also lead to some financial products not being offered to certain groups potentially resulting in unlawful discrimination based on protected characteristics,1,6
709,709,competition  fca consumer facing ai systems such as those used in open banking might improve competition in a market through improving consumers ability to assess access and act on information which in aggregate can increase competitive pressures on firms,1,0
710,710,open banking enables third parties to access consumers transaction data with their explicit consent in a secured environment,1,0
711,711,leveraging on these data to generate user centric insights firms can provide innovative and tailored solutions to end users,1,27
712,712,for example enabling consumers to solve complex problems to optimise which financial product offers them best value or supporting consumers in making better financial decisions as well as providing access to financial markets with robo advisor tools ai systems can remove some of the informational advantage which sellers have over buyers in markets with complex financial products,1,0
713,713,academic research has shown that by detecting price changes from rivals and enabling rapid or automatic response ai systems could potentially facilitate collusive strategies between sellers and punish deviation from a collusive strategy,1,0
714,714,where ai is particularly relevant to a business practice the costs of entry including both the staff and skills as well as the data and technology itself may be raised to a level that limits market entry with potentially harmful effects on competition,1,0
715,715,the cma published a paper on algorithms how they can reduce competition and harm consumers and an accompanying call for information in january,1,13
716,716,safety and soundness  pra and fca ai can help financial services firms create better decision making tools develop new insights and new andor better products and services for consumers,1,0
717,717,these benefits are largely due to the uplift in predictive power more granular classification and segmentation ability to capture non linear relationships and the ability to analyse larger volumes of data and new data sources such as unstructured and alternative data,1,13
718,718,financial services firms are already benefitting from the use of ai by improving operational efficiency across a range of areas which can help reduce costs and processing times,1,0
719,719,ai models can also be more accurate in predicting default risk for consumer and corporate credit,1,0
720,720,these improved probability of default estimates could in turn lead to more appropriate modelling of regulatory capital,1,22
721,721,there are a number of challenges and risks related to the use of ai which may amplify prudential risks credit liquidity market operational reputational etc and have implications for the safety and soundness of firms,1,0
722,722,for example in credit risk accuracy or consistency errors within the dataset will likely impact the ability of the ai model to accurately quantify the probability of default and risk of loss which could in turn lead to inaccurate capital modelling,1,18
723,723,one of the strengths of some ai systems is their dynamic nature including their ability to learn continuously from data,1,4
724,724,the nature of the hyperparametersfootnote functional form of the models and their outputs can also adapt continuously,1,20
725,725,but this can also make such systems more susceptible to data drift and concept drift which in can in turn make the models and systems less stable,1,26
726,726,moreover the complexity of ai models is increasing which can lead to a lack of explainability or interpretability known as the black box problemfootnote the potential for drift combined with the lack of explainability can in turn lead to a range of prudential risks,1,14
727,727,for example the use of complex andor opaque ai systems to model probability of default and loss given default in irb credit models could lead to firms having incorrect levels of regulatory capital,1,0
728,728,insurance policyholder protection  pra and fca ai in the insurance sector has the potential to improve the efficiency of data processing and decision making in terms of both underwriting and claims processing,1,0
729,729,in life insurance which includes an investment component firms could leverage ai to support the investment choices of policyholders,1,0
730,730,in general insurance ai could be used for automating claims management,1,0
731,731,firms in the insurance sector can also use ai to analyse new unstructured data sources like telematics or data collected from wearable devices to provide more tailored products andor pricing,1,0
732,732,insurers use ai across a range of business areas which could pose risks to policyholder protection,1,0
733,733,risks related to underwriting could lead to inappropriate pricing and marketing,1,3
734,734,for example ai models trained on historical data may not account for a breakthrough healthcare treatment which can lead to mispriced policies,1,9
735,735,similarly risks such as concept drift and lack of explainability in claims management ai systems could impact policyholders ability to claim and their overall protection,1,10
736,736,risks related to building ai models for cash flow and capital reserve estimates could result in inaccurate predictions and reserve levels that could in turn impact insurers ability to meet future liabilities,1,0
737,737,financial stability and market integrity  the bank and fca the benefits of ai for individual firms may also extend to the financial system,1,0
738,738,for example more efficient processing of information by ai in credit decisions insurance contracts and customer interaction may contribute to a more efficient financial system overall,1,4
739,739,given the highly complex and non linear nature of financial systems ai can be a powerful tool for modelling macro financial risks and dynamics,1,0
740,740,ais capacity to process and analyse large volumes of multidimensional data means that it can be used to detect patterns in unstructured data and for example identify shifts in secular trends and narrative consensus,1,4
741,741,similarly the use of ai by individual firms and within financial markets may amplify many of the existing risks to financial stability through various transmission channels,1,0
742,742,for example the use of similar datasets and ai algorithms may result in uniformity across models and approaches at multiple firms which could amplify procyclical behaviour and lead to herding in certain use cases such as algorithmic trading,1,0
743,743,markets could also potentially become vulnerable to manipulation and prone to flash bubblesfootnote or crashes if sentiment analysis and social media signals were used at scale in ai trading,1,0
744,744,the feedback loopfootnote between the data and ai algorithm could exacerbate these effects,1,4
745,745,also a further key challenge for firms lies in their ability to monitor operations and risk management activities that take place outside their organisations at third parties,1,0
746,746,increased reliance on third parties often outside the regulatory perimeter for datasets ai algorithms and other it outsourcing such as cloud computing may amplify systemic risks,1,8
747,747,for example operational failures and cyberattacks at critical third parties could result in disruption to certain ai services and therefore lead to a single point of failure that could impact multiple firms and markets,1,0
748,748,the supervisory authorities published dp operational resilience critical third parties to the uk financial sector to explore these issues,1,0
749,749,the potential risks associated with ai can also be assessed in relation to their outcome the impact the technology has on markets consumers or indeed the environment,1,0
750,750,the oecd for example has developed a baseline framework to advance a shared understanding of ai including metrics for risks associated with the use of ai,1,3
751,751,similarly the national institute of standards and technology is currently developing a framework to better manage risks to individuals organisations and society associated with the use of ai,1,3
752,752,a key question is whether support for safe and responsible ai adoption is best delivered through a process based or an outcomes based framework or indeed a combination of the two,1,16
753,753,for an outcomes based approach to ai the following question arises what are the most relevant metrics to measure impact including what evidence is required to demonstrate good outcomes and how such evidence can be collected,1,17
754,754,finally the use of ai by fmi firms including in the clearing settlement and recording of financial transactions may have implications for financial stability,1,0
755,755,although the dp does not consider these issues in detail fmis are a vital part of the uk financial system and the supervisory authorities are keen to understand if there are any such benefits and risks specific to fmis use of ai so would encourage stakeholders with an interest in this sector to provide feedback,1,0
756,756,questions benefits risks and harms of aiq which potential benefits and risks should supervisory authorities prioritise,1,3
757,757,q how are the benefits and risks likely to change as the technology evolves,1,0
758,758,q are there any novel challenges specific to the use of ai within financial services that are not covered in this dp,1,0
759,759,q how could the use of ai impact groups sharing protected characteristics,1,11
760,760,also how can any such impacts be mitigated by either firms andor the supervisory authorities,1,0
761,761,q what metrics are most relevant when assessing the benefits and risks of ai in financial services including as part of an approach that focuses on outcomes,1,0
762,762,regulation this chapter discusses some of the current legal requirements and guidance that are considered by the supervisory authorities to be most relevant to mitigating the risks associated with ai,1,8
763,763,it also includes discussion of how financial services regulation sits alongside a body of cross sectoral legislation and regulation as well as the domestic and international context of emerging ai specific legal requirements and guidance,1,8
764,764,this chapter focuses on legal requirements and guidance that are applicable to pra and fca authorised firms rather than bank supervised fmis,1,0
765,765,as with the previous chapter the section headings relate to the supervisory authorities objectives and remits,1,19
766,766,introduction as noted in the foreword there is a wide ranging debate about the regulation of ai,1,8
767,767,one of the central questions is whether the technology can be managed through extensions of existing regulatory frameworks or whether a new approach is needed,1,8
768,768,this debate is happening across different sectors of the economy and both here in the uk and other jurisdictions around the world,1,2
769,769,governments regulators and other authorities have published numerous documents on the subject,1,8
770,770,these range from emerging regulations and laws such as those in the european union peoples republic of china and canada to cross sectoral principles aimed at regulators including the uk government proposal and oecd principles and sector specific principles for financial services firms such as the ones issued by the de nederlandsche bank the hong kong monetary authority and the monetary authority of singapore as well as various policy papers and reports,1,0
771,771,this dp sits within the broader context of this global debate and focuses on a specific area  the regulation of ai within uk financial services,1,8
772,772,at the same time previous work by the supervisory authorities including the aippf and  ml survey have found that one of the challenges to the adoption of ai in uk financial services is the lack of clarity surrounding the current rules regulations and principles in particular how these apply to ai and what that means for firms at a practical level,1,0
773,773,to help address this challenge this chapter discusses some parts of the current regulatory framework that are considered by the supervisory authorities to be most relevant to the regulation of ai,1,8
774,774,the supervisory authorities are keen to gather feedback from stakeholders as to whether additional clarification of existing legal requirements and guidance in respect of ai may be helpful if the current regulatory framework could benefit from extension to better encompass ai and how the supervisory authorities may best support the safe and responsible adoption of ai in uk financial services,1,8
775,775,box  proportionality and the supervisory authorities approach to regulationone of the regulatory principles under the financial services and markets act  fsma that the pra and the fca must have regard to in discharging their general functions is that a burden or restriction which is imposed on a person or on the carrying on of an activity should be proportionate to the benefits considered in general terms which are expected to result from the imposition of that burden or restrictionfootnote this principle of proportionality informs the supervisory authorities thinking and approach to ai including any potential future regulatory interventions,1,0
776,776,other financial services regulators and authorities have explicitly noted the importance of proportionality in relation to the regulation of ai,1,8
777,777,for example the international organization of securities commissions iosco guidance on the use of ai by market intermediaries and asset managers states the iosco members and firms should consider proportionality when implementing measures,1,0
778,778,it notes that firms and regulators should in judging proportionality consider the activity that is being undertaken the complexity of the activity risk profiles the degree of autonomy of the ai applications and the potential impact that the technology has on client outcomes and market integrity,1,0
779,779,similarly de nederlandsche bank the central bank of the netherlands states that the applicability of their ai principles should be considered in light of the scale complexity and materiality of an organisations ai applications,1,0
780,780,consumer protection  fca the fcas approach to consumer protection is based on a combination of the fcas principles for businesses the principles other high level rules and detailed rules and guidance,1,0
781,781,these include principles and rules contained in the fca handbook,1,1
782,782,the principles are general statements of the fundamental obligations of firms and other persons to whom they apply who are liable to disciplinary sanctions if they breach one or more of the principles,1,1
783,783,the fca has recently introduced new rules to raise standards for firms dealing with retail customers the policy statement  a new consumer duty the consumer duty,1,0
784,784,these rules come into force for new and existing products and services that are open to sale or renewal on  july,1,22
785,785,the rules come into force for closed products and services on  july,1,22
786,786,the consumer duty includes a new consumer principle which sets a higher standard than the existing principles  and  in terms of how firms need to treat retail customers,1,0
787,787,the consumer duty requires firms to play a greater and more positive role in delivering good outcomes for consumers including where a firm can determine or materially influence outcomes those who are not direct customers of the firm,1,0
788,788,the consumer principle requires firms to play a greater and more positive role in delivering good outcomes for retail customers including those who are not clients of the firm,1,0
789,789,the consumer duty also includes cross cutting rules requiring firms to act in good faith towards retail customers avoid causing foreseeable harm to retail customers and enable and support retail customers to pursue their financial objectives,1,1
790,790,in addition the consumer duty has rules relating to four key elements of the firmconsumer relationship which are instrumental in helping to drive good outcomes for customersproducts and services designed to meet the needs of retail customersproducts and services to offer fair value for retail customersfirms communications to enable retail customers to understand products and services their features and risks and the implications of any decisions customers must makefirms to provide ongoing support that meets their retail customers needs,1,0
791,791,the fca also has the power to enforce the consumer protection requirements in the consumer protection from unfair trading regulations  cputrs,1,0
792,792,these prohibit unfair commercial practices that involve misleading actions misleading omissions of relevant information or aggressive commercial practicesfootnote the subsections below outline at a high level how some of the principles and the fcas rules and guidance may be relevant to the ai risks to consumer protection discussed in chapter,1,1
793,793,potential bias and vulnerability a number of the principles may have particular relevance to this consumer protection risk includingprinciple  customers interests  a firm must pay due regard to the interests of its customers and treat them fairlyfootnoteprinciple  communication with clients  a firm must pay due regard to the information needs of its clients and communicate information to them in a way which is clear fair and not misleading,1,0
794,794,principle  customers relationships of trust  a firm must take reasonable care to ensure the suitability of its advice and discretionary decisions for any customer who is entitled to rely upon its judgment,1,0
795,795,principle  consumer duty  a firm must act to deliver good outcomes for retail customers,1,0
796,796,principle  and the consumer duty apply from  july  where firms deal with retail customers,1,0
797,797,principles  and  continue to apply to conduct outside the scope of the consumer duty,1,1
798,798,the consumer duty does not prevent firms from adopting business models with different pricing by groups for instance risk based pricing but firms would need to ensure the price charged is reasonable relative to the expected benefits ie that products and services provide fair value to retail customers including being able to justify the price offered to different groups of customers and considering those with characteristics of vulnerability or protected characteristics under the equality act,1,0
799,799,certain ai derived price discrimination strategies could breach the requirements if they result in poor outcomes for groups of retail customers,1,6
800,800,as such firms should be able to monitor explain and justify if their ai models result in differences in price and value for different cohorts of customers,1,0
801,801,firms also have to take appropriate action under prin a r to mitigate or remediate harm to existing customers and prevent harm to new customers if they identify that the product no longer providers fair value,1,0
802,802,prior to the consumer duty the fca had published its vulnerable customer guidance which sets out what firms should do to comply with their obligations under the principles and ensure they treat vulnerable customers fairly including under principle,1,0
803,803,the vulnerable customer guidance notes that customers in vulnerable circumstances may be exposed to an increased risk of harm where firms do not understand the characteristics of vulnerability of their target market and main customer base and so fail to ensure that products and services meet these needs,1,0
804,804,the fca notes in its vulnerable customer guidance that it wants to see customers in vulnerable circumstances experience outcomes as good as those for other customers and receive consistently fair treatment across the firms and sectors it regulates  where firms use of ai does not take into account the differing needs and characteristics of such customers those customers may be exposed to greater harm,1,0
805,805,potential bias and discrimination risk discriminatory decisions made through using ai systems could be a breach of the equality act  equality act which protects individuals from discrimination on the basis of nine protected characteristicsfootnote the ehrc is the body with primary responsibility for upholding equality and human rights laws in the uk,1,6
806,806,the supervisory authorities are subject to a public sector equality duty under the equality act which requires them to have due regard in the exercise of their functions to the need to eliminate discrimination and other conduct prohibited under the equality act and advance equality of opportunity and foster good relations between persons who share a relevant protected characteristic and others,1,16
807,807,they take an interest in equality issues that arise in a financial services context,1,0
808,808,for example the fcas vulnerable customer guidance notes that firms may need to have regard to the equality act and seeks similar outcomes to the equality acts anticipatory duty on reasonable adjustmentsfootnote some of the characteristics of vulnerability described in the fcas vulnerable customer guidance may overlap with protected characteristics under the equality act,1,0
809,809,discriminatory decisions by ai systems that could lead to a breach of the equality act may therefore also breach the principles or fca rules and be subject to action from the fca,1,6
810,810,the consumer duty also addresses discrimination harms by requiring firms to consider the diverse needs of their customers including the fair treatment of customers with characteristics of vulnerability and those with protected characteristics,1,6
811,811,firms will be required to monitor the outcomes their customers receive in practice and take action if they identify particular groups of customers are getting poor outcomes,1,0
812,812,firms designing products or services will need to define a target market and ensure the product or service meets the needs characteristics and objectives of the target market,1,0
813,813,the consumer duty is aligned with the detailed product governance requirements contained in the product intervention and product governance sourcebook prod,1,1
814,814,where a firms product or service is subject to the rules in prod   or  it must continue to comply with those rules in respect of that product or service and the rules under the products and services outcome of the consumer duty do not apply to the firm for that product or service,1,1
815,815,however the consumer duty as a whole is broader than the existing rules in prod satisfying the prod rules is unlikely to mean a firm meets all aspects of the consumer duty,1,1
816,816,for example firms would still need to consider elements of the consumer duty such as the consumer support outcome for their product or service and to pay appropriate regard to the nature and scale of characteristics of vulnerability that exist in the target market,1,0
817,817,financial exclusion the fcas strategy  to  sets out four overarching outcomes expected from financial services,1,0
818,818,one of these is access which includes meeting diverse consumer needs through low exclusion,1,27
819,819,principles  and  are also likely to have particular relevance to financial exclusion risks related to ai,1,0
820,820,consent and privacy where financial services firms use ai to process personal data firms will have obligations under uk data protection law,1,0
821,821,for the icos guidance on ai see explaining decisions made with ai and the guidance on ai and data protection,1,11
822,822,certain practices may also breach the principles or the fca consumer duty for instance where a firm did not present the way they would use customer data in a way that was clear fair and not misleading and used their data in ways to which they had not consented and which was potentially to their detriment,1,1
823,823,competition  fca and prarelevant legislation the fca has functions under the competition act  in relation to agreements decisions and concerted practices that prevent restrict or distort competition conduct that amounts to abuse of a dominant position and transferred eu anti trust commitments and directions as they relate to the provision of financial services in the uk or the provision of claims management services in great britain,1,0
824,824,the fca may use these functions in relation to applications of ai in financial services in the uk,1,0
825,825,market studies under fsma the fca has powers to carry out market studiesfootnote the fca also has concurrent functions with the cma to carry out market studies for financial services under the enterprise act  which may be used to assess whether features of a market including the use of ai have or may have effects adverse to the interests of consumers,1,0
826,826,following a market study whether under the enterprise act or under fsma the fca can introduce remedies that could include market wide remedies such as rulemakingfootnote or firm specific measures such as the imposition of requirements,1,0
827,827,this is particularly relevant to some of the potential ai competition risks highlighted in chapter,1,0
828,828,the fca could after completing a market study also make a market investigation reference mir to the cma for an in depth market investigation,1,0
829,829,pra secondary competition objective sco in addition to its primary objectives the pra also has a secondary competition objective to facilitate effective competition in the markets for services provided by pra authorised persons in carrying on regulated activities,1,0
830,830,the sco is set out in fsma and came into force in march,1,22
831,831,safety and soundness data  pra and fca data are a crucial element of ai,1,11
832,832,from sourcing large amounts of data and creating datasets for training testing and validating through to the continuous analysis of data once the ai system is deployed the safe and responsible ai adoption in uk financial services is underpinned by high quality data,1,0
833,833,similarly the increasing volumes of data involved in the use of ai means that data security and related issues such as data protection and subject privacy are ever more important to ensuring safe and responsible ai adoption,1,11
834,834,to the extent that ai processes personal information financial firms will need to comply with their data protection obligations,1,0
835,835,the ico has published guidance on this topic,1,27
836,836,the data protection and digital information bill includes a provision on ai and processing special category data for the purposes of monitoring ai bias,1,11
837,837,data quality sourcing and assurance poor quality or inappropriate data can potentially compromise any process that relies upon those data,1,9
838,838,the way in which data are sourced and aggregated can impact the intended outcome and the overall quality of the data,1,24
839,839,the current regulatory framework aims to address these specific risk components of the data lifecycle,1,3
840,840,for example the basel committee on banking supervisions bcbsfootnote principles for effective risk data aggregation and risk reporting bcbs  contains principles aimed at strengthening prudential risk data aggregation such as ensuring the accuracy integrity completeness timeliness and adaptability of data,1,0
841,841,the pra expects the uks globally systemically important banks in particular to adhere to these principlesfootnote with respect to insurance rule  of the technical provisions part and rule  of the conditions governing business part of the pra rulebook for solvency ii require firms to have internal processes and procedures in place to ensure the completeness accuracy and appropriateness of the data used in the calculation of their technical provisions,1,0
842,842,data privacy security and retention data security is important in ensuring information is protected from malicious threats such as unauthorised access theft and corruption,1,11
843,843,the privacy of this data and use of this data in a responsible manner is also important,1,9
844,844,most notably uk data protection legislation applies standards for data privacy and security in respect of personal data,1,11
845,845,firms also need to consider their obligations under the money laundering terrorist financing and transfer of funds information on the payer regulations,1,0
846,846,the payment services regulations  psrs are focused on security and quality of data transfers to third parties,1,0
847,847,data architecture infrastructure and resilience data architecture and infrastructure refers to the system standards and policies in which data are stored arranged and integrated,1,11
848,848,data resilience refers to the ability for data to be preserved after a failure or disruption,1,11
849,849,regulation within this area is more general requiring firms to have strong data architecture and risk management infrastructure,1,8
850,850,examples include the risk control part of the rulebook fundamental rules  and  bcbs  principles and bcbss guidelines on corporate governance principles for banks,1,0
851,851,data governance data governance can be summarised as a set of principles and practices that ensure quality throughout the lifecycle of data,1,11
852,852,regulation within this area is broader with principles such as bcbs  highlighting the governance structures andor board responsibilities and the uk gdpr setting out the accountability of controllers and processors for compliance with data protection requirements,1,2
853,853,while there appears to be some thematic overlaps eg on data quality most notably the various regulations differ in focus and scope,1,8
854,854,for instance the psrs are focused on security and quality of data transfers to third parties requirements contained within the basel committees fundamental review of the trading book revised market risk standard frtb would only apply to market risk datafootnote uk gdpr and data protection act  only apply to personal data bcbs  only applies to systemically important banks and is focused on the aggregation of risk data rules implementing solvency ii apply to technical provisions and internal model data of insurers and markets in financial instruments regulation mifir obligations apply to trade data with the aim of improving protections for investors,1,0
855,855,organisational and legacy issues make total data integration a significant challenge,1,11
856,856,the prudential regimes eg solvency ii tend to lay out high level principles on data without prescribing detailed guidelines on their implementation,1,26
857,857,safety and soundness model risk management  pra as the aippf final report notes model risk management mrm is becoming increasingly important as a primary framework for some firms to manage and mitigate potential ai related risks,1,3
858,858,however the current scope of mrm regulation in the uk is very limited with only principles and regulation in place for the use of models in specific areas or tasks eg internal capital models or stress testing models,1,8
859,859,at the same time mrm regulation does not explicitly mention ai and there is currently no explicit guidance on issues like model explainabilityinterpretability of ai models,1,14
860,860,box  pra consultation paper on mrmthe pra published cp on tuesday  june  which includes a proposed set of principles which it considers to be key in establishing an effective mrm framework,1,27
861,861,the principles are proposed to be embedded as supervisory expectations as set out in a proposed new ss and includemodel identification and model risk classificationgovernancemodel development implementation and useindependent model validationmodel risk mitigantsthe proposed principles would apply to all regulated uk incorporated banks building societies and pra designated investment firms,1,0
862,862,the pra proposes that the principles should be applied by firms in a way commensurate with their size business activities and the complexity and extent of their model use,1,0
863,863,moreover the proposed principles cover all elements of the model lifecycle and would be applicable to all types of models that are used to inform key business decisions whether developed in house or externally including vendor models and models used for financial reporting purposes,1,20
864,864,this includes ai models and the cp includes a question concerning the adequacy of the proposed principles to address the risks associated with ai models,1,5
865,865,to submit a response please go directly to cp,1,22
866,866,the consultation closes on friday  october,1,2
867,867,identification and classification while the supervisory authorities do not currently provide an explicit definition of a model the pra proposed definition is being consulted on in cp the pra expects that banks should give consideration to what factors would constitute a model and must establish their own definition of a model and furthermore banks should maintain a model inventory with clear information regarding but not limited to owners users uses and direct or material dependencies see pra ss model risk management principles for stress testing,1,20
868,868,similarly pra ss algorithmic trading expects firms to define the term algorithm section  as used by the firms in the context of algorithmic trading,1,26
869,869,effective governance framework policies procedures and controls to manage model risk effective governance provides support and structure to mrm activity through policies which define relevant risk management activities procedures that implement those policies and the allocation of resources and mechanisms for testing that policies and procedures are being carried out as specified,1,3
870,870,beyond pra rules the bcbs corporate governance principles for banks states that it is the responsibility of a banks risk management function to ensure that the board and management are aware of the assumptions used in and potential shortcomings of the banks risk models and analyses paragraph,1,0
871,871,the guidelines also stress that risk identification and measurement should include both quantitative and qualitative elements paragraph,1,3
872,872,robust model development and implementation process a robust model development process ensures models are developed to appropriate standards and use representative data which are important considerations for ai models,1,20
873,873,model validation and independent review the validation and independent review of an ai model is important in order to ensure an objective view is given on the model inclusive of the way in which it is developed and that it is suitable for the intended purpose,1,20
874,874,the iosco board an international standards setting body for securities regulation has also developed guidance for regulators on supervising the use of ai and ml by market intermediaries and asset managers specifically on the development testing and ongoing monitoring of ai techniques,1,0
875,875,for example measure  of the iosco guidance on ai states that regulators should require firms to adequately test and monitor the algorithms to validate the results of an ai technique on a continuous basis with any testing reflecting the underlying complexity and systematic risks posed by the use of ai,1,18
876,876,in particular iosco recommends the implementation of kill switch functionality in the control framework which is similar to the existing expectations and requirements for algorithmic trading in ss and commission delegated regulation eu,1,25
877,877,safety and soundness governance  pra and fca good governance is essential for supporting the safe and responsible adoption of ai,1,16
878,878,this is because governance underpins proper procedures and effective risk management across the ai lifecycle by putting in place the set of rules controls and policies for a firms use of ai,1,3
879,879,the supervisory authorities take a principles based approach to governance,1,16
880,880,principle  of the fca principles for businesses requires states that a firm must take reasonable care to organise and control its affairs responsibly and effectively with adequate risk management systems,1,16
881,881,fundamental rule  of the pras fundamental rules states that a firm must organise and control its affairs responsibly and effectively,1,1
882,882,both the fca handbook and the pra rulebook contain provisions in respect of compliance internal audit financial crime risk control outsourcing and record keeping,1,0
883,883,these provisions include sysc  and rule  of the general organisational requirements part of the pra rulebook which both state a firm must have robust governance arrangements which include a clear organisational structure with well defined transparent and consistent lines of responsibility effective processes to identify manage monitor and report the risks it is or might be exposed to and internal control mechanisms including sound administrative and accounting procedures and effective control and safeguard arrangements for information processing systems,1,0
884,884,the general rules guidance and principles will be relevant to the firms use of ai,1,8
885,885,there are also specific requirements of relevance to ai  for example mifid org regulation requires investment firms to store records in a way that it is not possible for them to be manipulated and altered other than corrections or amendments and which allows for it or other efficient exploitation of the data when the analysis of the data cannot be easily carried out due to the volume and nature of the data,1,4
886,886,board composition collective expertise and engagement concerning board composition collective expertise and engagement ss  sets out the expectations of the pra in relation to how firms should comply with the rules in the general organisational requirements skills knowledge and expertise compliance and internal audit risk control outsourcing and record keeping parts of the pra rulebook,1,19
887,887,specifically fca handbook sysc  details the role of the chief risk senior management function in challenging and ensuring the quality quantity and use of data,1,3
888,888,as highlighted in the aippf final report there may be a lack of understanding of the challenges and risks arising from the use of advanced technologies at firms senior management and board levels both individually and collectively leading to a skills and engagement gap,1,3
889,889,this may be partly a cultural issue and could lead to ineffective governance,1,25
890,890,there are requirements and expectations on firms to address this skill gap including the pra expectations set out in pra ss corporate governance board responsibilities that boards should have the diversity of experience and capacity to provide effective challenge across the full range of the firms business and boards should pay close attention to the skills of its members and the fca requirements for issuers to publish information on board diversity policies in their corporate governance statement,1,27
891,891,the pra and fca could also use a range of supervisory tools to assess how firms are addressing their potential technology knowledge gap including as part of board effectiveness reviews,1,0
892,892,where appropriate questions about the use of ai and relevant controls could be included in question banks for smf interviews,1,0
893,893,who should be accountable for ai under the smcr,1,16
894,894,the supervisory authorities existing rules and guidance in particular those implementing the smcr emphasise senior management accountability and responsibility and are relevant to the use of ai,1,0
895,895,the smr requirements are set out in the pra rulebook eg for crr firms allocation of responsibilities senior management functions etc and for sii firms some are listed under insurance  allocation of responsibilities and insurance  senior manager functions,1,0
896,896,the supervisory statements provide a source of guidance on the smcr eg pra ss strengthening individual accountability in banking and ss strengthening individual accountability in insurance set out the pras expectations on strengthening individual accountability in banking and insurance,1,0
897,897,specifically for international banks pra ss international banks the pras approach to branch and subsidiary supervision states the pra expectations on the accountability of senior management functions smf for branches and subsidiaries,1,0
898,898,within the smcr there is at present no dedicated smf for ai,1,0
899,899,currently technology systems are the responsibility of the smf chief operations function  see the pra rulebook senior management functions part rule  and sup cb systems and controls functions other,1,26
900,900,separately the smf chief risk function has responsibility for overall management of the risk controls of a firm including the setting and managing of its risk exposures,1,3
901,901,these functions apply to pra authorised smcr banking and insurance firms and fca authorised enhanced scope smcr firms but not core or limited scope smcr firms,1,0
902,902,pra authorised smcr banking and insurance firms and fca authorised enhanced scope smcr firms must ensure that one or more of their smf managers have overall responsibility for each of the activities business areas and management functions of the firm,1,0
903,903,that means any use of ai in relation to an activity business area or management function of a firm would fall within the scope of a smf managers responsibilities,1,4
904,904,for banks the general organisational requirements part  states that firms must have robust governance arrangements which include a clear organisational structure with well defined transparent and consistent lines of responsibility and effective control and safeguard arrangements for information processing systems,1,26
905,905,part  lays out the board responsibilities stating that the board oversees and is accountable for the implementation of governance arrangements the prudent management of the firm including the segregation of duties in the organisation and the prevention of conflicts of interest,1,16
906,906,and that the management body approves and oversees implementation of the firms strategic objectives risk strategy and internal governance,1,16
907,907,the guidance from iosco measure   governance and responsibilities states that regulators should consider requiring firms to have designated senior management responsible for the oversight of ai development testing deployment monitoring and controls,1,8
908,908,this includes a documented internal governance framework with clear lines of accountability,1,16
909,909,the guidance also states that firms should designate an appropriately senior individual or groups of individuals with the relevant skill set and knowledge to sign off on initial deployment and substantial updates of the technology,1,27
910,910,this measure looks to embed accountability in all aspects of a firms use of ai and ml and helps ensure the technology and its underlying data is appropriately understood tested deployed and monitored,1,18
911,911,furthermore this accountability extends to the actions and outcomes of ai and ml models including externally sourced models,1,16
912,912,as the aippf final report noted a key question for all firms is who should ultimately be responsible for ai and whether this should be a single individual or shared between several senior managers,1,16
913,913,a key question therefore will be how firms identify the relevant smfs with responsibility for the use of ai in the business,1,0
914,914,therefore the most appropriate smfs may depend on the organisational structure of the firm its risk profile and the areas or use cases where ai is deployed within the firm,1,3
915,915,this is without prejudice to the collective responsibility of boards and the respective responsibilities of each of the three lines of defence,1,16
916,916,looking ahead there is a question as to whether there should be a dedicated smf andor a prescribed responsibility pr for ai under the smcr,1,27
917,917,arguably ai use may not have yet reached a level of materiality or pervasiveness to justify these changes,1,4
918,918,however the aippf highlighted that the split in responsibilities is an area of uncertainty for firms and that more guidance on governance functions roles and responsibilities would help provide clarity,1,23
919,919,given the technical complexity of ai systems it is important that staff responsible for developing or deploying them are competent to do so,1,4
920,920,one possibility for ensuring this could be a new certification function for ai similar to the fcas certification function for algorithmic trading,1,0
921,921,the algorithmic trading certification function extends to persons who  approve the deployment of trading algorithms ii approve the amendment to trading algorithms and iii have significant responsibility for the management of the monitoring or decide whether or not trading algorithms are compliant with a firms obligations,1,0
922,922,pra ss algorithmic trading sets out expectations for governance eg cross lines of defence coordination smfs testing with regard to the use of algorithms in the context of trading,1,26
923,923,ai lifecycle one useful approach to understanding firms obligations is to look at them from the perspective of the ai lifecycle,1,0
924,924,for examplepre deployment how should the quality of training data be assessed,1,18
925,925,how should ai models be tested before live deployment,1,20
926,926,should ai models be compliant by design,1,4
927,927,who is the accountable smf,1,0
928,928,identification of and allocation of responsibility for new risks presented by ai,1,3
929,929,deployment how should the performance of live ai systems be monitored,1,20
930,930,what safeguards should be introduced to monitor for detect and stop potential harm eg kill switch mechanisms,1,25
931,931,recovery and redress if an ai systems performance leads to crystallised risks should firms be required or expected to undo the damage by  reversing decisions made by the model where possible and appropriate andor ii compensating any relevant external parties who suffered damage as a result,1,10
932,932,ai and reasonable steps the concept of reasonable steps is a core element of the smcr,1,21
933,933,smfs can be subject to enforcement action under sa andor sb of fsma if an area of the firm for which the smf has responsibility breaches regulatory requirements and the fca andor pra can demonstrate that they failed to take such steps as a person in the senior managers position could reasonably be expected to take to prevent or stop these breaches,1,0
934,934,one of the areas that could benefit the most from further discussion is what may constitute reasonable steps in an ai context and how if at all these steps differ from the reasonable steps that smfs are generally required to take,1,21
935,935,pra ss ss and depp  deciding whether to take action which are the key reference sources on the reasonable steps criterion under the smcr have detailed but not exhaustive expectations on what may constitute reasonable steps and on how firms and smfs can document and evidence them,1,21
936,936,this guidance built on the pras fcas and fsas prior enforcement activity and supervisory experience and was issued at a time when autonomous decision making technology such as ai was not as widespread and as a result the guidance does not explicitly refer to such technology,1,0
937,937,a particularly useful approach could be to consider what may constitute reasonable steps at each successive stage of the lifecycle of a typical ai system,1,4
938,938,human in the loop an important element in the operation of an ai system is the level of human involvement in the decision loop,1,4
939,939,humans typically interact with a system in the design and training stages and may also be involved in operating the system and interpreting outputs,1,15
940,940,the human element can act as a valuable safeguard against harmful outcomes providing contextual knowledge that may be outside the capability of a model and identifying where an automated decision could be problematic and therefore requiring further review,1,15
941,941,consumers and others affected by the decisions made by automated systems may feel uncomfortable where important decisions are made without a human in the loop,1,15
942,942,in certain contexts human involvement can be a regulatory requirement for instance under the uk gdpr automated decisions are treated differently to human decisions,1,21
943,943,article  of the uk gdpr restricts fully automated decisions which have legal or similarly significant effects on individuals to a more limited set of lawful bases and requires certain safeguards to be in place,1,21
944,944,the ico has explained that the human input needs to be meaningful and a decision does not fall outside article  just because a human has rubber stamped it,1,21
945,945,the uk government is planning to reform these laws including amending the right to human intervention in the data protection and digital information bill which is currently at the second reading stage in the house of commons,1,8
946,946,it is worth noting there are limits to the effectiveness of human involvement in an automated decision,1,21
947,947,some reviewers may be subject to automation bias where they simply accept automated recommendations or may be unable to effectively interpret the outputs of complex systems and falsely reject an accurate output,1,15
948,948,however we think firms deploying ai systems need to have a sufficiently strong set of oversight and governance arrangements that make effective use of humans in the decision making loop and review the accuracy of those arrangements,1,8
949,949,safety and soundness operational resilience outsourcing and third party risk management  pra and fca since  the supervisory authorities have developed and implemented a coordinated regulatory and supervisory framework to strengthen the operational resilience of the uk financial services sector see joint covering document operational resilience impact tolerances for important business services,1,0
950,950,operational resilience refers to the ability of firms and the financial services sector as a whole to prevent adapt to respond to recover from and learn from operational disruptions,1,3
951,951,operational resilience applies to the use of ai by firms and fmis when it supports an important business service,1,0
952,952,this means firms and fmis should set an impact tolerance for disruption for each of those important business services that involve ai and ensure they are able to remain within their impact tolerances for each important business service in the event of a severe or in the case of fmis extreme but plausible disruption,1,17
953,953,many of the principles expectations and requirements for operational resilience may provide a useful basis for the management of certain risks posed by ai and support its safe and responsible adoption,1,3
954,954,for example developing and implementing effective business continuity and contingency plans for ai systems that support an important business service,1,0
955,955,as the aippf final report noted backup and remediation actions being considered before the model is put into production can enable firms to respond appropriately and in a timely manner if an ai model performance or output deteriorates beyond the accepted risk threshold which can help manage risks,1,20
956,956,it is also worth noting firms and fmis are expected to andor required to meet applicable operational resilience requirements and expectations irrespective of whether the ai is developed in house or by third parties,1,25
957,957,therefore the supervisory authorities requirements for outsourcing and third party risk management see sysc    and  of the fca handbook for crr firms the outsourcing part and for solvency ii firms chapter  of the conditions governing business part of the pra rulebook and pra ss also apply to third party ai models used by firms,1,0
958,958,most recently the supervisory authorities set out potential measures to oversee and strengthen the resilience of services provided by critical third parties ctps to firms and fmis,1,0
959,959,the discussion in dp operational resilience critical third parties to the uk financial sector explored measures that could complement not replace firms and fmis existing expectations andor requirements to manage potentials risks to their operational resilience including as a result of the impact of the failure or disruption of a third party,1,3
960,960,the supervisory authorities note that certain third parties providing data and ai models could emerge as future potential ctps as a result of the increasing use of these data and models,1,8
961,961,ai standards industry technical standards such as those issued by the international organization for standardization and the international electrotechnical commission can form part of a route to establishing common best practice for the development deployment and use of ai systems as well as a way for firms to signal their ai systems meet a certain benchmark of quality,1,18
962,962,standards can also potentially complement the regulatory system by helping firms build trust among users that their systems meet widely accepted standards which may go above satisfying minimum requirements for regulatory compliance,1,8
963,963,introductionartificial intelligence ai systems are poised to have a significant impact on the lives of canadians and the operations of canadian businesses,2,4
964,964,in june  the government of canada tabled the artificial intelligence and data act aida as part of bill c  the digital charter implementation act,2,2
965,965,the aida represents an important milestone in implementing the digital charter and ensuring that canadians can trust the digital technologies that they use every day,2,2
966,966,the design development and use of ai systems must be safe and must respect the values of canadians,2,16
967,967,the framework proposed in the aida is the first step towards a new regulatory system designed to guide ai innovation in a positive direction and to encourage the responsible adoption of ai technologies by canadians and canadian businesses,2,2
968,968,the government intends to build on this framework through an open and transparent regulatory development process,2,8
969,969,consultations would be organized to gather input from a variety of stakeholders across canada to ensure that the regulations achieve outcomes aligned with canadian values,2,27
970,970,the global interconnectedness of the digital economy requires that the regulation of ai systems in the marketplace be coordinated internationally,2,8
971,971,canada has drawn from and will work together with international partners  such as the european union eu the united kingdom and the united states us  to align approaches in order to ensure that canadians are protected globally and that canadian firms can be recognized internationally as meeting robust standards,2,2
972,972,ai is a powerful enabler and canada has a leadership role in this significant technology area,2,4
973,973,that is why the governments proposed approach in this area has attracted a lot of attention,2,17
974,974,this document aims to reassure canadians in two key ways,2,27
975,975,first the government recognizes that canadians have concerns about the risks associated with this emerging technology and need to know that the government has a plan to ensure that ai systems that impact their lives are safe,2,2
976,976,the recently published report of the public awareness working group of the advisory council on ai reveals significant interest among canadians in the opportunities offered by ai but also concerns regarding potential harms,2,2
977,977,nearly two thirds of respondents believed that ai has the potential to cause harm to society while  believed that it could be trusted if regulated by public authorities,2,2
978,978,thus we aim to reassure canadians that we have a thoughtful plan to manage this emerging technology and maintain trust in a growing area of the economy,2,27
979,979,at the same time ai researchers and innovators are concerned by the uncertainty that exists regarding future regulation,2,8
980,980,recognizing that the regulation of this powerful technology is now an emerging international norm many in the field are worried that regulation will be inflexible or that it will unfairly stigmatize their field of work,2,8
981,981,such an outcome would have significant impacts on opportunities for canadians and the canadian economy,2,17
982,982,this document aims to reassure actors in the ai ecosystem in canada that the aim of this act is not to entrap good faith actors or to chill innovation but to regulate the most powerful uses of this technology that pose the risk of harm,2,2
983,983,specifically this paper is intended to address both of these sets of concerns and provide assurance to canadians that the risks posed by ai systems will not fall through the cracks of consumer protection and human rights legislation while also making it clear that the government intends to take an agile approach that will not stifle responsible innovation or needlessly single out ai developers researchers investors or entrepreneurs,2,2
984,984,what follows is a roadmap for the aida explaining its intent and the governments key considerations for operationalizing it through future regulations,2,17
985,985,it is intended to build understanding among stakeholders and canadians on the proposed legislation as well to support parliamentary consideration of the bill,2,17
986,986,canada and the global artificial intelligence ai landscapecanada is a world leader in the field of artificial intelligence,2,2
987,987,it is home to  public ai research labs  ai incubators and accelerators  groups of ai investors from across the country and over  ai related start up businesses,2,2
988,988,canadians have also played key roles in the development of ai technology since the s,2,4
989,989,canada was the first country in the world to create a national strategy for ai releasing it in  and is a co founding member of the global partnership on ai gpai,2,2
990,990,the federal government has allocated a total of  million cad to advance research and innovation in the ai field develop a skilled talent pool as well as to develop and adopt industry standards for ai systems as part of the national strategy for ai,2,2
991,991,these investments have been instrumental in the development of the pan canadian ai strategy to position canada as a leading global player in ai research and commercialization,2,2
992,992,revenues from the global artificial intelligence market have been growing in recent years and are expected to surpass  billion in,2,0
993,993,market research has projected that the global ai market will reach a size of  trillion cad by  and suggests the market could grow to over  trillion cad by,2,0
994,994,artificial intelligence enables computers to learn to complete complex tasks such as generating content or making decisions and recommendations by recognizing and replicating patterns identified in data,2,4
995,995,over the last  years the capabilities of ai systems have advanced significantly to the point where they are able to perform tasks that previously required human intelligence such as identifying and modifying images performing translation and generating creative content,2,4
996,996,ai systems are increasingly being used to make important predictions or decisions about people such as with regard to credit hiring and digital services,2,0
997,997,ai systems are being developed and used in canada today for a variety of applications that add value to the canadian economy and improve the lives of canadians,2,4
998,998,technology that seemed unthinkable just a short time ago is now a part of everyday life,2,22
999,999,ai offers a multitude of benefits for canadians among which areenabling advances in healthcare such as cancer screenings and improving at home healthcare services improving agriculture precision harvesting and improving energy supply chain efficiency introducing new smart products and personalized servicesincreasing the capabilities of language processing technologies including translation and text to speech andenhancing citizens abilities to find and process information,2,4
1000,1000,why now is the time for a responsible ai framework in canadain the digital economy uses of ai are quickly becoming ubiquitous,2,16
1001,1001,as its capabilities and scale of deployment expand it is important for standards to emerge for businesses and the public to have clear expectations regarding how the technology needs to be managed,2,0
1002,1002,absent clear standards it is difficult for consumers to trust the technology and for businesses to demonstrate that they are using it responsibly,2,0
1003,1003,while many ai systems have the potential to change lives for the better high profile incidents of harmful or discriminatory outcomes have contributed to an erosion of trust for examplea resume screening ai system used by a large multinational company to shortlist candidates for interviews was found to discriminate against women,2,6
1004,1004,an analysis of well known facial recognition systems showed evidence of bias against women and people of color,2,6
1005,1005,ai systems have been used to create deepfake images audio and video that can cause harm to individuals,2,12
1006,1006,the increasing importance and prevalence of ai technology across industries today as well as growing public concern regarding both impacts on individuals and potential systemic impacts has led to rapid international mobilization around the need to guide and govern ai,2,2
1007,1007,since  a draft ai act has been introduced in the european union the united kingdom has published a proposal for regulating ai and the united states has published its blueprint for an ai bill of rights,2,2
1008,1008,if canadas advanced data economy is to thrive it needs a corresponding framework to enable citizen trust encourage responsible innovation and remain interoperable with international markets,2,0
1009,1009,canadas approach and consultation timelinecanada already possesses robust legal frameworks that apply to many of the uses of ai,2,8
1010,1010,the personal information protection and electronic documents act provides important guardrails around how businesses use personal information,2,11
1011,1011,the government has proposed the consumer privacy protection act as part of bill c  to modernize this law in the context of the digital economy and it is also undertaking broader efforts to ensure that laws governing marketplace activities and communications services keep pace,2,11
1012,1012,in addition a number of other frameworks for consumer protection human rights and criminal law apply to the use of ai includingthe canada consumer product safety actthe food and drugs actthe motor vehicle safety actthe bank actthe canadian human rights act and provincial human rights laws andthe criminal code,2,12
1013,1013,indeed existing consumer protection regulators are already moving to address some of the impacts of ai within their legislative authorities,2,8
1014,1014,for example health canada has issued guiding principles for the development of medical devices that use machine learning and the office of the superintendent of financial institutions is working on updating its model risk guidelines to account for the use of new technologies including ai,2,2
1015,1015,human rights commissions are also moving to understand the implications of ai for discrimination and other human rights issues,2,6
1016,1016,however the government is cognizant that developments in ai have created regulatory gaps that must be filled in order for canadians to trust the technology,2,8
1017,1017,for examplemechanisms such as human rights commissions provide for redress in cases of discrimination however individuals subject to ai bias may never be aware that it has occurredgiven the wide range of uses of ai systems throughout the economy many sensitive use cases do not fall under existing sectoral regulators andthere is a need for minimum standards as well as greater coordination and expertise to ensure consistent protections for canadians across use contexts,2,6
1018,1018,in this context the government has developed a framework intended to ensure the proactive identification and mitigation of risks in order to prevent harms and discriminatory outcomes while recognizing the unique nature of ai ecosystem and ensuring that research and responsible innovation are supported,2,16
1019,1019,as the technology evolves new capabilities and uses of ai systems will emerge and canada needs an approach that can adapt to the shifting landscape,2,2
1020,1020,the government will take an agile approach to ai regulation in the coming years by developing and evaluating regulations and guidelines in close collaboration with stakeholders on a regular cycle and adapting enforcement to the needs of the changing environment,2,8
1021,1021,implementation of the initial set of aida regulations is expected to take the following pathconsultation on regulations  monthsdevelopment of draft regulations  monthsconsultation on draft regulations  monthscoming into force of initial set of regulations  monthsthis would provide a period of at least two years after bill c  receives royal assent before the new law comes into force meaning that the provisions of aida would come into force no sooner than,2,17
1022,1022,how the artificial intelligence and data act would workthe aida is intended to protect canadians ensure the development of responsible ai in canada and to prominently position canadian firms and values in global ai development,2,2
1023,1023,the risk based approach in aida including key definitions and concepts was designed to reflect and align with evolving international norms in the ai space  including the eu ai act the organization of economic co operation and development oecd ai principles  and the us national institute of standards and technology nist risk management framework rmf  while integrating seamlessly with existing canadian legal frameworks,2,3
1024,1024,for example the definition of artificial intelligence systems in aida aligns with concepts developed through the oecd that are also represented in the eu ai act,2,26
1025,1025,inter operability with legal frameworks in other jurisdictions would also be a key consideration in the development of regulations in order to facilitate canadian companies access to international markets,2,8
1026,1026,the aida proposes the following approachbuilding on existing canadian consumer protection and human rights law aida would ensure that high impact ai systems meet the same expectations with respect to safety and human rights to which canadians are accustomed,2,17
1027,1027,regulations defining which systems would be considered high impact as well as specific requirements would be developed in consultation with a broad range of stakeholders to ensure that they are effective at protecting the interests of the canadian public while avoiding imposing an undue burden on the canadian ai ecosystem,2,17
1028,1028,the minister of innovation science and industry would be empowered to administer and enforce the act to ensure that policy and enforcement move together as the technology evolves,2,8
1029,1029,an office headed by a new ai and data commissioner would be created as a centre of expertise in support of both regulatory development and administration of the act,2,8
1030,1030,the role would undergo gradual evolution of the functions of the commissioner from solely education and assistance to also include compliance and enforcement once the act has come into force and ecosystem adjusted,2,19
1031,1031,prohibit reckless and malicious uses of ai that cause serious harm to canadians and their interests through the creation of new criminal law provisions,2,8
1032,1032,the aida would ensure accountability for risks associated with high impact ai systems used in the course of international and interprovincial trade and commerce,2,17
1033,1033,it identifies activities involved in the lifecycle of a high impact ai system and imposes obligations for businesses carrying out those activities in order to ensure accountability at each point where risk may be introduced,2,17
1034,1034,high impact ai systems considerations and systems of interestunder aida the criteria for high impact systems would be defined in regulation in order to allow for precision in the identification of systems that need to be regulated through this framework for inter operability with international frameworks such as the eu ai act and for updates to occur as the technology advances,2,17
1035,1035,this would enable the government to avoid imposing undue impacts on the ai ecosystem,2,17
1036,1036,the government considers the following to be among the key factors to be examined in determining which ai systems would be considered to be high impactevidence of risks of harm to health and safety or a risk of adverse impact on human rights based on both the intended purpose and potential unintended consequencesthe severity of potential harmsthe scale of usethe nature of harms or adverse impacts that have already taken placethe extent to which for practical or legal reasons it is not reasonably possible to opt out from that systemimbalances of economic or social circumstances or age of impacted persons andthe degree to which the risks are adequately regulated under another law,2,3
1037,1037,would the aida impact access to open source software or open access ai systems,2,2
1038,1038,an ai system generally requires a model as well as the use of datasets to train the model to perform certain tasks,2,4
1039,1039,it is common for researchers to publish models or other tools as open source software which can then be used by anyone to develop ai systems based on their own data and objectives,2,2
1040,1040,as these models alone do not constitute a complete ai system the distribution of open source software would not be subject to obligations regarding making available for use,2,25
1041,1041,however these obligations would apply to a person making available for use a fully functioning high impact ai system including if it was made available through open access,2,5
1042,1042,the government is cognizant that the impacts of ai systems depend on their capabilities and the contexts in which they are used,2,4
1043,1043,the following are examples of systems that are of interest to the government in terms of their potential impactsscreening systems impacting access to services or employmentthese ai systems are intended to make decisions recommendations or predictions for purposes relating to access to services such as credit or employment,2,17
1044,1044,they carry the potential of producing discriminatory outcomes and economic harm particularly for women and other historically marginalized groups,2,6
1045,1045,biometric systems used for identification and inferencecertain ai systems use biometric data to make predictions about people for example identifying a person remotely or making predictions about the characteristics psychology or behaviours of individuals,2,12
1046,1046,such systems have the potential to have significant impacts on mental health and autonomy,2,15
1047,1047,systems that can influence human behaviour at scaleapplications such as ai powered online content recommendation systems have been shown to have the ability to influence human behavior expression and emotion on a large scale,2,4
1048,1048,the potential impacts of these systems include harm to psychological and physical health,2,17
1049,1049,systems critical to health and safetycertain ai applications are integrated in health and safety functions for example making critical decisions or recommendations on the basis of data collected from sensors,2,4
1050,1050,these include autonomous driving systems and systems making triage decisions in the health sector,2,15
1051,1051,these ai systems have the potential to cause direct physical harm while bias may also result if risks have not been adequately mitigated,2,3
1052,1052,individual harms collective harms and biased outputthe aida addresses two types of adverse impacts associated with high impact ai systems,2,17
1053,1053,first it addresses a range of harms to individuals,2,17
1054,1054,second it is the first legal framework in canada to address the adverse impacts due to systemic bias in ai systems in a commercial context,2,2
1055,1055,harm includes physical harm psychological harm damage to property or economic loss to an individual,2,10
1056,1056,it is intended to encapsulate a broad range of adverse impacts that may result across the sectors of the economy,2,17
1057,1057,harms may be experienced by individuals independently or may be experienced broadly across groups of individuals increasing the severity of the impact,2,17
1058,1058,for example more vulnerable groups such as children may face greater risk of harm from a high impact ai system and necessitate specific risk mitigation efforts,2,17
1059,1059,under the aida biased output occurs when there is unjustified and adverse differential impact based on any of prohibited grounds for discrimination in the canadian human rights act,2,6
1060,1060,this includes differentiation that occurs directly or indirectly such as through variables that act as a proxies for prohibited grounds,2,1
1061,1061,adverse differentiation could be considered justified if it is unavoidable in the context of real world factors affecting a decision or recommendationfor example individual income often correlates with the prohibited grounds such as race and gender but income is also relevant to decisions or recommendations related to credit,2,1
1062,1062,the challenge in this instance is to ensure that a system does not use proxies for race or gender as indicators of creditworthiness,2,6
1063,1063,for example if the system amplifies the underlying correlation or produces unfair results for specific individuals based on the prohibited grounds this would not be considered justified,2,6
1064,1064,how does the aida protect canadians from collective harms,2,17
1065,1065,currently individuals have recourse for discrimination under the canadian human rights act chra or provincial human rights legislation,2,6
1066,1066,however some uses of ai systems may pose risks of causing harm to historically marginalized communities on a large scale if not properly assessed for bias,2,6
1067,1067,the aida would address this risk by requiring businesses conducting regulated activities to proactively assess and mitigate the risk of bias on grounds prohibited in the chra,2,17
1068,1068,regulatory requirementsaida would require that appropriate measures be put in place to identify assess and mitigate risks of harm or biased output prior to a high impact system being made available for use,2,17
1069,1069,this is intended to facilitate compliance by setting clear expectations regarding what is required at each stage of the lifecycle,2,27
1070,1070,the obligations for high impact ai systems would be guided by the following principles which are intended to align with international norms on the governance of ai systemshuman oversight  monitoringhuman oversight means that high impact ai systems must be designed and developed in such a way as to enable people managing the operations of the system to exercise meaningful oversight,2,16
1071,1071,this includes a level of interpretability appropriate to the context,2,14
1072,1072,monitoring through measurement and assessment of high impact ai systems and their output is critical in supporting effective human oversight,2,15
1073,1073,transparencytransparency means providing the public with appropriate information about how high impact ai systems are being used,2,17
1074,1074,the information provided should be sufficient to allow the public to understand the capabilities limitations and potential impacts of the systems,2,14
1075,1075,fairness and equityfairness and equity means building high impact ai systems with an awareness of the potential for discriminatory outcomes,2,6
1076,1076,appropriate actions must be taken to mitigate discriminatory outcomes for individuals and groups,2,6
1077,1077,safetysafety means that high impact ai systems must be proactively assessed to identify harms that could result from use of the system including through reasonably foreseeable misuse,2,3
1078,1078,measures must be taken to mitigate the risk of harm,2,3
1079,1079,accountabilityaccountability means that organizations must put in place governance mechanisms needed to ensure compliance with all legal obligations of high impact ai systems in the context in which they will be used,2,16
1080,1080,this includes the proactive documentation of policies processes and measures implemented,2,17
1081,1081,validity  robustnessvalidity means a high impact ai system performs consistently with intended objectives,2,18
1082,1082,robustness means a high impact ai system is stable and resilient in a variety of circumstances,2,4
1083,1083,businesses would be expected to institute appropriate accountability mechanisms to ensure compliance with their obligations under the act,2,16
1084,1084,they would be held accountable for the creation and enforcement of appropriate internal governance processes and policies to achieve compliance with the aida,2,17
1085,1085,measures would be set through regulation and would be tailored to the context and risks associated with specific regulated activities in the lifecycle of a high impact ai system,2,17
1086,1086,the regulated activities laid out in the aida would then be associated with distinct obligations that are proportionate to the risk,2,17
1087,1087,it is important to note that activities such as research or the development of methodologies are not in themselves regulated activities under aida,2,26
1088,1088,depending on the specific context and value chain configuration multiple businesses could be involved in carrying out regulated activities for a single ai system,2,0
1089,1089,the specific measures required by regulation would be developed through extensive consultation and would be based on international standards and best practices in order to avoid undue impacts on innovation,2,8
1090,1090,under aida businesses putting in place such measures would have to monitor compliance with the measures as well as their effectiveness,2,17
1091,1091,the regulations that would follow the royal assent of the aida would ensure that responsibilities for monitoring would be proportionate to the level of influence that an actor has on the risk associated with the system,2,17
1092,1092,how do i know what my company is responsible for,2,24
1093,1093,currently there are no clear accountabilities in canada for what businesses should do to ensure that high impact ai systems are safe and non discriminatory,2,17
1094,1094,under the aida businesses conducting regulated activities would be accountable for ensuring that employees implement measures to address risks associated with high impact ai systems,2,17
1095,1095,businesses who design or develop a high impact ai system would be expected to take measures to identify and address risks with regards to harm and bias document appropriate use and limitations and adjust the measures as needed,2,17
1096,1096,businesses who make a high impact ai system available for use would be expected to consider potential uses when deployed and take measures to ensure users are aware of any restrictions on how the system is meant to be used and understand its limitations,2,17
1097,1097,businesses who manage the operations of an ai system would be expected to use ai systems as indicated assess and mitigate risk and ensure ongoing monitoring of the system,2,5
1098,1098,for example certain ai systems perform generally applicable functions  such as text audio or video generation  and can be used in a variety of different contexts,2,4
1099,1099,as end users of general purpose systems have limited influence over how such systems function developers of general purpose systems would need to ensure that risks related to bias or harmful content are documented and addressed,2,15
1100,1100,for example businesses involved only in the design or development of a high impact ai system but with no practical ability to monitor the system after the development would have different obligations from those managing its operations,2,25
1101,1101,individual employees would not be expected to be responsible for obligations associated with the business as a whole,2,1
1102,1102,in addition to obligations associated with risk assessment and mitigation businesses responsible for regulated activities associated with a high impact system would also be required to notify the minister if a system causes or is likely to cause material harm,2,17
1103,1103,the table below illustrates the types of measures that could apply at each stage of the lifecycle of an ai system,2,4
1104,1104,the design and development requirements would need to be met before a high impact system is made available for use,2,17
1105,1105,regulated activityexamples of measures to assess and mitigate risksystem design  includes determining ai system objectives and data needs methodologies or models based on those objectives,2,3
1106,1106,performing an initial assessment of potential risks associated with the use of an ai system in the context and deciding whether the use of ai is appropriateassessing and addressing potential biases introduced by the dataset selectionassessing the level of interpretability needed and making design decisions accordinglysystem development  includes processing datasets training systems using the datasets modifying parameters of the system developing and modifying methodologies or models used in the system or testing the system,2,23
1107,1107,documenting datasets and models usedperforming evaluation and validation including retraining as neededbuilding in mechanisms for human oversight and monitoringdocumenting appropriate uses and limitationsmaking a system available for use  deployment of a fully functional system whether by the person who developed it through a commercial transaction through an application programming interface api or by making the working system publicly available,2,15
1108,1108,keeping documentation regarding how the requirements for design and development have been metproviding appropriate documentation to users regarding datasets used limitations and appropriate usesperforming a risk assessment regarding the way the system has been made availablemanaging the operations of a system  supervision of the system while in use including beginning or ceasing its operation monitoring and controlling access to its output while it is in operation altering parameters pertaining to its operation in context,2,15
1109,1109,logging and monitoring the output of the system as appropriate in the contextensuring adequate monitoring and human oversightintervening as needed based on operational parametersoversight and enforcementin the initial years after it comes into force the focus of aida would be on education establishing guidelines and helping businesses to come into compliance through voluntary means,2,17
1110,1110,the government intends to allow ample time for the ecosystem to adjust to the new framework before enforcement actions are undertaken,2,22
1111,1111,the minister of innovation science and industry would be responsible for administration and enforcement of all parts of the act that do not involve prosecutable offences,2,16
1112,1112,in addition the aida would create a new statutory role for an ai and data commissioner who would support the minister in carrying out these responsibilities,2,17
1113,1113,codifying the role of the commissioner would separate the functions from other activities within ised and allow the commissioner to build a centre of expertise in ai regulation,2,8
1114,1114,in addition to administration and enforcement of the act the commissioners work would include supporting and coordinating with other regulators to ensure consistent regulatory capacity across different contexts as well as tracking and studying of potential systemic effects of ai systems in order to inform administrative and policy decisions,2,8
1115,1115,this model was chosen in careful consideration of a number of factors given the unique ai regulatory context and the objectives of the regulatory scheme,2,20
1116,1116,the governance and regulation of ai is an emerging area which will evolve rapidly in the coming years,2,8
1117,1117,as a result administration and enforcement decisions have important implications for policy and the two functions would need to be work in close collaboration in the early years of the framework under the direction of the minister,2,17
1118,1118,the minister would have powers to ensure the safety of canadians,2,1
1119,1119,in cases where a system could result in harm or biased output or where a contravention may have occurred they may take actions such asorder the production of records to demonstrate compliance ororder an independent audit,2,10
1120,1120,in cases where there is a risk of imminent harm the minister may take actions such asorder cessation of use of a system ordisclose publicly information regarding contraventions of the act or for the purpose of preventing harm,2,17
1121,1121,how would the three different enforcement mechanisms be used,2,11
1122,1122,aida provides for two types of penalties for regulatory non compliance  administrative monetary penalties amps and prosecution of regulatory offences  as well as a separate mechanism for true criminal offences,2,17
1123,1123,amps are a flexible compliance tool that could be used directly by the regulator in response to any violation in order to encourage compliance with the obligations of the act,2,17
1124,1124,while the act allows for the creation of an amps regime it would require regulations and consultations to come into force,2,17
1125,1125,regulatory offences could be prosecuted in more serious cases of non compliance with regulatory obligations,2,8
1126,1126,due to the seriousness of the process guilt must be proven beyond a reasonable doubt and a firm could defend itself by demonstrating that it had shown due care in complying with its obligations,2,10
1127,1127,the minister would not have any influence on whether to prosecute an offence and the public prosecution service of canada would need to determine that a prosecution is in the public interest before it could proceed,2,26
1128,1128,for example a firm could be prosecuted for committing a regulatory offence if it refused to comply with a regulatory obligation even after receiving a ministerial order under aida,2,1
1129,1129,true criminal offences are separate from the regulatory obligations in aida and relate only to prohibiting knowing or intentional behaviour where a person causes serious harm,2,12
1130,1130,for example a person could be prosecuted if they made an ai system available that caused serious harm and they were aware that it was likely to cause such harm and did not take reasonable measures to prevent it,2,25
1131,1131,it is important to keep in mind that the minister would have no role in determining who should be prosecuted under the act,2,12
1132,1132,the minister would only have the ability to refer cases to the public prosecution service of canada which could choose at their discretion whether or not to proceed,2,26
1133,1133,the ministers regulatory powers could not be used to investigate the criminal offences discussed in the next section which are true crimes that require criminal intent and are punishable by imprisonment,2,26
1134,1134,the aida would also mobilize external expertise in the private sector academia and civil society to ensure that enforcement activities are conducted in the context of a rapidly developing environment,2,17
1135,1135,this would occur throughthe designation of external experts as analysts to support administration and enforcement of actthe use of ai audits performed by qualified independent auditorsthe appointment of an advisory committee to provide the minister with advice,2,0
1136,1136,case example a system developed by multiple actorsconsider the case of an ai system with multiple development steps involving both research and commercial activities,2,20
1137,1137,step  a researcher at a university publishes a new model that can be used to develop ai systems  no regulatory requirements or liability under aida as it is not a commercial activity and the model alone is not a complete ai system,2,20
1138,1138,step  firm a uses this model to develop a high impact system by training it on data under their control and then place it on the market for use  firm a would need to comply with the requirements for development and making available for use which would be laid out in regulation eg testing ensuring that all measures needed for safe and fair operation are in place providing documentation to firms purchasing the system,2,20
1139,1139,if firm a did not fulfill these obligations they could be liable for penalties including amps once the scheme has been brought in through regulation,2,1
1140,1140,if firm a made the system available for use knowing that it was likely to cause serious harm they could be prosecuted for a criminal offence,2,25
1141,1141,step  firm b puts the system into operation for their own commercial purposes and manages the operations  firm b would need to comply with the requirements for managing operations eg ensuring that this use is appropriate given the risks and limitations documented by firm a monitoring the system publishing a description of the system,2,0
1142,1142,if the operating system causes harm firm b would be only liable if they did not meet the obligations related to managing operations,2,10
1143,1143,if in operating the system firm b showed reckless disregard for the safety of other persons they could be prosecuted for a criminal offence under the criminal code,2,25
1144,1144,in addition voluntary certifications can play an important role as the ecosystem is evolving,2,27
1145,1145,the ai and data commissioner would assess the progress of the ecosystem over time and ensure that administration and enforcement activities take into account the capabilities and scale of impact of regulated organizations,2,8
1146,1146,for example smaller firms would not be expected to have governance structures policies and procedures comparable to those of larger firms with a greater number of employees and a wider range of activities,2,26
1147,1147,small and medium sized businesses would also receive particular assistance in adopting the practices needed to meet the requirements,2,27
1148,1148,once the ecosystem and regulatory framework have sufficiently matured the aida does provide for the creation of an administrative monetary penalty amp scheme through regulation,2,17
1149,1149,the penalties would be designed in a proportionate manner to the objective of encouraging compliance including with respect to the relative size of firms,2,1
1150,1150,for example amps could be applied in the case of clear violations where other attempts to encourage compliance had failed,2,17
1151,1151,in addition to possible administrative penalties non compliance with the requirements would also constitute a prosecutable offence consistent with other legal frameworks intended to protect the public from harm,2,1
1152,1152,the most serious cases of non compliance could be prosecuted at the discretion of the public prosecution service of canada ppsc,2,25
1153,1153,these offences are intended to capture only those who had a responsibility to ensure that requirements were met,2,12
1154,1154,for example if a firm obstructs attempts to verify whether they have complied with their obligations or provides false or misleading information they could be subject to prosecution,2,10
1155,1155,criminal prohibitionsthe criminal code codifies most of the criminal offences in canada,2,12
1156,1156,these are behaviours that are both sufficiently harmful to canadian society and exhibit moral blameworthiness on the part of those who do them,2,12
1157,1157,consequently they carry strong punishments including imprisonment and lead to significant social stigma following conviction,2,12
1158,1158,these are distinct from regulatory non compliance offences which are primarily related to an unreasonable failure to live up to regulatory obligations,2,1
1159,1159,due to the severity of the consequences of conviction for a criminal offence prosecution of these offences requires proof beyond a reasonable doubt not only that a particular act was committed but that it was intentional,2,10
1160,1160,while many criminal code offences can apply to malicious or even grossly negligent uses of ai systems these offences are not targeted to this behaviour and their application to potential uses of the technology involves some uncertainty and novelty,2,12
1161,1161,aida creates three new criminal offences to directly prohibit and address specific behaviours of concern,2,17
1162,1162,these criminal offences are completely separate from the regulatory obligations and related offences discussed in the previous section,2,1
1163,1163,these offences of a criminal nature aim to prohibit and punish ai related activities that are done by someone who is aware of or who appreciates the harm they are causing or at risk of causing,2,12
1164,1164,the three offences are the followingknowingly possessing or using unlawfully obtained personal information to design develop use or make available for use an ai system,2,12
1165,1165,this could include knowingly using personal information obtained from a data breach to train an ai system,2,9
1166,1166,making an ai system available for use knowing or being reckless as to whether it is likely to cause serious harm or substantial damage to property where its use actually causes such harm or damage,2,21
1167,1167,making an ai system available for use with intent to defraud the public and to cause substantial economic loss to an individual where its use actually causes that loss,2,0
1168,1168,these crimes could be investigated by law enforcement and prosecuted at the discretion of public prosecution service of canadathe path aheadthe aida is one of the first national regulatory frameworks for ai to be proposed,2,8
1169,1169,it is designed to protect individuals and communities from the adverse impacts associated with high impact ai systems and to support the responsible development and adoption of ai across the canadian economy,2,17
1170,1170,it aligns with the eus draft ai act by taking a risk based approach and would be supported by industry standards developed over the coming years,2,2
1171,1171,following royal assent of bill c  the government is intending to conduct a broad and inclusive consultation of industry academia civil society and canadian communities to inform the implementation of aida and its regulations,2,2
1172,1172,this is expected to includethe types of systems that should be considered as high impactthe types of standards and certifications that should be considered in ensuring that ai systems meet the expectations of canadianspriorities in the development and enforcement of regulations including with regard to an amps schemethe work of the ai and data commissioner andthe establishment of an advisory committee,2,5
1173,1173,following this process the government would pre publish draft regulations in part  of the canada gazette and conduct another consultation for  days,2,2
1174,1174,the initial set of regulations would then be published in part  of the canada gazette,2,8
1175,1175,ised would continue to assess the effectiveness of the regulations as it administers and enforces the act,2,17
1176,1176,it would also work together with and support other regulators operating in the ai space in order to ensure that canadians are protected in a consistent and effective manner across regulatory contexts,2,8
1177,1177,on june   the minister of innovation science and industry tabled bill c   introducing updates to the federal private sector privacy regime and a new law on artificial intelligence,2,2
1178,1178,if passed the artificial intelligence and data act aida would be the first law in canada regulating the use of artificial intelligence systems,2,2
1179,1179,the stated objective of aida is to establish common requirements across canada for the design development and deployment of artificial intelligence systems that are consistent with national and international standards and to prohibit certain conduct in relation to artificial intelligence systems that may result in serious harm to individuals or their interests in each case in a manner that upholds canadian norms and values in line with principles of international human rights law,2,2
1180,1180,while the general approach in aida is apparent the full impact of the legislation will only be appreciated with the release of associated regulations which will set out most of the detailed application,2,17
1181,1181,bill c  sets out the following frameworkapproach adopts a risk based approach designed to focus on areas that create the highest risk similar to the approach found in the proposed artificial intelligence act in the eu by focusing on areas where there is greatest risk of harm and bias by establishing rules for the use of artificial intelligence systems that are high impact a term that will be defined in the regulations,2,3
1182,1182,it will be difficult to know the full impact of aida without the release of the regulations,2,17
1183,1183,scope of application the aida applies to private sector organizations that design develop or make available for use artificial intelligence systems in the course of international or interprovincial trade and commerce an area of regulation within the federal governments legislative authority,2,2
1184,1184,an artificial intelligence system is broadly defined and captures any technological system that autonomously or partly autonomously processes data related to human activities through the use of a genetic algorithm a neural network machine learning or another technique in order to generate content or make decisions recommendations or predictionsgeneral  requirementsassessment and risk mitigation measures persons responsible for artificial intelligence systems must assess whether it is a high impact system a term to be defined in the regs and establish measures to identify assess and mitigate risk of harm or biased output that could result in use of the system,2,5
1185,1185,monitoring persons responsible for high impact systems must establish measures to monitor compliance with the risk mitigation measures,2,17
1186,1186,transparency persons that make available for use or manage the operation of a high impact system must publish on a publicly available website in plain english a description ofhow the system is or intended to be usedthe types of content that it generates and the decisions recommendations or predictions that it makesthe mitigation measures established to identify assess and mitigate the risks of harm or biased output that could result from the use of the system andany other information prescribed by regulation,2,17
1187,1187,recording keeping persons that carry out a regulated activity must comply with prescribed record keeping requirements,2,12
1188,1188,notification persons responsible for high impact systems must notify the minister if use of the system results or is likely to result in material harm,2,17
1189,1189,use of anonymized data persons that carry out activities regulated by the act and who process or make available for use anonymized data in the course of the activity must in accordance with the regulations establish measures with respect to  the manner in which data is anonymized and  the usemanagement of anonymized data,2,9
1190,1190,ministerial orders the minister may by order requireproduction of recordsconduct an audit or engage an independent auditor to conduct an auditan organization that is the subject of an audit to implement any measure specified in the auditan organization responsible for a high impact system to cease using it or making it available for use if there are reasonable grounds to believe the use of the system gives rise to a serious risk of imminent harmadministration aida creates a statutory right for the minister to designate a senior official of the department over which the minister presides to be called the artificial intelligence and data commissioner whose role is to assist the minister in the administration and enforcement of aida,2,17
1191,1191,enforcementadministrative monetary penalties will be set out in regulationspenalties for contraventions of aida are significant  up to  of global revenue or c millionhigher penalties of up to  of global revenue or c million or imprisonment in the case of an individual apply for more serious offences forpossessing or using personal information obtained through criminal or other unlawful means for the purposes of creating using or making available an ai systemusing an ai system knowing or being reckless as to whether the system is likely to cause serious or psychological harm or substantial damage to property if such harm or damage occurs andusing an ai system with intent to defraud the public and cause economic loss if such loss occurs,2,17
1192,1192,what is the purpose of the aida,2,17
1193,1193,the stated purposes of the aida are toregulate international and interprovincial trade and commerce in ai systems by establishing common requirements applicable across canada for the design development and use of those systems andprohibit certain conduct in relation to ai systems that may result in serious harm to individuals or harm to their interests,2,17
1194,1194,the aida defines harm as  physical or psychological harm to an individual  damage to an individuals property or  economic loss to an individual,2,10
1195,1195,to which entities will the aida apply,2,17
1196,1196,the regulation of ai under the aida will focus on those persons carrying out a regulated activity which means any of the following in the course of international or interprovincial trade and commerceprocessing or making available for use any data relating to human activities for the purpose of designing developing or using an ai system ordesigning developing or making available for use an ai system or managing its operations,2,8
1197,1197,these references are broad such that it is easy to imagine many ai systems would fall within the meaning of a regulated activity,2,4
1198,1198,the aida imposes regulatory requirements for both ai systems generally and those ai systems specifically referred to as high impact systems which we discuss in further detail below,2,17
1199,1199,as noted above the aida limits application to interprovincial and international commerce ostensibly leaving intra provincial ai matters to the regulation of the provinces if and when they so choose,2,17
1200,1200,this is narrower than the personal information protection and electronic documents acts pipeda application to all organizations that collect use or disclose personal information in the course of commercial activities except where there is substantively similar provincial legislation,2,26
1201,1201,the aida also exempts the federal government from its application,2,26
1202,1202,most federal government entities must comply with an existing directive on automated decision making the directive the objective of which is to ensure such systems are used in a way that reduces associated risks to canadians and federal institutions,2,15
1203,1203,although the directive is limited to automated decision making it may provide some insight into the regulatory framework that will follow if the aida becomes law,2,21
1204,1204,as part of the directive government agencies must complete an algorithmic impact assessment of a system before production,2,17
1205,1205,high impact systems are assessed on various factors with impact levels varying based on the systems effect on the rights health and wellbeing of individuals or communities economic interests and sustainability of an ecosystem reversibility and duration,2,17
1206,1206,canadian regulation of ai systems in the private sectorthe degree of regulation of private sector ai systems under the aida will depend in part on whether the system falls within the definition of a high impact system with such systems being subject to a higher degree of regulation,2,8
1207,1207,as presently drafted ai systems subject to the aida will fall into one of only two categories those that are high impact systems and those that are not meaning ai systems within the scope of the aida but do not meet the definition of a high impact system,2,17
1208,1208,the differences between regulated activities and high impact systems are outlined below,2,17
1209,1209,by contrast regulatory developments in the eu some of which are discussed below consider degrees of impact,2,2
1210,1210,the federal governments directive also uses impact level definitions little or no impact to very high impact and based on the degree of impact the deputy head or treasury board will be responsible for approval of the system,2,17
1211,1211,while the treasury board may impose consequences for non compliance this is not the same as the broader enforcement powers under the aida,2,26
1212,1212,anonymized dataunder the proposed consumer privacy protection act once personal information has been anonymized the act no longer applies to such information,2,11
1213,1213,by contrast under the aida  a person who carries out a regulated activity and who processes or makes available for use anonymized data in the course of that activity will be required to establish measures with respect to  the manner in which data is anonymized and  the use or management of anonymized data,2,9
1214,1214,the aida does not define anonymized data,2,9
1215,1215,high impact systemsthe existing definition of a high impact system is vague and will be addressed by criteria to be established by regulation,2,17
1216,1216,it is possible that what constitutes a high impact system will be similar to what the eu defines as a high risk ai system or it may take some of its framework from the directive,2,17
1217,1217,at this time it is difficult to evaluate the impact and scope of the aida  and we will have to wait for the regulation to define a high impact system,2,17
1218,1218,the aida prohibits certain conduct in relation to a high impact system that may result in serious harm to individuals or biased outputs,2,17
1219,1219,the aida specifies that biased output means content that is generated or a decision recommendation or prediction that is made by an ai system and that adversely differentiates in relation to an individual on one or more of the prohibited grounds of discrimination set out in the canadian human rights act or on a combination of such prohibited grounds however biased output will not include content or a decision recommendation or prediction the purpose and effect of which are to prevent disadvantages that are likely to be suffered by or to eliminate or reduce disadvantages that are suffered by any group of individuals when those disadvantages would be based on or related to prohibited grounds,2,6
1220,1220,those responsible for an ai system will be required to assess whether the ai system is a high impact system,2,17
1221,1221,where an ai system meets this definition the person responsible mustestablish measures to identify assess and mitigate the risks of harm or biased output that could result from the use of the systemestablish measures to monitor compliance with the mitigation measures and the effectiveness of those mitigation measureswhere the system is made available for use publish on a public website a plain language description of the system that explains among other things how the system is intended to be used the types of content that it is intended to generate and the types of decisions recommendations or predictions it is intended to make and the risk mitigation measures establishedwhere the person is managing the operation of the system publish on a public website a plain language description of the system that explains among other things how the system is used the types of content that it generates and the decisions recommendations or predictions that it makes and the mitigation measures established andnotify the minister if use of the system results or is likely to result in material harm,2,5
1222,1222,audits and information sharingthe minister responsible for administering the aida will have considerable powers under the act to promote and ensure compliance,2,17
1223,1223,one of the options available to the minister to ensure compliance is the ability to conduct or direct an audit by a qualified personat the expense of the person being auditedwhere the minister believes there has been a contravention of certain sections of the act,2,0
1224,1224,unlike the existing pipeda framework which has minimal enforcement powers the potential implications of an audit for those operating ai systems are substantial,2,0
1225,1225,the minister mayrequire any person responsible for a high impact system to cease using it or making it available for use where the minister has reasonable grounds to believe that the use of the system gives rise to a serious risk of imminent harmrequire the audited person implement any measure specified in the order to address anything referred to in an audit report orrequire a person to publish on a publicly available website certain information including audit details so long as it does not require disclosure confidential business information,2,17
1226,1226,those subject to an order must comply with the order,2,22
1227,1227,as another remedial measure the minister may file a copy of any order with the federal court,2,22
1228,1228,while there are obligations on the minister to take measures to maintain confidentiality there are several circumstances in which the minister may disclose information to third parties including analysts,2,1
1229,1229,where the minister believes the information obtained may also constitute violations of certain legislation the minister may also disclose this information to the entities responsible for enforcing those statutes,2,1
1230,1230,these entities include the privacy commissioner and the canadian human rights commission or their provincial counterparts as well as the commissioner of competition and the canadian radio television and telecommunications commission,2,12
1231,1231,this enhanced information sharing can considerably expand the scope of regulatory review by government compliance and enforcement agencies,2,8
1232,1232,recordkeeping and productionpersons carrying out any regulated activity must also retain records describing in general terms the measures established with respect to the anonymized data high impact system assessments mitigation and monitoring obligations specified above,2,9
1233,1233,regulations may require retention of additional records,2,22
1234,1234,the responsible minister by order may require production of these records,2,1
1235,1235,in addition where the minister has reasonable grounds to believe that the use of a high impact system could result in harm or biased output the minister may require by order a person to provide specific records relating to that system,2,17
1236,1236,administrationin addition to limited regulation making authority the aida provides authority for the minister to designate a senior official the artificial intelligence and data commissioner whose role would be to assist the minister in the administration and enforcement of the aida,2,17
1237,1237,the minister may also designate analysts and establish an advisory committee,2,0
1238,1238,administrative penaltiesthose violating the aida can be liable for administrative monetary penalties,2,17
1239,1239,the governor in council will have authority to create an enforcement scheme giving it the power todesignate violationsclassify violations as minor serious or very seriouscommence proceedingsdefine available defencesdetermine the range and amount of administrative penalties that may be imposedregulate reviews or appeals of findings that a violation has been committed and imposition of administrative monetary penaltiesregulate compliance agreements andregulate the persons or classes of persons who may exercise any power or perform any duty or function in relation to the scheme including the designation of such persons or classes of persons by the ministeroffences related to ai systemsviolations of the aida can also constitute an offence which in turn can result in severe fines and in prescribed circumstances potential imprisonment,2,17
1240,1240,however the aida requires an election a person may be subject to an administrative penalty for a violation or more serious sanctions of an offence,2,17
1241,1241,international developments in artificial intelligence regulationcountries regions and inter governmental organizations are working on guidance standards regulation and laws specific to ai,2,8
1242,1242,notable international collaborations include the global partnership on artificial intelligence gpai which has  member countries including canada unescos work on ai and projects by standard setting organizations including the institute of electrical and electronics engineers ieee and the international telecommunication union itu,2,25
1243,1243,emerging regulation and general concerns over ai ethics may also produce a new industry offering ai assurance,2,2
1244,1244,there is broad international consensus on the key legal and regulatory challenges of ai safety including robustness of performance and cyber security transparency and explainability accountability human control bias mitigation and privacy protection see for example the european commissions ethics guidelines for trustworthy ai,2,8
1245,1245,there is less consensus on whether and how to tackle the broad potential and current social and economic effects of widespread ai adoption,2,2
1246,1246,the most advanced and far reaching proposal for cross cutting regulation of ai per se is the european unions proposed ai act,2,2
1247,1247,like the eus privacy laws the gdpr this is expected to become a de facto requirement for companies internationally the so called brussels effect,2,2
1248,1248,the draft ai act prohibits some uses of ai such as subliminal techniques and defines others as high risk such as biometric identification for which it requires special measures including with respect to risk management data governance technical documentation record keeping transparency human oversight accuracy robustness and cyber security,2,2
1249,1249,the latest draft also includes requirements for general ai systems that could be used for high risk applications,2,5
1250,1250,the ai act is not expected to come into force before  but companies should start addressing it now as it will require the development and implementation of technical solutions that may have a material effect on product and service development,2,2
1251,1251,below the level of cross cutting regulation sector specific regulators are working on frameworks for ai use,2,8
1252,1252,for example work in the uk is ongoing by among others the ministry of justice the law commission department of transport the civil aviation authority the information commissioner the competition and markets authority the financial conduct authority the medicines and healthcare products regulatory agency and the national health service,2,1
1253,1253,however the shortage of ai specialists hampers regulatory work internationally see for example this report by the uks turing institute,2,8
1254,1254,since ai is of general application across all sectors and activities all laws potentially apply to the use of ai and some particularly relating to privacy and ip are already significant considerations,2,8
1255,1255,as bill c  is only at the second reading stage there will likely be much more debate and potential amendments as the bill makes its way through parliament,2,22
1256,1256,international developments regarding the regulation of ai are likely to have a material effect on subsequent changes to the aida  and any regulations enacted after it becomes law,2,17
1257,1257,in november  the federal government tabled substantial changes to the canadian privacy landscape with the introduction of bill c  which proposed to repeal the personal information related provisions of the current federal private sector privacy law the personal information protection and electronic documents act pipeda and replace them with a new privacy and data legal framework,2,11
1258,1258,bill c  never made it into law in part because a federal election was called in,2,26
1259,1259,however on june    following last years elections and a period of suspense  the federal government resurrected bill c  with the introduction of bill c  in the house of commons,2,22
1260,1260,titled the digital charter implementation act  bill c  retains the core elements of bill c  including its proposals toenact the consumer privacy protection act cppa  which would replace part  of pipeda entitled protection of personal information in the private sector andenact the personal information and data protection tribunal act pidpta which establishes an administrative tribunal to hear appeals of certain decisions made by the privacy commissioner of canada under the cppa and impose penalties for the contravention of certain provisions of the cppa or conversely allow the appeal and in doing so substitute its own finding order or decision for that of the commissioner,2,11
1261,1261,despite these similarities there are notable differences between bills c  and c,2,26
1262,1262,a key new aspect of bill c  is how it deals with artificial intelligence ai while bill c  addressed certain issues related to the use of ai via its proposed provisions on automated decision making bill c  would vastly expand on this framework through its proposal to enact the artificial intelligence and data act aida to regulate ai systems,2,2
1263,1263,bill c  is part of a global push to strengthen privacy regulations around the world  a trend that commenced with the european unions general data protection regulation gdpr which came into effect on may   and that also includes bill  quebecs new privacy legislation,2,2
1264,1264,for more information about bill  please refer to our prior articles published on april   and june,2,2
1265,1265,this article focuses on the key distinctions between bill c  and bill c,2,26
1266,1266,for an in depth review of the changes first proposed under bill c  please refer to our prior article,2,22
1267,1267,the consumer privacy protection actthe ccpa retains the expanded powers of privacy commissioner of canada as well as the severe penalties in case of contravention,2,12
1268,1268,these includeadministrative monetary penalties of up to the higher of  per cent of gross global revenue or  millionincreased fines for certain serious contraventions of the law up to a maximum fine of the higher of  per cent of gross global revenue or  millionauditing and ordering making powers for the privacy commissioner of canada anda private right of action against an organization for damages for loss due to a contravention of the cppa,2,7
1269,1269,bill c  proposes the following additional key changespersonal information of minors is expressly considered to be sensitive personal information thereby potentially imposing more rigorous requirements with respect to appropriate purposes for its use the nature of the consent required ie,2,12
1270,1270,express vs implied the duration of the retention period the security safeguards undertaken breach notification and de identificationthe business activity exemption to consent includes an additional exception an organization may collect or use an individuals personal information without the individuals knowledge or consent for the purpose of an activity in which the organization has a  legitimate interest that outweighs the potential adverse effect on the individual subject to certain conditionsthe term anonymize is defined and it is clarified that anonymized information is not subject to the cppait clarifies that de identified information is personal information subject to a few exceptions but expands the cases where de identified information may be used to re identify an individualorganizations are required to consider the sensitivity of personal information when determining retention periods and make readily available information regarding those retention periodssecurity safeguards must include reasonable measures to authenticate the identity of the individual to whom the personal information relatesorganizations are required to dispose of personal information which can include anonymization upon the request of an individual but only in certain circumstances and where the request does not otherwise fall within an enhanced list of exceptions andwhile an organization using an automated decision system must like in the prior bill c  provide an explanation of the systems prediction recommendation or decision regarding an individual the circumstances in which such an explanation must be provided are limited to those where there could be a significant impact on the individual,2,11
1271,1271,where applicable the factors that must be addressed in the explanation have been expanded to include the type of personal information that was used its source and the reasons or principal factors that led to the prediction recommendation or decision,2,14
1272,1272,the artificial intelligence and data acta new feature of bill c  is a proposal to enact the artificial intelligence and data act aida,2,2
1273,1273,the aida is the first federal law in canada regulating the creation and use of ai systems and would create penalties for non compliance,2,17
1274,1274,the stated purposes of the aida are toregulate international and interprovincial trade and commerce in ai systems by establishing common requirements applicable across canada for the design development and use of those systems andprohibit certain conduct in relation to ai systems that may result in serious harm to individuals or harm to their interests,2,17
1275,1275,the aida defines harm as  physical or psychological harm to an individual  damage to an individuals property or  economic loss to an individual,2,10
1276,1276,the aida defines an artificial intelligence system as a technological system that autonomously or partly autonomously processes data related to human activities through the use of a genetic algorithm a neural network machine learning or another technique in order to generate content or make decisions recommendations or predictions,2,4
1277,1277,regulation of ai under the aida is focused on organizations carrying out a regulated activity which means  processing or making available for use any data relating to human activities for the purpose of designing developing or using an ai system or  designing developing or making available for use an ai system or managing its operations,2,8
1278,1278,organizations that carry out any regulated activity and process or make available for use anonymized data must establish measures with respect to the manner in which data is anonymized and the use or management of anonymized data,2,9
1279,1279,the aida imposes regulatory requirements for both ai systems generally and those ai systems specifically referred to as high impact systems,2,17
1280,1280,the existing definition of a high impact system is vague and will be addressed by criteria to be established by regulation,2,17
1281,1281,these regulations remain to be drafted as of the publication of this article,2,1
1282,1282,all organizations responsible for an ai system will be required to assess whether the ai system is a high impact system,2,17
1283,1283,where an ai system meets the definition of a high impact system the person responsible mustestablish measures to identify assess and mitigate the risks of harm or biased output that could result from the use of the systemestablish measures to monitor compliance with the mitigation measures and the effectiveness of those mitigation measureswhere the system is made available for use or an organization is managing the operation of the system publish on a public website a plain language description of the system that includes prescribed content andnotify the minister of industry or other designated minister if use of the system results or is likely to result in material harm,2,17
1284,1284,further the aida provides the minister with broad order making powers and audit rights,2,26
1285,1285,in addition the aida provides authority for the minister to designate an artificial intelligence and data commissioner whose role would be to assist in the administration and enforcement of the aida,2,17
1286,1286,in addition to administrative penalties the aida would also introduce offences that include significant fines and potential imprisonment for non compliance,2,17
1287,1287,generally an organization that contravenes the aida is liable on indictment to a maximum fine of the greater of  million and  per cent of gross global revenues,2,10
1288,1288,further the aida establishes an even higher class of prohibited activities where the contravention involvesprocessing or use of unlawfully obtained personal information in ai systeman ai system resulting in serious physical or psychological harm or substantial damage to property oran ai system defrauding the public and causing substantial economic loss,2,1
1289,1289,such serious offences would result in fines on indictment of up to the greater of  million and  per cent of gross global revenues,2,10
1290,1290,next stepsas bill c  is only at the second reading stage there will likely be much more debate and potential amendments as the bill makes its way through parliament,2,22
1291,1291,the reintroduction of privacy reform legislation in the current parliamentary session is a clear signal that the federal government is committed to implementing sweeping changes to canadian privacy law following changes enacted by the province of qubec with bill,2,2
1292,1292,if enacted bill c  will come into force on a date to be fixed by order of the governor in council though each part of bill c  also contains specific coming into force provisions,2,22
1293,1293,over the past three years the ai governance landscape has become considerably more defined with several governments proposing policies for governing ai technologies within their jurisdictions,3,8
1294,1294,while ai governance initiatives are still nascent distinct approaches to regulatory policy appear to be emerging in different jurisdictions,3,8
1295,1295,this divergence has the potential to undermine international cooperation on ai governance and bring about challenges for regulatory interoperability,3,8
1296,1296,understanding the similarities and differences between different governments approaches is an important first step for promoting deeper cooperation and improved interoperability of regulatory frameworks for ai,3,8
1297,1297,in this report we develop an accessible comparative framework that captures the key similarities and differences in governments approaches to regulatory policy for governing ai,3,8
1298,1298,this comparative framework contains seven categories  definition of ai  key aims  scope and focal areas  approach to risk  regulatory requirements  monitoring and enforcement and  flexibility and revisions,3,8
1299,1299,we then apply this framework to the regulatory approaches of five ai early movers in ai regulatory policy  canada china the european union eu the united kingdom uk and the united states of america usa  including a detailed comparative analysis of their approaches to risk regulatory requirements with deep dives into the role of technical standards impact assessments and audit and monitoring and enforcement,3,8
1300,1300,in this detailed comparative analysis we find that approaches to risk the eu and canada propose a horizontal and graduated approach to risk by defining risk thresholds eu and impact levels canada,3,3
1301,1301,canadas focus on impact is similar to the uks which emphasises the actual impact of ai technologies rather than hypothetical risks as well as the usas blueprint for an ai bill of rights which focuses on the impacts ai systems can have on rights and democratic values,3,17
1302,1302,in contrast to the horizontal approach by the eu and canada the usa and uk focus more on the sectoral impact of these technologies where ai risks are treated as domain specific and there is no overarching legally binding risk framework,3,3
1303,1303,however there are voluntary risk frameworks in these jurisdictions for example the national institute for standards  technology in the usa has developed an ai risk management framework,3,3
1304,1304,china represents a hybrid case of the above approaches with an overarching approach to risk being developed for science and technology research alongside specific risk frameworks for certain ai technologies,3,3
1305,1305,regulatory requirements the eu and canada both take a horizontal hard law approach proposing a series of requirements that vary in stringency and type based on their respective classifications of ai systems in levels of risk eu and impact canada,3,8
1306,1306,they also introduce a series of proportional obligations that vary depending on the person responsible for an ai system in each regulation,3,8
1307,1307,the usa uk and china take more varied approaches,3,2
1308,1308,while the usa is more likely to rely on existing not ai specific sectoral regulation china has developed overarching soft law ethical guidance for ai in general and hard law regulatory requirements targeted at specific types of ai technologies,3,8
1309,1309,the uk approach is context based and sector led with regulators asked to apply their existing powers and expertise to ai focusing on light touch options in the first instance,3,8
1310,1310,monitoring and enforcement the eu and canada intend to establish new enforcement bodies such as the artificial intelligence board for the eu draft ai act and the ai  data commissioner for canadas proposed artificial intelligence and data act aida,3,2
1311,1311,however the approach of the eu is comparatively more complex and less centralised with the european ai board supporting the european commission as well as member states and their national competent authorities mainly in an advisory capacity,3,2
1312,1312,the usa and the uk take a more decentralised approach relying on the existing powers of regulators rather than establishing new monitoring bodies,3,8
1313,1313,both favour lighter touch options with self monitoring and compliance preferred over enforcement with the uk putting a particularly strong emphasis on third party assurance,3,2
1314,1314,chinas ministry of science and technology provides overarching direction for chinas monitoring and enforcement typically through guidance with the cyberspace administration of china cac introducing and enforcing hard law measures related to specific ai technologies and data protection,3,8
1315,1315,while this report focuses on applying our comparative framework to these five early mover jurisdictions we designed the categories to be jurisdiction agnostic and robust to future policy developments,3,8
1316,1316,we plan for the project  of which this report is the first output  to provide a comparison of numerous governments approaches to ai regulatory policy,3,8
1317,1317,the project will gradually be expanded to account for the approaches of other jurisdictions and the comparisons iteratively updated to account for new policy developments,3,17
1318,1318,we hope that our comparative framework can be used to analyse present and future regulatory ai approaches according to a common ground and in turn foster enhanced cooperation,3,8
1319,1319,since  policymakers globally have been increasingly focused on the development and implementation of initiatives and national strategies for the governance of artificial intelligence ai systems,3,8
1320,1320,over the past three years the ai governance landscape has become considerably more defined with several governments proposing regulatory policies for governing ai within their jurisdictions,3,8
1321,1321,in parallel there have been a number of efforts to further international agreement on ai governance such as through the organisation for economic co operation and developments oecd ai principles and oecd expert working groups the council of europes committee on ai the unesco ethics of ai principles the global partnership on ai and the eu us trade and technology council amongst others,3,2
1322,1322,the common thread amongst this plethora of international initiatives is the aim to foster dialogue and advance common frameworks for ai governance,3,8
1323,1323,while domestic and international governance initiatives are still nascent distinct approaches to regulatory policy appear to be emerging in different jurisdictions,3,8
1324,1324,this divergence has the potential to undermine fledgling international initiatives create a fragmented regulatory landscape and bring about challenges for regulatory interoperability,3,8
1325,1325,in particular a fragmented regulatory environment could create significant barriers for governments seeking to deepen cooperation and organisations looking to deploy responsible ai systems across borders,3,8
1326,1326,understanding the similarities and differences between different governments approaches is an important first step for promoting deeper cooperation and improved interoperability of regulatory frameworks for ai technologies,3,8
1327,1327,existing comparisons of different governments approaches to ai regulatory policy take the form of repositories most notably the oecd ai observatorys policy tracker or academic analyses that compare two or more jurisdictions repositories are useful for providing an aggregate picture of the different policy documents that have been published however they do not offer an analytical function for understanding the similarities and differences between different jurisdictions,3,8
1328,1328,academic analyses offer detailed discussions about the particularities of one or more jurisdictions yet they typically lack the accessibility of a higher level aggregated comparison of key aspects of governance regimes,3,26
1329,1329,as a result the audience of such academic pieces is more restricted,3,14
1330,1330,the purpose of this project is to fill the gap between these two approaches by developing an accessible comparative framework that captures the key similarities and differences in governments approaches to regulatory policy for governing ai we use the term regulatory policy in line with the oecds definition of the use of regulations laws and other instruments to deliver better economic and social outcomes regulatory policy as understood here includes both hard and soft law initiatives which aim to create rules or guidance for designing developing andor deploying ai,3,8
1331,1331,we define hard law as legally binding instruments eg primary and secondary legislation whereas soft law as non binding quasi legal instruments,3,26
1332,1332,we specifically chose this inclusive understanding of regulatory policy that encompasses soft law initiatives as many jurisdictions currently favour lighter touch approaches which a hard law focus would not capture,3,8
1333,1333,this report is the first stage of the project,3,24
1334,1334,in this report we develop a comparative framework that we apply to five governments approaches to ai regulatory policy canada china the european union eu the united kingdom uk and the united states of america usa,3,8
1335,1335,these governments were chosen for the first stage of this project for two related reasons,3,26
1336,1336,firstly these countries have been early movers in terms of outlining their distinct approaches to ai governance which may to varying degrees have an influence on the policy decisions made in other jurisdictions secondly they rank highly on many relevant metrics for international influence in the field of ai governance including leading in research and development investment domiciled ai companies and having the foundations necessary for maintaining influence eg an internationally leading education sector,3,8
1337,1337,following this report a second phase of work spring  will apply this comparative framework to further countries with subsequent phases expanding the analyses to other jurisdictions or updating existing case studies based on new policy developments,3,22
1338,1338,it is important to acknowledge from the outset that the comparative framework developed in this report is a heuristic for understanding key similarities and differences between jurisdictions approaches to ai regulatory policy,3,8
1339,1339,it does not seek to provide an exhaustive comparison of for instance differences between each jurisdictions political and legal institutions,3,26
1340,1340,this context is useful for understanding the rationale and trajectory of each governments approach yet it is beyond the scope of this report,3,26
1341,1341,accordingly if an exhaustive understanding of each jurisdictions approach is sought other academic and legal resources should be consulted in conjunction with this reportwith these caveats in mind the target audience we foresee this analysis will be most valuable for includes policymakers who want to contextualise their approaches to regulatory policy in relation to other jurisdictions or understand existing options for specific governance challenges international and national bodies including standards organisations seeking to promote cooperation or convergence in governance between different jurisdictions multinational corporations and smes trying to understand and respect the different requirements that may apply to them in different jurisdictions prospective audit and certification bodies seeking to develop and provide bespoke ai auditing and certification services civil society organisations that seek a comparable high level understanding of regulatory policy in each jurisdiction researchers who want to understand relevant similarities and differences between governments approaches to ai regulatory policy,3,8
1342,1342,the remainder of this report is structured as follows,3,22
1343,1343,first we outline the methodological approach taken for developing the comparative framework,3,26
1344,1344,second we present the comparative framework which considers seven features of the regulatory approach of each of the five jurisdictions,3,8
1345,1345,finally we provide a detailed analysis of several categories of the comparative framework including approaches to risk section  regulatory requirements section  and monitoring and enforcement section,3,3
1346,1346,the section on regulatory requirements also includes a summary table on the role of standards impact assessments and audits,3,1
1347,1347,to develop the comparative framework we began with a set of draft categories related to key features of ai regulatory policy,3,8
1348,1348,these initial categories were based on consultations with ai policy experts from ceimia and the project steering committee who outlined areas of ai regulatory policy that they considered important for our audience,3,8
1349,1349,following this we revised the categories iteratively based on a content analysis of published regulatory policies from canada china the eu the uk and the usa as of january,3,8
1350,1350,to identify relevant regulatory policies for iterating our framework we undertook a systematised literature search that culminated in a corpus of relevant ai regulatory policy documents see appendix,3,8
1351,1351,our literature search involved three steps,3,24
1352,1352,we reviewed ai regulatory policy documents published on the oecds ai policy observatory and filtered them based on an inclusionexclusion criteria that can be found in appendix,3,8
1353,1353,we added relevant documents to our corpus based on domain knowledge and expertise,3,24
1354,1354,we presented our corpus to the steering committee members who have expertise in each of the five jurisdictions within scope and added documents based on the selection criteria,3,27
1355,1355,while the comparative framework is mostly based on the regulatory policy documents of the five jurisdictions analysed in this report we took two steps to ensure the robustness of the categories when adding other governments approaches later in this project,3,8
1356,1356,first we designed the categories to be sufficiently general to capture a full breadth of approaches eg centralised or decentralised hard or soft law approaches,3,26
1357,1357,this was aided by the diversity of the approaches taken by the early movers which necessitated a high degree of generality to ensure comparability second we cross referenced the comparative framework with a sample of regulatory policies from other jurisdictions to ensure its applicabilitywe use the same corpus of ai regulatory policies to inform the granular analysis of each governments approach in section four,3,8
1358,1358,in this section of the report we structure our comparative analysis around some of the key categories of the framework,3,26
1359,1359,in each subsection we order our analysis beginning with the eu and canada who have taken relatively similar hard law approaches before turning to the usa uk and china respectively,3,2
1360,1360,this ordering does not represent a value judgement about the desirability of governments approaches,3,26
1361,1361,before turning to the comparative analysis based on the framework categories it is helpful to first provide an overview of the key aims of each jurisdictions approach and to briefly comment on their scope,3,26
1362,1362,the eu aims for a holistic and mostly binding regulatory approach to ai,3,2
1363,1363,the document at the heart of its approach is the draft ai act a piece of horizontal regulation ie designed to apply to applications of ai across most sectors and applications which was introduced in april,3,2
1364,1364,the draft is currently being discussed in the eu council and the parliament and is expected to enter into force by late  or early,3,2
1365,1365,the document lays down harmonised rules on ai which would be interrelated with a set of legal initiatives such as a civic liability framework product and ai liability rules september  accompanied by a revision of sectoral safety legislation eg machinery regulation may  general product safety directive june  and an upgrade of the rules governing digital services digital markets act september  digital services act july  fit for ai and the digital age,3,2
1366,1366,the draft ai act would also translate some principles and recommendations derived from the non binding ethics guidelines for trustworthy ai april  of the high level expert group hleg and the white paper on ai february  into legal requirements,3,2
1367,1367,these initiatives are part of the wider european ai strategy which strives to make the eu a world class hub for ai and ensure that ai is human centric and trustworthy,3,2
1368,1368,the eus stated main goal is to ensure safety the protection of fundamental rights and to avoid harm without constraining innovation and development,3,2
1369,1369,with regards to safety and rights the main aims outlined in the draft ai act are to ensure that ai systems placed on the union market and used are safe and respect existing law on fundamental rights and union values and to enhance governance and effective enforcement of existing law on fundamental rights and safety requirements applicable to ai systems in terms of innovation the main aims are to ensure legal certainty to facilitate investment and innovation in ai and to facilitate the development of a single market for lawful safe and trustworthy ai applications and prevent market fragmentation,3,2
1370,1370,this balance is also echoed in the objectives of the digital markets act dma and digital services act dsa which even though more generally focused on the digital sector and platforms aim to create a safer digital space in which the fundamental rights of all users of digital services are protected and to establish a level playing field to foster innovation growth and competitiveness both in the european single market and globally,3,23
1371,1371,these goals join in the ai liability directive september  whose aim is to harmonise national liability rules for ai to make it easier for the victims of ai related damage to claim redress,3,10
1372,1372,the draft ai act adopts a horizontal risk based approach by outlining requirements and proportionate obligations according to a classification of ai systems into unacceptable risk high risk limited risk minimal or no risk,3,5
1373,1373,how and whether general purpose ai systems fall within this classification is currently still up for discussion,3,4
1374,1374,in terms of scope the ai act would apply to providers and users in the public and private sector across the ai value chain,3,2
1375,1375,however it would not apply to ai systems developed or used exclusively for military purposes,3,25
1376,1376,depending on how the eu regulatory approach is finalised it may reinforce the brussels effect,3,2
1377,1377,currently the draft ai act the dsa and the ai liability directive would all apply to actors deploying their services in the eu regardless of their place of establishment,3,2
1378,1378,however the extent to which this shapes international regulation or reinforces the eus existing global influence on online platforms is yet to be seen,3,7
1379,1379,in canada the main policies at the heart of its regulatory approach are the proposed artificial intelligence and data act aida and the already adopted directive on automated decision making,3,8
1380,1380,the former would apply to the private sector while the latter applies to government institutions,3,26
1381,1381,aida was initially proposed as part of the governments current attempt to comprehensively reform its federal privacy law bill c  in june,3,2
1382,1382,the directive which came into force in april  is part of the governments efforts to utilise ai to make or assist in making administrative decisions to improve service delivery,3,17
1383,1383,additionally aida aims to regulate ai systems while the directive focuses on automated decision systems,3,17
1384,1384,similar to the eu canadas approach presents a concern with balancing the protection of rights with fostering innovation,3,0
1385,1385,the key aim guiding aida is to regulate trade by establishing common requirements applicable across canada for the design development and use of ai systems and to avoid harm by prohibiting certain conduct in relation to ai systems with a specific focus on high impact systems,3,8
1386,1386,much of the substance and details of aida however are currently left to be elaborated in future regulations including the key definition of high impact ai systems to which most of aidas obligations attach,3,17
1387,1387,this is different from the eus draft ai act where a full chapter of the future regulation is currently devoted to outlining the classification of high risk systems,3,5
1388,1388,aida additionally would not apply to a government institution nor to systems used for military aims,3,26
1389,1389,canadas directive on automated decision making aims to ensure that automated decision systems are deployed in a way that reduces risks to canadians and federal institutions while concurrently leading to more efficient accurate consistent and interpretable decisions made pursuant to canadian law,3,21
1390,1390,it does so by imposing several requirements on the federal governments use of automated decision making technologies and on businesses that licence or sell such technologies to the federal government,3,15
1391,1391,similar to the eu draft ai act the directive takes a horizontal approach by defining different levels of impact for decision systems to which different requirements attach,3,17
1392,1392,the directive applies to any system tool or statistical models that provide external services and are used to recommend or make an administrative decision about a client,3,15
1393,1393,specifically it applies to systems in production it excludes those operating in test environments and it excludes national security systems from its scope,3,15
1394,1394,while the eu and canada have taken fairly centralised approaches through introducing horizontal regulations that provide for relatively centralised enforcement discussed at length in section,3,2
1395,1395,the usa uk and china have taken more decentralised approaches that rely on a patchwork of regulatory policies,3,8
1396,1396,this includes a greater reliance on vertical regulations ie which applies to only a specific application of ai or a specific sector which are enforced by different regulatory agencies,3,8
1397,1397,that said as will be stressed below there are key differences in the way these decentralised approaches are enacted,3,26
1398,1398,the usas approach is characterised by non binding principles voluntary guidance on risk management and the application of existing sectoral legislation rather than the development of new ai specific legislation at the federal level,3,26
1399,1399,the white house has played an important role in advancing guiding ethical principles for both the public and private sector,3,16
1400,1400,first the trump administrations executive order  promoting the use of trustworthy ai in the federal government  established principles for the use of ai by federal agencies section  and a process for implementing them through common policy guidance section  and inter agency coordination section,3,2
1401,1401,its aim was to increase the adoption of ai systems in the federal government and public trust therein,3,2
1402,1402,furthermore the executive order  maintaining american leadership in artificial intelligence  laid the foundation for the office of management and budget omb guidance to federal agencies on ai regulation which included privacy and liberties concerns as well as safety and security among factors to be considered,3,2
1403,1403,these executive orders were important precursors to the biden administrations blueprint for an ai bill of rights bor published in october  which defined five overarching principles to protect the american public from potential harms to their civil rights and liberties the bor asserts that the application of the principles will depend significantly on the context in which the ai systems are used and acknowledges that future sector specific guidance will likely be necessary,3,2
1404,1404,while the principles are non binding and horizontal the bor provides guidance on how they can be enforced by existing federaland state level sectoral legislation as well as federal agency led activities amongst others,3,26
1405,1405,similar to the eus ethics guidelines for trustworthy ai these principles can be understood as a national values statement which seek to influence norms and perhaps legislative efforts at the federal level in the usa,3,16
1406,1406,congress has not yet passed legislation concerning ai regulation,3,8
1407,1407,draft bills such as the algorithmic accountability act aaa and the american data privacy and protection act adppa have been introduced in congress to address risks associated with ai systems especially around privacy,3,2
1408,1408,for instance the aaa would direct the federal trade commission ftc to require covered entities that sell or use automated decision systems and augmented critical decision processes to complete impact assessments including accuracy fairness bias and discrimination,3,6
1409,1409,however at this stage neither have passed through the house or senate and are not likely to come into force in the near future,3,26
1410,1410,for this reason these draft bills will not be analysed in this report,3,22
1411,1411,in addition to the white house and congress efforts the national institute of standards and technology nist released the ai risk management framework rmf in january,3,3
1412,1412,developed in collaboration with the public and private sector it is designed to be a practical resource for different stakeholders to manage risks throughout the entire lifecycle of ai systems more specifically it is intended to be voluntary rights preserving non sector specific and use case agnostic providing flexibility to organisations of all sizes and in all sectors however the rmf will not be a compliance mechanism nor will it be a checklist intended to be used in isolation,3,2
1413,1413,the core of the framework describes four specific functions  govern map measure and manage  to help relevant stakeholders address the risks of ai systems and the playbook contains guidance for operationalising these functions,3,3
1414,1414,nist plans to continuously update the rmf and the playbook based on feedback and in house monitoring to ensure it remains fit for purpose across contexts and sectors over time,3,22
1415,1415,the uks approach which is most clearly laid out in the document establishing a pro innovation approach to regulating ai  proposes a sector led approach that relies on regulators to address the impacts of ai in their specific context,3,8
1416,1416,in contrast to the approaches taken by the eu and canada this will place the burden for designing regulatory policy for ai on several different regulators,3,8
1417,1417,this approach is designed to provide a clear innovation friendly and flexible approach to regulating ai that addresses harms within particular contexts and can be regularly updated,3,8
1418,1418,to ensure consistency across different sectoral regulators the proposal encourages cooperation between regulators through mechanisms such as the digital regulation cooperation forum drcf which has a specific algorithmic processing workstream,3,23
1419,1419,on top of this the approach proposes specific characteristics of ai and a set of cross sectoral principles that will guide sector led governance however like the usas bor the uk stresses that the interpretation of these principles should be context dependent,3,8
1420,1420,in terms of encouraging innovation these principles ask that regulators focus on high risk concerns rather than hypothetical or low risks associated with ai,3,3
1421,1421,additionally they ask that regulators consider lighter touch options such as guidance or voluntary measures in the first instance,3,8
1422,1422,as far as possible they will also seek to work with existing processes rather than creating new ones,3,25
1423,1423,this emphasis on sector led governance and light touch instruments is designed to ensure comprehensive regulatory coverage and flexibility so that the uk approach can be regularly updated based on new opportunities and risks from ai,3,8
1424,1424,importantly the uk is planning to publish a white paper which will provide further details on the countrys approach to ai regulatory policy,3,8
1425,1425,in addition to the policy paper on the uks approach to regulating ai several policy documents have been released by other government bodies,3,8
1426,1426,one area where the uk has been particularly interested in developing regulatory policy is in the area of assurance,3,8
1427,1427,the uks centre for data ethics and innovation cdei published the roadmap to an effective ai assurance ecosystem  which was launched to drive the development of the assurance ecosystem a market based solution to support the wider pro growth risk based approach to ai governance,3,2
1428,1428,engagement with industry that followed this publication resulted in the industry temperature check  which looks at the barriers and enablers to ai assurance and sets out clear interventions that the government and others can make to overcome these barriers,3,2
1429,1429,a portfolio of assurance techniques that will showcase ongoing good practice across industry will be published in the first half of,3,27
1430,1430,chinas approach to ai regulatory policy is also not laid out in a single regulatory document with several government organisations publishing relevant documents,3,8
1431,1431,overall the main aims that can be inferred from various ai regulatory policies are to preserve national security and stability protect the public interest and the interests of citizens qua consumers and to stimulate the healthy development of ai technologies,3,8
1432,1432,the ministry of science and technology has acted as the overarching coordinative body for governing ai introducing voluntary principles and guidance on integrating ethics into the whole ai lifecycle,3,16
1433,1433,the cyberspace administration of china cac has complemented this overarching soft law approach by releasing hard law measures,3,8
1434,1434,like the initiatives proposed in the eu and canada these policies introduce specific prohibitions and legal requirements for ai see section,3,8
1435,1435,however unlike ai regulation in the eu and canada which has sought to address ai technologies in general the approach taken by the cac has been more targeted at specific types of ai such as recommender systems  and generative algorithms,3,8
1436,1436,likewise while the eu and canadas initiatives are primary legislation the cacs regulatory initiatives are secondary legislation based on powers from primary data protection statutes such as the personal information protection law,3,2
1437,1437,alongside these ai specific measures the state council  chinas chief administrative authority  has published guiding opinions on strengthening ethical governance of science and technology,3,16
1438,1438,while this document considers science and technology in general it is indicative of further regulatory measures being introduced and review bodies established that apply to ai research and development,3,8
1439,1439,accordingly future regulatory policy in china will likely come from a combination of initiatives covering science and technology research in general as well as measures more specifically focused on the development and use of ai technologies,3,8
1440,1440,in this section we focus on risk as an umbrella concept that broadly captures a jurisdictions approach to dealing with future uncertainties related to the design development and deployment of ai systems,3,3
1441,1441,the approach to risk is a theme through which the differences and similarities between the jurisdictions approaches become more clear,3,3
1442,1442,ai harms for example vary by context where they might be already addressed by particular sectoral laws,3,10
1443,1443,at the same time several harms can readily be traced to a pattern of similar problems and typically get characterised as risks or in terms of their impact,3,3
1444,1444,in this section we analyse how risk is framed or defined in each jurisdictions approach to ai regulatory policy and how if at all a jurisdiction builds a framework for risk management,3,8
1445,1445,the eus approach to risk frames regulation around different risk classifications rather than a specific definition,3,2
1446,1446,in the draft ai act this is achieved by defining different thresholds for risk through an approach that features mostly horizontal but also some vertical components,3,3
1447,1447,in terms of horizontal components it differentiates between unacceptable risk high risk limited risk minimal or no risk ai systems,3,26
1448,1448,unacceptable risk systems are prohibited,3,5
1449,1449,they include systems for social scoring the use of biometric identification in public spaces and subliminal techniques as defined in title ii,3,12
1450,1450,high risk systems are permitted subject to compliance with certain mandatory requirements and an ex ante conformity assessment outlined in title iii,3,5
1451,1451,they include among others systems that predict a persons risk of committing a crime or that automate hiring decisions such as sorting resumes or cvs as defined in annex iii,3,26
1452,1452,limited risk systems are permitted subject to transparency obligations outlined in title iv,3,5
1453,1453,they include systems for biometric categorization emotion recognition and deep fake systems,3,12
1454,1454,minimal or no risk systems include all other systems not covered by the draft ai act safeguards and regulations,3,5
1455,1455,the draft ai act features a specific focus on high risk systems,3,5
1456,1456,the high risk classification of the draft depends on the function performed by the ai system as well as on the specific purpose and modalities for which that system is used,3,3
1457,1457,these would be assessed by outlining a set of specific areas eg biometrics critical infrastructure education and vocational training and law enforcement and criteria eg the likelihood of the use of the ai system the potential extent of the harm and the reversibility of its outcome,3,12
1458,1458,this specification of sectoral areas for high risk systems introduces a minor vertical component to the horizontal approach,3,26
1459,1459,the draft ai act also asks that a risk assessment is conducted with respect to whether a system is high risk and to assess systemic risks respectively and that this assessment should be agile iterative and constantly adapting to the changing nature of technology and systemic risks,3,3
1460,1460,the canadian approach to risk is semantically different to the eus with the aida focusing on the impact of ai systems,3,3
1461,1461,canadas aida leaves the definition of high impact open ended as it defers its specification to later regulation,3,17
1462,1462,still it is concerned with high impact ai systems with respect to setting out requirements to identify assess and mitigate the risk of harm and biassed output that could result from the use of such ai systems,3,17
1463,1463,the directive on automated decision making specifies impact on both the individual and the community along the dimensions of rights health or well being economic interests and sustainability of an ecosystem,3,21
1464,1464,with regards to building a risk framework canadas aida only proposes a division that distinguishes high impact systems from any regulated activity without specifying risk thresholds,3,17
1465,1465,with regards to risk management it would require that a person who is responsible for an ai system must in accordance with the regulations assess whether it is a high impact system part  article  and that the person establish measures to identify assess and mitigate the risks of harm or biassed output that could result from the use of the system part  article,3,5
1466,1466,this is similar to what the draft eu ai act would require,3,2
1467,1467,however aida defers to later regulations for further specifications on what those measures should be,3,17
1468,1468,meanwhile the directive on automated decision making introduces an algorithmic impact assessment aia tool which establishes a framework with four different impact assessment levels from lowest to highest,3,17
1469,1469,these levels have a horizontal component where impact is considered reversible and brief level i likely reversible and short term level ii difficult to reverse and ongoing level iii and irreversible and perpetual level iv,3,22
1470,1470,as mentioned above however impact is also vertically assessed along the dimensions of  rights ii health and well being iii economic loss and iv sustainability,3,17
1471,1471,each level of impact comes with its own requirements mentioned in section  below,3,17
1472,1472,both the eu and the canadian approach to risk entail horizontal risk frameworks with some vertical components,3,26
1473,1473,however the horizontal component is risk thresholds for the former while impact levels for the latter,3,17
1474,1474,additionally they differ in their vertical specifications with the eu focusing on specific areas of application and criteria and canada outlining more abstract dimensions,3,2
1475,1475,in the usa several regulatory policies address risks associated with the development and use of ai systems in both the public and private sector,3,8
1476,1476,the bors principles are explicitly framed as an overlapping set of backstops against potential harms while the executive order  emphasises that the benefits of using ai systems must outweigh the risks,3,14
1477,1477,a noteworthy nuance in the bor is its focus on ai technologies that have the potential to meaningfully impact the american publics rights opportunities or access to critical resources or services while it is written at a higher level of abstraction this focus on impacts on rights and opportunities is consistent with the eus ai act,3,2
1478,1478,furthermore as comprehensively listed by the bor fact sheet various federal agencies have established councils and frameworks for addressing domain specific ai risks such as the department of energys ai advancement council,3,3
1479,1479,the most sophisticated articulation of ai risks is in nists ai rmf which provides an overarching framework for minimising the negative impacts and maximising the positive impacts of ai systems,3,3
1480,1480,the rmf defines risks  that is risks of negative impacts  as a composite measure of an events probability of occurring and the magnitude of the consequences of the corresponding events section,3,3
1481,1481,this definition is notably different to the eu and canadian approaches which classify ai risks into graduating tiers,3,3
1482,1482,while ai shares some risks with other technologies the rmf states that ai systems bring a set of risks that require specific consideration and approaches and therefore require a bespoke risk management framework,3,3
1483,1483,the rmf expands the oecds framework for the classification of ai  to delineate specific risks that may emerge at each phase of the lifecycle the lifecycle is broken down to five dimensions which each contain lifecycle stages application context eg,3,3
1484,1484,system design data and input eg,3,24
1485,1485,data collection and processing the ai model eg,3,4
1486,1486,building verification and validation task and output eg,3,20
1487,1487,model deployment and people and planet eg,3,20
1488,1488,model uses and impacts,3,20
1489,1489,some dimensions such as application content are relevant both at the beginning eg,3,4
1490,1490,design of the system and once implemented eg,3,15
1491,1491,monitoring,3,24
1492,1492,in addition to identifying risks associated with each stage of the ai lifecycle the rmf specifies specific activities that different stakeholders can carry out to mitigate potential risks,3,3
1493,1493,the rmf emphasises the importance of context for risk assessment and management thus it does not prescribe a single way of measuring risks,3,3
1494,1494,this focus on context is consistent with the above mentioned regulatory policies which equally emphasise the importance of context for the interpretation and application of ethical principles,3,1
1495,1495,the uk takes a similar approach to risk as canada and the usa focusing on the actual impact of ai technologies on individuals and groups concurrently the uk places a strong emphasis on the contextual impacts of these technologies that will be identified and addressed by individual regulators,3,3
1496,1496,importantly the uk also specifies that the risk of missed opportunities eg of not using the technologies should be considered something that is reflective of its overarching pro innovation approach,3,3
1497,1497,given the uks sector based approach to ai governance a cross cutting or overarching risk management framework like that of the eu and canada is not present,3,3
1498,1498,the policy document establishing a pro innovation approach to regulating ai specifies that there should be evidence of real risk rather than hypothetical risks however the document states that it anticipates that regulators will establish risk based criteria and thresholds for the specific contexts that they are regulating,3,8
1499,1499,given this it is likely that multiple risk frameworks will be published in the uk,3,3
1500,1500,that said the policy document highlights the importance of regulatory coordination for this approach to work to avoid contradictory approaches and help spot emerging issues,3,8
1501,1501,in china there is currently no single authoritative document that outlines the countrys approach to ai risk management,3,3
1502,1502,one of the most relevant documents for understanding chinas approach to ai risk is guiding opinions on strengthening ethical governance of science and technology  which frames risk in the general context of science and technology and has a strong focus on regulatory policy for research and development,3,16
1503,1503,this document emphasises societal and ecosystem risk stating thatscientific and technological activities should objectively assess and prudently treat the risks of uncertainty and technological applications should strive to avoid and prevent risks that may be triggered prevent misuse and abuse of scientific and technological outcomes and avoid endangering the safety and security of society the public biology and ecologythis passage is indicative of a difference in framing of harms between china and the eu,3,3
1504,1504,namely while both china and the eus approach to risk focus on harms the eu approach centres around individuals while the chinese approach is people centric and focuses more on society,3,2
1505,1505,in fact the ai act specifically refers to high risk systems as a safety component in products or as a risk to the health and safety or the fundamental rights of persons eg systems for biometric identification and the management and operation of critical infrastructure,3,5
1506,1506,here the semantic difference between people in aggregate and individual persons is of importance,3,4
1507,1507,in terms of risk frameworks rather than offering a risk framework for ai specifically the guiding opinions on strengthening ethical governance of science and technology refers to the creation of a list of high risk scientific and technological activities for ethics in science and technology which will be formulated by the national committee on the ethics in science and technology,3,3
1508,1508,the content of the list remains unspecified and is applicable to scientific and technological activities in general leaving it uncertain as to the degree to which ai will be focused on specifically,3,4
1509,1509,risk frameworks related to certain ai technologies will also likely be developed in the near future by the cac,3,3
1510,1510,for instance in the internet information service algorithmic recommendation management provisions  it is stated thatin conjunction with relevant departments such as for telecommunications public security and market regulation the internet information department is to establish a hierarchical and categorical management system to conduct management by grade and category of algorithmic recommendation service providers based on the algorithmic recommendation services public sentiment attributes and capacity to mobilise the public the content types the scale of users the importance of the data handled by the algorithmic recommendation technology the degree of interference in user conduct and so forththis passage indicates that some form of risk framework will be developed for recommender systems based on the specific characteristics stated above,3,21
1511,1511,in line with the differing overarching approaches taken to ai risk the regulatory requirements differ within the five jurisdictions,3,8
1512,1512,at present the eu draft ai act presents a series of regulatory requirements for high risk ai systems,3,5
1513,1513,these are outlined in chapter  title iii of the draft ai act and are in relation to risk management data and data governance documentation and recording keeping transparency and provision of information to users human oversight robustness accuracy and security,3,11
1514,1514,article  for example would require that a risk management system is established consisting of a continuous iterative process and that it identifies estimates and evaluates potential risks arising from high risk ai systems,3,3
1515,1515,article  would require that technical documentation is drawn up before a high risk system is put on the market or into service and that such documentation is kept up to date,3,5
1516,1516,article  would require that a system have capabilities to enable automatic logging of events and article  would require that the systems operations are sufficiently transparent for a user to interpret,3,15
1517,1517,the precise technical solutions to achieve compliance with those requirements may be provided by standards see table  or by other technical specifications or otherwise be developed in accordance with general engineering or scientific knowledge at the discretion of the provider of the ai system,3,5
1518,1518,the draft ai act also introduces a set of obligations across providers of high risk ai systems with proportionate obligations for users and other participants across the ai value chain eg importers distributors authorised representatives,3,2
1519,1519,with regards to obligations for providers these include among others that they ensure compliance with the above requirements through a quality management system that they take necessary corrective action if the ai system is not in conformity with these requirements and that they make the system undergo a conformity assessment procedure and be registered with a declaration of conformity before being put into use,3,27
1520,1520,ai systems that are safety components of products will undergo third party conformity assessment procedures already established under the relevant sectoral product safety legislation,3,5
1521,1521,however a new compliance and enforcement system will be established for stand alone high risk ai systems detailed in annex iii,3,5
1522,1522,similar to the eu canada put forward an overarching regulatory mechanism to guide its approach to ai regulation that however bifurcates into the proposed aida for the private sector and the adopted directive on automated decision making for the public sector,3,8
1523,1523,for the private sector the aida lists a series of requirements that entail measures with respect to anonymized data the assessment of whether a system is high impact and risk mitigation record keeping and reporting obligations and the publishing of a publicly available ai statement,3,17
1524,1524,some of these requirements resemble those of the draft ai act including the requirement to establish measures to identify assess and mitigate the risks of harm and to establish measures to monitor compliance with the mitigation measures,3,17
1525,1525,however some differ in their stringency,3,26
1526,1526,for example while aida would require notification for all high impact systems that are likely to cause material harm the draft ai act would only require it in case of a serious incident or malfunctioning,3,17
1527,1527,finally different from the draft ai act aida would not require a certification system for high impact systems through a conformity assessment,3,17
1528,1528,overall the aida would apply to persons including trusts joint ventures partnerships unincorporated associations and any other legal entities who carry out any of the regulated activities as specified above,3,26
1529,1529,for the public sector the directive on automated decision making requires completing an algorithmic impact assessment prior to the production of any automated decision system see table,3,21
1530,1530,there are four different levels of impact from lower to higher,3,17
1531,1531,each comes with its own requirements varying with regards to the stringency of review notice human in the loop explanation training documentation contingency planning and approval for the system to operate,3,27
1532,1532,for example with respect to human inthe loop requirements decisions from systems of impact level i and ii may be rendered without human involvement,3,21
1533,1533,however those from impact level iii and iv cannot be carried out without human intervention at specific points during the decision making process and it is required that the final decision is taken by a human,3,21
1534,1534,while most requirements in the draft ai act are focused on high risk systems the directive on automated decision making sets them at differing degrees according to the increase in levels of impact,3,3
1535,1535,as mentioned above the directive on automated decision making applies to the federal governments external use of automated decision making technologies listing consequences for individuals as well as institutions see section,3,21
1536,1536,the regulatory policy requirements related to ai in the usa uk and china are more decentralised but each takes a different approach,3,8
1537,1537,in the usa a clear emphasis has been placed on lighter touch options such as ethical principles and voluntary guidance as well as the application of sectoral regulation,3,27
1538,1538,for instance the principles in the bor and the ai risk management guidance in the rmf were made for voluntary use and thus do not require compliance,3,1
1539,1539,indeed the bor explicitly states that it does not constitute binding guidance for the public or federal agencies and therefore does not require compliance while the principles of the bor also are non binding the bor provides guidance on how they can or in some cases already are enforced through federaland state level legislation within particular sectors,3,26
1540,1540,for example the equal employment opportunity commission and the department of justice have provided guidance on how employers use of software that relies on algorithmic decision making may violate existing requirements under title  of the americans with disabilities act ada and how employment discrimination law can be enforced to tackle discriminatory practices by employers,3,6
1541,1541,similarly the ftc has published guidance on how various acts should be interpreted in light of ai systems the ftc act section  prohibits unfair or deceptive practices including the sale or use of  for example  racially biassed algorithms the fair credit reporting act may be enforced when an algorithm is used to deny people employment housing credit insurance or other benefits and the equal credit opportunity act makes it illegal for a company to use a biassed algorithm that results in credit discrimination on the basis of race colour religion national origin sex marital status age or because a person receives public assistance,3,6
1542,1542,these examples illustrate the usas emphasis on the enforcement of existing sectoral regulations adapted to ai,3,8
1543,1543,with regards to the public sectors use of ai systems as required by the executive order  section  agencies are required to catalogue non classified non sensitive and non research ai use cases in an online inventory which was launched in june,3,2
1544,1544,in the uk much of the existing guidance has been focused on the public sector including public sector procurement compliance with equalities law and police use of facial recognition technology,3,8
1545,1545,each of these regulatory policies introduces different specific requirements or guidance related to public sector use of ai,3,8
1546,1546,for instance the uks equalities regulator provides a checklist for public sector organisations using ai which helps them determine whether they are meeting their public sector equality duty,3,0
1547,1547,the algorithmic transparency recording standard has been developed to help government bodies provide information on the type of systems they are using and why,3,11
1548,1548,government organisations that use algorithmic systems which may have a potential public effect or impact decision making are encouraged to use the standard,3,15
1549,1549,information required as part of the standard includes a rationale of how and why the system is being used the persons responsible for the tool the datasets used to train the tool impact assessments undertaken and potential risks and mitigations,3,15
1550,1550,however unlike the usas online inventory which is mandatory for federal agencies using ai the uks algorithmic transparency recording standard is voluntary,3,0
1551,1551,various pieces of guidance have also been produced that apply to the private sector for instance ico guidance on explaining ai decision making or using facial recognition technology in public spaces,3,14
1552,1552,in both cases these pieces of guidance specify how data protection law should be interpreted in light of the challenges raised by these ai technologies,3,11
1553,1553,because the uk is taking a context based and sector led approach to governance liability varies depending on the particular target of a piece of guidance as well as the regulators specific powers and jurisdiction,3,26
1554,1554,in china a mixture of regulatory and non regulatory requirements have been introduced,3,1
1555,1555,regarding regulatory measures these have mostly been introduced by the cac and have focused on specific ai technologies,3,8
1556,1556,for instance in march  a regulation on algorithmic recommendations came into force introducing a variety of requirements and prohibitions in relation to these technologies,3,8
1557,1557,this includes banning the use of algorithmic systems for manipulating search results ranking pushing addictive content towards minors or using discriminatory tags in recommender systems,3,7
1558,1558,this echoes the eu dsa which lists a set of regulated responsibilities to address systemic issues such as disinformation hoaxes and manipulation during pandemics harm to vulnerable groups and other emerging societal harms,3,17
1559,1559,a variety of requirements are also outlined including regularly examining and verifying the algorithms producing a complete feature database and providing users with an option not to receive algorithmic recommendations,3,27
1560,1560,at present these requirements are all outlined at a high level with little detail provided as to how they should be enacted in practice,3,27
1561,1561,a database of private sector recommender systems  similar to the public sector ai databases in the usa and uk  has also been established,3,0
1562,1562,another hard law policy introduced recently by the cac is the draft provisions on the administration of deep synthesis internet information services  which seek to regulate generative algorithms such as those that create deepfakes,3,8
1563,1563,in this regulation the use of generative algorithms for pornography or false information is prohibited and real name identification is required for the users of generative algorithms,3,12
1564,1564,both cac regulations predominantly focus on the service provider and in some cases the user,3,1
1565,1565,depending on how these regulations are interpreted and the particulars of the supply chain this may mean that developers avoid liability,3,1
1566,1566,accompanying these hard regulatory measures are a number of softer voluntary initiatives from the ministry of science and technology including ai ethics principles and norms which are designed to guide ethical behaviour throughout the whole ai lifecycle,3,16
1567,1567,by and large these principles reflect those published by the eus hleg however given the high level nature of ethics principles the interpretations of how they are to be enacted may differ in practice,3,2
1568,1568,standards impact assessments and audits are regulatory requirements that have been proposed or introduced to varying degrees in each of the five early mover jurisdictions,3,18
1569,1569,these regulatory tools support in turning high level policy objectives into tangible outcomes so are worthy of particular attention,3,17
1570,1570,standards are of particular note for this report as they can be used to both demonstrate conformity with emerging ai regulation and promote interoperability among different jurisdictions,3,8
1571,1571,this is because the adoption of international standards  developed in international standards bodies such as the iso and ieee  can support harmonisation of how technical and ethical regulatory stipulations are enacted in practice the table below provides a summary of each of these specific requirements,3,1
1572,1572,the government of canada has been at the forefront of ai standards development both for its internal oversight of ai systems and to support external regulatory objectives,3,8
1573,1573,for the governments use of ai canada was the first national government to launch a policy of this kind the directive on automate decision making which was released in spring,3,21
1574,1574,given its early and successful adoption the directive has set the bar for the oversight of adms and thus set the standard for external use,3,0
1575,1575,since then looking to external oversight canada is playing a key role in the modernization of its regulatory system,3,8
1576,1576,through the standards council of canada scc canada has been on the front seat of important international standards organization iso developments,3,2
1577,1577,canada has been extremely engaged in the isoiec jct sc committee which deals with ai standardisation,3,2
1578,1578,specifically it was one of the initial drafters of the isoiec dis  standard,3,26
1579,1579,the latter aims to create a standard for an ai conformity assessment scheme which could also be adopted in the eu draft ai act,3,18
1580,1580,the scc is currently testing both this standard and the aia through a pilot which involves one conformity assessment body and one ai developeruser,3,27
1581,1581,additionally canada is establishing an ai standardization collaborative which consists of a cross sector group of artificial intelligence developers users researchers and regulators to identify needed standards and conformity assessment tools in support of canadian artificial intelligence interests and priorities,3,27
1582,1582,as a key component of the directive on automated decision making there are varying degrees of compliance based on the impact of each system,3,15
1583,1583,evaluated through an algorithmic impact assessment tool aia a component of the directive system deployers are required to assess their system using the aia to determine the impact level of automated decision making systems adms and follow the appropriate compliance requirements as outlined in annex c of the directive,3,17
1584,1584,there are four different levels of impact from lower to higher,3,17
1585,1585,each comes with its own requirements varying with regards to the stringency of review notice human in the loop explanation training documentation contingency planning and approval for the system to operate,3,27
1586,1586,following a similar pattern the proposed legislation aida will require the persons responsible for an artificial intelligence system to conduct an assessment on whether it is a high impact system,3,17
1587,1587,this assessment ought to be conducted in accordance with further regulations which are yet to be defined,3,18
1588,1588,in the case of aida as currently drafted the minister of industry may by order require that the person responsible for the ai system  conduct an audit with respect to the possible contravention or  engage the services of an independent auditor to conduct the audit if they have reasonable grounds to believe that the required sections of aida sections   have been violated eg,3,0
1589,1589,requirements on data anonymization record keeping or on the assessment of high impact systems,3,11
1590,1590,with regards to the directive on automated decision making the government of canada retains the right to authorise external parties to review and audit proprietary software components used for automated decision making systems in accordance with the information required by the algorithmic impact assessment,3,15
1591,1591,china has taken a keen interest in developing technical standards for ai,3,4
1592,1592,in  the standardization administration of china the countrys main standards setting body issued a call for the development of a full range of standards for ai,3,18
1593,1593,in october  the central government published a national strategy for technical standards which specifically included ai as an area to strengthen standardisation research,3,2
1594,1594,traditionally china has followed a largely state led approach to the development of technical standards with this strategy incentivising more industry participation in standards making,3,0
1595,1595,several technical standards committees focused on aspects of ai have been established,3,27
1596,1596,some of these endeavours have resulted in the development of standards for instance a standard for autonomous driving test scenarios initiated by china was formally accepted by the international organisation for standardisation iso,3,18
1597,1597,the personal information protection law requires an ex ante data protection impact assessment if personal information is handled or used for automated decision making,3,9
1598,1598,on top of this regulatory provisions require that the service providers of recommender systems with certain properties must provide relevant regulators with information on the systems and an algorithm self assessment report,3,21
1599,1599,however the exact information required in these reports is currently unclear,3,14
1600,1600,the ethical norms for the new generation artificial intelligence specifies that those researching and developing ai systems should gradually realise auditability,3,16
1601,1601,however the manner in which this should be achieved is not elaborated,3,14
1602,1602,chinas public registry for recommender algorithms could also be seen as a type of audit with the cac able to review required documents such as the aforementioned algorithmic self assessment,3,0
1603,1603,finally through cybersecurity reviews conducted by the cac it is possible that ai systems will be audited to ensure compliance with data protection regulations,3,0
1604,1604,the draft ai act requires high risk systems to be in compliance with harmonised standards as defined in regulation eu no  of the european parliament and of the council and where the standardisation process is blocked or delayed the commission should be able to establish via implementing acts common specifications for certain requirements in the ai act,3,5
1605,1605,the objective is to specify common requirements for risk management data governance transparencyhumanrobustnessmanagementproceduresassessment,3,3
1606,1606,the draft ai act calls for the involvement of smes in the elaboration of standards to promote innovation and competitiveness,3,2
1607,1607,the recent eu commissions draft standardization request for the ai act envisions the european committee foroversightresilience qualityand provide for conformityaccuracy standardisationeuropeanelectrotechnicalcenelec as the main european standardization organizations esos to create standards through their joint technical committee  for ai,3,2
1608,1608,additionally the internal market consumer protection imco committee draft report on the european standardization strategy calls for the creation of an annual standardisation dashboard and cross community collaboration on standards,3,2
1609,1609,the technical standards developed by cen  cenelec are voluntary but organisations who follow and adopt them will benefit from a presumption of conformity with the ai act in the relevant area,3,27
1610,1610,in article  chapter  title iii the draft ai act states that a risk management system shall be established implemented documented and maintained in relation to high risk ai systems,3,5
1611,1611,the system would consist of a continuous iterative process run throughout the entire lifecycle of a high risk ai system requiring regular systematic updating,3,5
1612,1612,it would require specific steps including the identification and analysis of the known and foreseeable risks associated with each high risk ai system and the estimation and evaluation of the risks that may emerge when the high risk ai system is used in accordance with its intended purpose and under conditions of reasonably foreseeable misuse,3,5
1613,1613,n chapter  title iii the draft ai act states that the provider should ensure the accomplishment of a required conformity assessment procedure and be registered with a declaration of conformity before use develop a quality management system to ensure compliance draw up the relevant documentation and establish a robust post market monitoring system to monitor the performance and compliance of the system throughout its lifecycle,3,18
1614,1614,to check on the approved quality management system the conformity assessment body shall carry out periodic audits to make sure that the provider maintains and applies the quality management system and shall provide the provider with an audit report,3,18
1615,1615,in the context of those audits the notified body may carry out additional tests of the ai systems for which an eu technical documentation assessment certificate was issued,3,18
1616,1616,the nist ai risk management framework rmf is a voluntary resource for different stakeholders to manage risks across the lifecycle of ai systems,3,3
1617,1617,while it does not propose standards per se it recommends that its risk management approaches should align with existing sectoror application specific guidelines or standards,3,3
1618,1618,it aims to take advantage of and foster greater awareness of existing standards guidelines and tools for managing ai risks as well as illustrate the need for additional improved resources,3,3
1619,1619,the rmf follows nists us,3,22
1620,1620,leadership in ai plan for federal engagement in developing technical standards  which promotes research on ai standards coordination among agencies public private partnerships and international engagements,3,2
1621,1621,the rmf aims to foster the development of innovative approaches to the management of ai risks including the use of impact assessments,3,3
1622,1622,while it does not propose an impact assessment procedure or template it underlines the importance of impact assessments to understand the potential impacts or harms of ai systems within specific contexts,3,17
1623,1623,furthermore it states that actors such as impact assessors and evaluators will provide technical human factor socio cultural and legal expertise to carry out impact assessments of ai systems including evaluating the requirements for ai system accountability combating bias examining the impacts of ai systems product safety liability and security among others,3,17
1624,1624,similar to impact assessments the nist ai rmf underlines the role of audits in the identification and evaluation of potential risks or impacts that may emerge during the life cycle of ai systems,3,18
1625,1625,it aims to foster the development of innovative approaches including audits to address various characteristics of trustworthy ai systems including accuracy explainability privacy robustness safety and mitigation of harmful biases,3,18
1626,1626,it states that audits that confirm that a system is performing as intended will be an important part of making impact assessments,3,17
1627,1627,some jurisdictions intend to establish new monitoring bodies to enforce their respective ai regulations,3,8
1628,1628,as proposed by the european commission the draft ai act would establish an artificial intelligence board or ai office which will assist the european commission as well as member states and their national competent authorities mainly in advisory capacity,3,2
1629,1629,among other things it will provide guidance on matters related to the implementation of the ai act including on enforcement matters,3,17
1630,1630,additionally the board would establish two standing sub groups to provide a platform for cooperation and exchange among market surveillance authorities and notifying authorities on issues related respectively to market surveillance and notified bodies,3,1
1631,1631,in addition to the board the commission will have the authority  to maintain a publicly accessible database of information concerning high risk systems ii to oversee the conformity assessment process for high risk systems and iii to oversee market surveillance activities,3,19
1632,1632,at the national level member states will have to designate one or more national competent authorities and among them the national supervisory authority for the purpose of supervising the application and implementation of the regulation,3,1
1633,1633,the european data protection supervisor will act as the competent authority for the supervision of the union institutions agencies and bodies when they fall within the scope of this regulation,3,2
1634,1634,there have been concerns that national authorities may have insufficient knowledge or resources to enforce requirements and the negotiations in the european parliament might take a different direction,3,2
1635,1635,these considerations are at the heart of the amendments that are taking place at this draft stage of the ai act,3,2
1636,1636,in cases where ai providers breach their duty of care eg,3,4
1637,1637,they do not comply with one of the regulations outlined in the previous section and this harms users eg,3,1
1638,1638,algorithmic discrimination the proposed ai liability directive introduces a fault based system for them to claim compensation,3,10
1639,1639,given plausible evidence of harm national courts can order providers to disclose evidence to claimants about the high risk system to check compliance with the regulations laid out in the draft ai act,3,10
1640,1640,like the eu draft ai act the canadian government intends to establish a new monitoring authority to assist with administration and enforcement of aida,3,17
1641,1641,specificallytheregulationwould beenforcedbyanaianddatacommissionerwho will be nominated by the minister of innovation science and industry,3,2
1642,1642,this commissioner would have three powers,3,22
1643,1643,first the commissioner could request by order the provision of records ie about system assessment risk management monitoring measures and data anonymization with the ability to request additional records if there were to be reasonable grounds to believe that the use of a high impact system could result in harm or biassed outputs,3,17
1644,1644,second the minister may by order require that the person responsible for the ai system  conduct an audit with respect to the possible contravention or  engage the services of an independent auditor to conduct the audit if they have reasonable grounds to believe that the required sections of aida sections   have been violated eg requirements on data anonymization record keeping or on the assessment of high impact systems,3,0
1645,1645,third they can order the cease of use and production of high impact ai systems if there are reasonable grounds to believe that the use of the ai system gives rise to a serious risk of imminent harm,3,5
1646,1646,however unlike the draft ai act the aida does not outright ban certain types of ai systems,3,2
1647,1647,in the case of the directive on automated decision making the consequences for non compliance are listed separately in the framework for the management of compliance,3,21
1648,1648,this framework clarifies the roles of the treasury board which is meant to ensure compliance,3,0
1649,1649,overall the treasury board will use information gathered through a range of sources that include reporting on compliance under this framework and renewed treasury board policies management accountability framework assessments internal and horizontal audits auditor general reports evaluations treasury board submissions and other reports to parliament to gauge the state of compliance management in the government,3,0
1650,1650,the framework explicates how enforcement should be undertaken for non compliance for both institutions and individuals,3,26
1651,1651,these are divided into minimal eg work collaboratively for organisations training and education for individuals moderate eg increase reporting requirements for organisations transfer or deployment for individuals more severe eg imposition of redress measures for organisations suspension or financial penalties for individuals and most severe consequences eg constrain authorities for organisations disqualify from public service employment for individuals,3,24
1652,1652,in the usa the soft law approach characterised by ethical principles in the executive order  the bor and the voluntary guidance in the nist ai rmf does not require monitoring or enforcement,3,1
1653,1653,nist through its trustworthy and responsible ai resource center will provide guidance on how to implement the rmf and it plans to continuously update the rmf and related resources through in house monitoring and multi stakeholder feedback but it does not have powers to enforce its implementation,3,27
1654,1654,with regards to public sector use of ai the federal chief information officers council is responsible for providing guidance to federal agencies concerning the preparation of annual inventories coordinating and sharing information between agencies and maintaining the online inventory of inter agency ai use cases,3,0
1655,1655,the use cases provided in the online inventories are provided by each agency rather than through monitoring conducted by the council itself,3,7
1656,1656,finally with regards to federal legislation the usas approach is characterised by a patchwork of sectoral legislation see under regulatory requirements which are enforced by appointed competent authorities,3,8
1657,1657,for instance ada is enforced by the department of justice and the ftc act is enforced by the ftc,3,0
1658,1658,this decentralised sectoral approach is similar to the sectoral approach of the uk,3,26
1659,1659,similarly the uk intends to rely largely on the existing powers of regulators to regulate ai systems rather than establishing a new monitoring body for ai regulation,3,8
1660,1660,it is specified in the uks policy document establishing a pro innovation approach to regulating ai  that ai will be regulated based on its use and the impact it has on individuals groups and businesses within a particular context and that responsibility will be delegated to regulators for designing and implementing proportionate regulatory responses,3,8
1661,1661,the rationale for this is that the potential risk associated with a system will depend on the context of its application with sector based regulators likely to have the most relevant knowledge about the actual impact on an individual within a specific context and the most appropriate response,3,3
1662,1662,due to the uk following a sector led approach to ai governance the specific monitoring and enforcement mechanisms will depend on the powers afforded to each specific regulator,3,8
1663,1663,for instance the equalities and human rights commission has powers to provide guidance on equalities law while the competition and markets authority has powers related to consumer law and competition,3,26
1664,1664,the policy document also states that there is a need to design a suitable monitoring and evaluation framework to monitor progress as well as criteria for future updates to the framework to ensure a robust approach to identifying and addressing evolving risks,3,17
1665,1665,this will be undertaken on two levels both at the overall system level and at the individual regulator level,3,17
1666,1666,on top of this the uks emphasis on proportionate and light touch regulatory policies means that there is also a significant role for self monitoring and enforcement within the uks approach,3,8
1667,1667,the role of third party audit is particularly notable in this respect,3,0
1668,1668,for instance in the centre for data ethics and innovations cdei ai assurance roadmap  a five year vision is outlined statingour vision is that the uk will have a thriving and effective ai assurance ecosystem within the next  years,3,2
1669,1669,strong existing professional services firms alongside innovative start ups and scale ups will provide arange of services to build justified trust in aithis third party assurance industry is seen as assessing testing and verifying ai systems of a provider to assure a user that their system is trustworthy,3,18
1670,1670,this vision is complemented by the cdeis ai assurance guide  which guides practitioners about how different assurance techniques such as bias audits and risk assessments can be applied to ai,3,3
1671,1671,in china monitoring and enforcement is also conducted by a variety of regulators each with different responsibilities and often taking different approaches,3,8
1672,1672,the ministry of science and technologys  chinas overarching coordinative body for ai  published ethical principles and norms,3,16
1673,1673,as these provisions are voluntary they are not supported by formal regulatory oversight that said they should be understood in the broader context of government pressure to strengthen industry self discipline,3,1
1674,1674,the cac chinas internet regulator has been most active in introducing hard regulatory measures related to different ai technologies,3,8
1675,1675,this body is responsible for the use of algorithms related to online content cybersecurity data security and privacy,3,11
1676,1676,as mentioned the cacs monitoring and enforcement powers are drawn from primary legislative documents eg,3,19
1677,1677,the cybersecurity law data security law personal information protection law which are explicated in secondary regulations eg,3,11
1678,1678,on recommender systems or generative algorithms,3,13
1679,1679,given the recency of the publication of these regulations practical examples of enforcement are limited,3,1
1680,1680,however the use of cybersecurity reviews by the cac which derive from the same primary data protection legislation suggest that active enforcement may take place,3,11
1681,1681,for instance a cybersecurity review into the ride hailing company didis practices resulted in a  billion fine,3,0
1682,1682,while in this case the issue was largely with the collection and processing of personal data it is plausible these reviews could also lead to fines based on provisions related to ai for instance based on security or transparency concerns about a system,3,0
1683,1683,looking forward the publication of the opinion on strengthening the ethics and governance in science and technology indicates that harder regulatory measures may be introduced and enforced by other regulatory bodies within china given the emphasis within the document on improving regulatory frameworks for science and technology research,3,8
1684,1684,in the past few years early mover governments have made significant progress in developing their approaches to ai regulatory policy,3,8
1685,1685,canada china the eu the uk and the usa all emphasise the importance of governing ai well and have introduced regulatory policies to fulfil this aim the approaches taken in each jurisdiction are distinct,3,2
1686,1686,at a high level similarities can be drawn between the eu and canada on the one hand who introduce horizontal hard law regulations for ai that are relatively centralised and the usa uk and china on the other who rely more on different types of decentralised regulatory policy,3,8
1687,1687,other more specific similarities can also be drawn for example the introduction of comparable thresholds to mitigate ai risks and their potential impact for the eu and canada,3,4
1688,1688,yet several key differences can be seen even between the seemingly similar approaches outlined above,3,14
1689,1689,for instance canadas focus on impact rather than risk is closer to the uk and usas emphasis on actual impact rather than hypothetical risk that ai technologies can have on individuals and groups,3,3
1690,1690,additionally although the usa and china have both taken more decentralised approaches to ai regulatory policy than the eu or canada chinas cyberspace administration has introduced hard law initiatives for specific ai technologies while the usas federal approach has largely relied on voluntary measures or guidance on applying existing sectoral legislation,3,8
1691,1691,while differences in ai regulatory policies are understandable and expected given the different aspirations and governance institutions of each jurisdiction some types of divergence could bring about negative outcomes,3,8
1692,1692,in particular a fragmented regulatory environment that lacks a high degree of mutual recognition could create barriers for interoperability and trade,3,8
1693,1693,although it is too early to assess the likelihood of this type of landscape emerging for ai regulatory policy it is a plausible outcome if distinct or mutually exclusive regulatory requirements are introduced given this as the efforts to introduce ai regulatory policies progress it is vital that stakeholders understand the similarities and differences between governments approaches so that they are able to reasonably assess the possibility of fragmentation and promote deeper cooperation,3,8
1694,1694,this report and our subsequent publications in this project will aid stakeholders in having this contextualised understanding of ai regulatory policy as ai regulatory policy continues to mature it is crucial that policymakers and other key stakeholders leverage this contextual understanding to promote regulatory cooperation coordination and where appropriate alignment,3,8
1695,1695,we filtered ai governance documents for relevance based on the following criteria inclusion criteria hard or soft law initiatives that are designed to govern ai technologies which have been drafted or published by a national levelgovernment institution including hard law introduced by national level government institutions inclusiveof both primary and secondary legislation eg,3,8
1696,1696,the eu ai act chinas provisions on the administration of deep synthesis internet information services soft law by national level government institutions eg,3,2
1697,1697,uks algorithmic transparency recording standard or usa nist ai risk management framework,3,0
1698,1698,exclusion criteria governance initiatives that do not include a hard or soft regulatory element that are drafted by sub national or non governmental bodies or that do not specifically focus on ai in any part of the document,3,8
1699,1699,this will include strategies that dont specify ai governance approach eg,3,8
1700,1700,uk national ai strategy state or municipal government initiatives eg,3,8
1701,1701,new yorks ai audit law legislative documents which do not specifically relate to ai even if theyare used to enforce protections eg,3,8
1702,1702,ftc act,3,0
1703,1703,document focused on regulatory policies for ai in the defence sector,3,8
1704,1704,we selected the above inclusionexclusion criteria to keep the document analysis manageable within the timeframe of this project while not excluding any key documents,3,24
1705,1705,however we acknowledge that this inclusionexclusion criteria still creates some issues for instance the chances of some draft regulations being passed is higher than others eg,3,26
1706,1706,the eu ai act vs the usas algorithmic accountability act some documents which do not explicitly mention ai technologies are necessary for understanding a jurisdictions approach to governing these technologies,3,2
1707,1707,to overcome these issues we considered the wider context of the documents in our analysis for instance by specifying that some legislation are more likely to pass than others and linking specific regulatory documents that act as reference points for a countrys approach to regulatory policy,3,8
1708,1708,the new rules are proportionate foster innovation growth and competitiveness and facilitate the scaling up of smaller platforms smes and start ups,4,0
1709,1709,the responsibilities of users platforms and public authorities are rebalanced according to european values placing citizens at the centre,4,2
1710,1710,the rulesbetter protect consumers and their fundamental rights onlineestablish a powerful transparency and a clear accountability framework for online platformsfoster innovation growth and competitiveness within the single marketfor citizensbetter protection of fundamental rightsmore choice lower pricesless exposure to illegal contentfor providers of digital serviceslegal certainty harmonisation of ruleseasier to start up and scale up in europefor business users of digital servicesmore choice lower pricesaccess to eu wide markets through platformslevel playing field against providers of illegal contentfor society at largegreater democratic control and oversight over systemic platformsmitigation of systemic risks such as manipulation or disinformationwhich providers are covered,4,7
1711,1711,the digital services act includes rules for online intermediary services which millions of europeans use every day,4,2
1712,1712,the obligations of different online players match their role size and impact in the online ecosystem,4,7
1713,1713,intermediary services offering network infrastructure internet access providers domain name registrars including alsohosting services such as cloud and webhosting services including alsoonline platforms bringing together sellers and consumers such as online marketplaces app stores collaborative economy platforms and social media platforms,4,7
1714,1714,very large online platforms pose particular risks in the dissemination of illegal content and societal harms,4,7
1715,1715,specific rules are foreseen for platforms reaching more than  of  million consumers in europe,4,7
1716,1716,all online intermediaries offering their services in the single market whether they are established in the eu or outside will have to comply with the new rules,4,2
1717,1717,micro and small companies will have obligations proportionate to their ability and size while ensuring they remain accountable,4,0
1718,1718,in addition even if micro and small companies grow significantly they would benefit from a targeted exemption from a set of obligations during a transitional  month period,4,0
1719,1719,the digital services act significantly improves the mechanisms for the removal of illegal content and for the effective protection of users fundamental rights online including the freedom of speech,4,23
1720,1720,it also creates a stronger public oversight of online platforms in particular for platforms that reach more than  of the eus population,4,7
1721,1721,this means concretelymeasures to counter illegal goods services or content online such as a mechanism for users to flag such content and for platforms to cooperate with trusted flaggers,4,7
1722,1722,new obligations on traceability of business users  in online market places to help identify sellers of illegal goods or reasonable efforts by online market places to randomly check whether products or services have been identified as being illegal in any official database,4,7
1723,1723,effective safeguards for users including the possibility to challenge platforms content moderation decisions,4,7
1724,1724,ban on certain type of targeted adverts on online platforms when they target children or when they use special categories of personal data such as ethnicity political views sexual orientation,4,7
1725,1725,transparency measures for online platforms on a variety of issues including on the algorithms used for recommendations,4,7
1726,1726,obligations for very large platforms and very large online search engines to prevent the misuse of their systems by taking risk based action and by independent audits of their risk management systems,4,7
1727,1727,access for researchers to key data of the largest platforms and search engines in order to understand how online risks evolve,4,7
1728,1728,oversight structure to address the complexity of the online space eu countries will have the primary role supported by a new european board for digital services for very large platforms supervision and enforcement by the commission,4,2
1729,1729,the digital services act and digital markets act create a safer and more open digital space for all users where their fundamental rights are protected and where they have access to quality digital services at lower prices,4,23
1730,1730,a safer online environmenttoday online platforms can be misused to spread illegal content such as hate speech terrorist content or child sexual abuse material as well as sell dangerous goods and counterfeit products or to offer illegal services exposing citizens to harm,4,7
1731,1731,what the new digital services act changeseasy and clear ways to report illegal content goods or services on online platforms,4,23
1732,1732,due diligence obligations for platforms and stronger obligations for very large platforms where the most serious harms occur,4,7
1733,1733,authorities will be better equipped to protect citizens by supervising platforms and enforcing rules together across the union,4,1
1734,1734,better protected consumerstoday the fundamental rights of european citizens are not adequately protected online,4,2
1735,1735,platforms can for example decide to delete users content without informing the user or providing a possibility of redress,4,7
1736,1736,this has strong implications for users freedom of speech,4,12
1737,1737,what the new digital services act changesusers are informed about and can contest removal of content by platforms,4,23
1738,1738,users will have access to dispute resolution mechanisms in their own country,4,10
1739,1739,transparent terms and conditions for platforms,4,7
1740,1740,more safety and better knowledge of the real sellers of products that users buy,4,0
1741,1741,stronger obligations for very large online platforms to assess and mitigate risks at the level of the overall organisation of their service for users rights where restrictions of rights and risks of viral spreading of illegal or harmful content are most impactful,4,7
1742,1742,fast crisis response mechanisms with additional risk management measures for public health and security crises,4,22
1743,1743,new protections for minors,4,12
1744,1744,bans on targeted adverts on online platforms directed at minors or using sensitive personal data,4,7
1745,1745,access to platforms data for researchers to understand risks on society and fundamental rights,4,7
1746,1746,empowered citizens and userstoday platforms optimise the presentation of information to capture attention and drive revenue but their users are often unaware of how their systems sort content and how platforms profile them,4,7
1747,1747,the manipulation of recommender systems and abuse of advertising systems can fuel dangerous disinformation and propagation of illegal content,4,7
1748,1748,quality digital services at lower pricethe systemic role of a few online platforms affects the lives of billions of users and millions of companies in europe,4,7
1749,1749,some companies have a major impact on control the access to and are entrenched in digital markets,4,0
1750,1750,they can impose unfair take it or leave it conditions on both their business users and consumers,4,0
1751,1751,what the new digital markets act changesban of unfair practices opening up the possibility for business users to offer consumers more choices of innovative services,4,0
1752,1752,better interoperability with services that are alternatives to those of gatekeepers,4,1
1753,1753,easier possibilities for consumers to switch platforms if they wish so,4,7
1754,1754,better services and lower prices for consumers,4,0
1755,1755,the digital services act and digital markets act create a level playing field that will allow innovative digital businesses to grow within the single market and compete globally,4,0
1756,1756,what the new digital services act changesthe same rules will apply in the entire union and will be the basis of a large domestic market for digital services to grow and prosper,4,23
1757,1757,cross border digital trade in the single market is expected to increase up to,4,0
1758,1758,small players will have legal certainty to develop services and protect users from illegal activities and they will be supported by standards codes of conduct and guidelines,4,7
1759,1759,small and micro enterprises are exempted from the most costly obligations but are free to apply the best practices for their competitive advantage,4,0
1760,1760,support to scale up exemptions for small enterprises are extended for  months after they scale past the turnover and personnel thresholds that qualify them as small companies,4,22
1761,1761,by tackling illegal activities and product online legitimate business can be thriving online,4,0
1762,1762,what the new digital services act changesremoving disincentives for companies to take voluntary measures to protect their users from illegal content goods or services,4,23
1763,1763,businesses will use new simple and effective mechanisms for flagging illegal content and goods that infringe their rights including intellectual property rights or compete on an unfair level,4,0
1764,1764,businesses may also become trusted flaggers of illegal content or goods with special priority procedures and tight cooperation with platforms,4,0
1765,1765,enhanced obligations for marketplaces to apply dissuasive measures such as know your business customer policies make reasonable efforts to perform random checks on products sold on their service or adopt new technologies for product traceability,4,0
1766,1766,smes and start ups are dependent on large platforms terms and conditions as to how their content is ranked and advertised and how their communication channels through platforms are moderated,4,7
1767,1767,businesses dont have access to data related to their consumers and stemming from their activity on a gatekeeper platform while since such data allows the business to adapt its market strategy,4,0
1768,1768,this is problematic for businesses in direct competition with a gatekeeper who can use such data to its own interest,4,9
1769,1769,what the new rules changethe digital services act will make the internal processes of online platforms more transparent and allow more informed business decisions,4,23
1770,1770,the digital markets act will give business access to certain data held by gatekeepers,4,0
1771,1771,they will also  be able to choose among different platforms where to offer their services or products,4,0
1772,1772,they would also have an increased possibility to switch and combine services according to their needs,4,0
1773,1773,a fair and balanced internal marketgatekeepers sometimes play a dual role and favour their own services leading to the exclusion of  business users who are dependent on them to reach consumers,4,0
1774,1774,this reduces choice for the businesses potentially undermines the quality of service and increases prices for consumers,4,0
1775,1775,of businesses and business users encountered unfair trading conditions on large platforms according to the outcome of the open public consultation on a new competition tool,4,7
1776,1776,what the new digital markets act changesopen up new opportunities for businesses who will be able to innovate and compete against gatekeepers own services in equal terms,4,0
1777,1777,consumers will be able to see which are the best options available and not just those that the gatekeepers want them to see,4,0
1778,1778,increased legal certainty for businessesgatekeeper platforms have the power to act as private rule makers that can unilaterally impose terms and conditions on its business users,4,7
1779,1779,what the new digital markets act changesbusiness users will know what to expect when dealing with gatekeepers,4,0
1780,1780,gatekeepers will know the clear obligations applicable to them,4,1
1781,1781,clearly defined procedural rules will ensure quick decisions that will translate in speedy advantages for both business users and consumers,4,21
1782,1782,what the new digital markets act changesit will allow businesses to have access to more information on how their products or services are performing on third party platforms,4,0
1783,1783,no more unfair ranking of gatekeepers own services and products compared to those offered by other businesses on the same platform,4,6
1784,1784,businesses will be able to more easily attract consumers who can no longer be locked in by gatekeeper platforms,4,0
1785,1785,the new rules will make it easier for smaller businesses and new entrants to grow and expand and compete with gatekeeper platforms,4,0
1786,1786,increased competition can be expected to lead to greater innovation potential amongst smaller businesses as well as improved quality of service with associated increases in consumer welfare,4,0
1787,1787,the digital markets act is expected to trigger a consumer surplus of around  billion euros,4,0
1788,1788,the increased opportunities for a diversity of market operators will lead to increased investment from their side,4,0
1789,1789,this can be expected to have an impact on economic growth ranging between  billion and  billion,4,0
1790,1790,more jobsthe gatekeepers are necessary gateways to the european internal market,4,0
1791,1791,these are platforms that have a significant impact on the internal market serve as an important gateway for business users to reach their customers and which enjoy or will foreseeably enjoy an entrenched and durable position,4,0
1792,1792,what the new digital markets act changesstart up and new businesses will emerge and grow and will create new jobs thanks to a level playing field,4,0
1793,1793,the digital services act and digital markets act set a high global benchmark for regulating digital services with clear obligations tailored to the importance of the online platforms,4,7
1794,1794,clear gatekeeper obligations across the eutoday the way in which gatekeepers conduct their businesses is either largely unregulated or based on sets of rules many of which pre date the digital economy,4,8
1795,1795,this is the case across the eu,4,2
1796,1796,gatekeeper related problems are currently not effectively addressed by member states or the eu in existing regulation,4,8
1797,1797,legal certainty for platformstoday national legislative initiatives in eu member states may partially address the problems identified but also lead to increased regulatory fragmentation in the eu,4,2
1798,1798,this can create increased compliance costs for platforms operating cross border,4,25
1799,1799,what the new digital markets act changesgatekeepers know beforehand the obligations they have to respect,4,22
1800,1800,other platforms will not be subject to these rules but will be able to benefit from fairer behaviours when doing businesses with gatekeepers,4,7
1801,1801,reduced compliance costs for gatekeepers and their business users,4,0
1802,1802,what the new digital services act changesone set of rules applicable throughout the eu,4,2
1803,1803,the new rules set up mechanisms for the commission and member states to coordinate their actions and ensure a proper implementation of the framework across the eu,4,2
1804,1804,tailored asymmetric obligationstoday users are exposed to illegal goods content or services and all decisions are mostly at the discretion of the platforms,4,7
1805,1805,the biggest impact comes from those platforms which have become quasi public spaces for communication and trading,4,0
1806,1806,what the new digital services act changesmeasures to counter illegal goods services or content online such as a mechanism for users to flag such content and for platforms to cooperate with trusted flaggers,4,23
1807,1807,new obligations on traceability of business users in online market places to help identify sellers of illegal goods,4,12
1808,1808,effective safeguards for users including the possibility to challenge platforms content moderation decisions,4,7
1809,1809,transparency measures for online platforms that are wide ranging including on the algorithms used for recommendation,4,7
1810,1810,obligations for very large platforms that reach more than  of the eus population to prevent abuse of their systems by taking risk based action and through independent audits of their risk management systems,4,7
1811,1811,researchers will have access to data of key platforms in order to scrutinise how platforms work,4,7
1812,1812,codes of conduct and technical standards will assist platforms and other players in their compliance with the new rules,4,27
1813,1813,other codes will enhance measures taken to ensure accessibility of platforms for people with disabilities or support further measures on advertising,4,27
1814,1814,all online intermediaries offering their services in the single market whether they are established in the eu or outside will have to comply with the new rules,4,2
1815,1815,oversight structure to match the complexity of the online space member states will have the primary role supported by a new european board for digital services for very large platforms supervision and enforcement by the commission,4,7
1816,1816,clarified liability and efficient compliance mechanismthe liability exemption for online intermediaries is a cornerstone of internet regulation ensuring that it is possible to tackle illegal content goods or services swiftly but also that platforms are not incentivised to remove legitimate content and are not obliged to monitor their users,4,7
1817,1817,today some of the rules led to fragmentation across the single market and there are uncertainties for diligent platforms who want to take measures and protect their users from illegal content,4,7
1818,1818,what the new digital services act changesreinforce and further clarify the conditions for liability exemptions platforms and other intermediaries are not liable for users unlawful behaviour unless they are aware of illegal acts and fail to remove them,4,23
1819,1819,rules for the liability exception will now be harmonised and uniform across the eu thanks to a directly applicable regulation,4,10
1820,1820,new clarifications on how these conditions apply for consumer protection liability,4,10
1821,1821,solving the paradox of voluntary measures taken by small platforms diligent platforms are not liable for illegal content they detect themselves,4,7
1822,1822,more legal certainty on interaction with authorities platforms will have to cooperate with authorities that issue legal orders that have minimum common criteria,4,8
1823,1823,small online platform scaling up in the euthere are more than  platforms in the eu over  of which are small and medium sized enterprises according to commission estimates,4,7
1824,1824,digital services in the eu currently have to deal with  different sets of national rules,4,2
1825,1825,only the largest companies can deal with the resulting compliance costs,4,25
1826,1826,what the new digital services act changesensure that small online platforms are not disproportionately affected but they remain accountable,4,7
1827,1827,small and micro enterprises are exempted from the most costly obligations but are free to apply the best practices for their competitive advantage,4,0
1828,1828,a single set of rules for the entire eu will create the right conditions for cross border digital services to prosper,4,2
1829,1829,this can create up to  more cross border digital trade in the single market,4,0
1830,1830,small players will have legal certainty to develop services and protect users from illegal activities as they will be supported by standards and guidelines,4,0
1831,1831,what are the next steps,4,22
1832,1832,following the entry into force of the digital services act on  november  online platforms will have  months to report the number of active end users  february  on their websites,4,7
1833,1833,the commission is also inviting all online platforms to notify to it the published numbers,4,7
1834,1834,based on these user numbers the commission will make an assessment as to whether a platform should be designated a very large online platform or search engine,4,7
1835,1835,following such a designation decision by the commission the entity in question will have  months to comply with the obligations under the dsa including carrying out and providing to the commission the first annual risk assessment exercise,4,19
1836,1836,eu member states will need to empower their digital services coordinators by  february  the general date of entry in application of the dsa when the dsa is fully applicable for all entities in its scope,4,23
1837,1837,context of the proposalreasons for and objectives of the proposalsince the adoption of directive ec  the e commerce directive new and innovative information society digital services have emerged changing the daily lives of union citizens and shaping and transforming how they communicate connect consume and do business,4,2
1838,1838,those services have contributed deeply to societal and economic transformations in the union and across the world,4,0
1839,1839,at the same time the use of those services has also become the source of new risks and challenges both for society as a whole and individuals using such services,4,12
1840,1840,digital services can support achieving sustainable development goals by contributing to economic social and environmental sustainability,4,23
1841,1841,the coronavirus crisis has shown the importance of digital technologies in all aspects of modern life,4,13
1842,1842,it has clearly shown the dependency of our economy and society on digital services and highlighted both the benefits and the risks stemming from the current framework for the functioning of digital services,4,23
1843,1843,in the communication shaping europes digital future   the commission committed to update the horizontal rules that define the responsibilities and obligations of providers of digital services and online platforms in particular,4,2
1844,1844,in doing so the commission has taken account of the issues identified in the european parliaments own initiative reports and analysed the proposals therein,4,19
1845,1845,the european parliament adopted two resolutions on the basis of article  of the treaty on the functioning of the european union tfeu on the digital services act  improving the functioning of the single market  and on the digital services act adapting commercial and civil law rules for commercial entities operating online,4,2
1846,1846,the european parliament also adopted a resolution under the non legislative procedure on the digital services act and fundamental rights issues posed,4,2
1847,1847,in substance the resolutions are complementary in many aspects,4,26
1848,1848,they include a strong call for maintaining the core principles of the e commerce directive and for protecting fundamental rights in the online environment as well as online anonymity wherever technically possible,4,7
1849,1849,they call for transparency information obligations and accountability for digital services providers and advocate for effective obligations to tackle illegal content online,4,7
1850,1850,they also advocate for public oversight at eu and national level and cooperation between competent authorities across jurisdictions in enforcing the law especially when addressing cross border matters,4,1
1851,1851,the resolution on digital services act  improving the functioning of the single market calls for an ambitious reform of the existing eu e commerce legal framework while maintaining the core principles of its liability regime the prohibition of general monitoring and the internal market clause which it considers to be still valid today,4,2
1852,1852,confirming the objectives of the e commerce directive the resolution calls for measures which have consumer protection at their core by including a detailed section on online marketplaces and which ensure consumer trust in the digital economy while respecting users fundamental rights,4,2
1853,1853,the resolution also advocates for rules to underpin a competitive digital environment in europe and envisages the digital services act as a standard setter at global level,4,23
1854,1854,the resolution on digital services act adapting commercial and civil law rules for commercial entities operating online calls for more fairness transparency and accountability for digital services content moderation processes ensuring that fundamental rights are respected and guaranteeing independent recourse to judicial redress,4,23
1855,1855,the resolution also includes the request for a detailed notice and action mechanism addressing illegal content comprehensive rules about online advertising including targeted advertising and enabling the development and use of smart contracts,4,7
1856,1856,the non legislative resolution on the digital services act and fundamental rights issues posed highlights the need for legal clarity for platforms and users and respect for fundamental rights given the rapid development of technology,4,23
1857,1857,it calls for harmonised rules for addressing illegal content online and for liability exemptions and content moderation,4,7
1858,1858,the resolution also includes clear reporting and transparency responsibilities for platforms and authoritiesthe councils conclusions  also welcomed the commissions announcement of a digital services act emphasising the need for clear and harmonised evidence based rules on responsibilities and accountability for digital services that would guarantee internet intermediaries an appropriate level of legal certainty and stressing the need to enhance european capabilities and the cooperation of national authorities preserving and reinforcing the fundamental principles of the single market and the need to enhance citizens safety and to protect their rights in the digital sphere across the single market,4,23
1859,1859,this call for action was reiterated in the councils conclusions of  october,4,22
1860,1860,building on the key principles set out in the e commerce directive which remain valid today this proposal seeks to ensure the best conditions for the provision of innovative digital services in the internal market to contribute to online safety and the protection of fundamental rights and to set a robust and durable governance structure for the effective supervision of providers of intermediary services,4,2
1861,1861,the proposal defines clear responsibilities and accountability for providers of intermediary services and in particular online platforms such as social media and marketplaces,4,7
1862,1862,by setting out clear due diligence obligations for certain intermediary services including notice and action procedures for illegal content and the possibility to challenge the platforms content moderation decisions the proposal seeks to improve users safety online across the entire union and improve the protection of their fundamental rights,4,7
1863,1863,furthermore an obligation for certain online platforms to receive store and partially verify and publish information on traders using their services will ensure a safer and more transparent online environment for consumers,4,7
1864,1864,recognising the particular impact of very large online platforms on our economy and society the proposal sets a higher standard of transparency and accountability on how the providers of such platforms moderate content on advertising and on algorithmic processes,4,7
1865,1865,it sets obligations to assess the risks their systems pose to develop appropriate risk management tools to protect the integrity of their services against the use of manipulative techniques,4,15
1866,1866,the operational threshold for service providers in scope of these obligations includes those online platforms with a significant reach in the union currently estimated to be amounting to more than  million recipients of the service,4,7
1867,1867,this threshold is proportionate to the risks brought by the reach of the platforms in the union where the unions population changes by a certain percentage the commission will adjust the number of recipients considered for the threshold so that it consistently corresponds to   of the unions population,4,6
1868,1868,additionally the digital services act will set out a co regulatory backstop including building on existing voluntary initiatives,4,23
1869,1869,the proposal maintains the liability rules for providers of intermediary services set out in the e commerce directive  by now established as a foundation of the digital economy and instrumental to the protection of fundamental rights online,4,7
1870,1870,those rules have been interpreted by the court of justice of the european union thus providing valuable clarifications and guidance,4,2
1871,1871,nevertheless to ensure an effective harmonisation across the union and avoid legal fragmentation it is necessary to include those rules in a regulation,4,1
1872,1872,it is also appropriate to clarify some aspects of those rules to eliminate existing disincentives towards voluntary own investigations undertaken by providers of intermediary services to ensure their users safety and to clarify their role from the perspective of consumers in certain circumstances,4,1
1873,1873,those clarifications should help smaller innovative providers scale up and grow by benefitting from greater legal certainty,4,0
1874,1874,a deeper borderless single market for digital services requires enhanced cooperation among member states to guarantee effective oversight and enforcement of the new rules set out in the proposed regulation,4,23
1875,1875,the proposal sets clear responsibilities for the member state supervising the compliance of service providers established in its territory with the obligations set by the proposed regulation,4,27
1876,1876,this ensures the swiftest and most effective enforcement of rules and protects all eu citizens,4,2
1877,1877,it aims at providing the simple and clear processes for both citizens and service providers to find relief in their interactions with supervising authorities,4,1
1878,1878,where systemic risks emerge across the union the proposed regulation provides for supervision and enforcement at union level,4,3
1879,1879,consistency with existing policy provisions in the policy areathe current eu legal framework regulating digital services is underpinned first and foremost by the e commerce directive,4,2
1880,1880,this proposed regulation is without prejudice to the e commerce directive and builds on the provisions laid down therein notably on the internal market principle set out in article,4,1
1881,1881,the proposed regulation provides for a cooperation and coordination mechanism for the supervision of the obligations it imposes,4,8
1882,1882,with regard to the horizontal framework of the liability exemption for providers of intermediary services this regulation deletes articles   in the e commerce directive and reproduces them in the regulation maintaining the liability exemptions of such providers as interpreted by the court of justice of the european union,4,1
1883,1883,depending on the legal system of each member state and the field of law at issue national judicial or administrative authorities may order providers of intermediary services to act against certain specific items of illegal content,4,1
1884,1884,such orders in particular where they require the provider to prevent that illegal content reappears must be issued in compliance with union law in particular with the prohibition of general monitoring obligations as interpreted by the court of justice of the european union,4,1
1885,1885,this proposal in particular its article  leaves this case law unaffected,4,10
1886,1886,this proposal should constitute the appropriate basis for the development of robust technologies to prevent the reappearance of illegal information accompanied with the highest safeguards to avoid that lawful content is taken down erroneously such tools could be developed on the basis of voluntary agreements between all parties concerned and should be encouraged by member states it is in the interest of all parties involved in the provision of intermediary services to adopt and implement such procedures the provisions of this regulation relating to liability should not preclude the development and effective operation by the different interested parties of technical systems of protection and identification and of automated recognition made possible by digital technology within the limits laid down by regulation,4,12
1887,1887,consistency with other union policiesthe proposed regulation introduces a horizontal framework for all categories of content products services and activities on intermediary services,4,1
1888,1888,the illegal nature of such content products or services is not defined in this regulation but results from union law or from national law in accordance with union law,4,1
1889,1889,sector specific instruments do not cover all the regulatory gaps evidenced in the impact assessment report they do not provide fully fledged rules on the procedural obligations related to illegal content and they only include basic rules on transparency and accountability of service providers and limited oversight mechanisms,4,1
1890,1890,in addition sector specific laws cover situations where adapted approaches are necessary,4,8
1891,1891,in terms of scope they are limited from two perspectives,4,26
1892,1892,first the sector specific interventions address a small subset of issues eg,4,17
1893,1893,copyright infringements terrorist content child sexual abuse material or illegal hate speech some illegal products,4,1
1894,1894,second they only cover the dissemination of such content on certain types of services eg,4,7
1895,1895,sub set of online platforms for copyright infringements only video sharing platforms and only as regards audiovisual terrorist content or hate speech,4,7
1896,1896,however it is important that the relationship between the new proposed regulation and the sector specific instruments is clarified,4,1
1897,1897,the proposed regulation complements existing sector specific legislation and does not affect the application of existing eu laws regulating certain aspects of the provision of information society services which apply as lex specialis,4,2
1898,1898,by way of example the obligations set out in directive ec as amended by directive eu  on video sharing platform providers avsmd as regards audiovisual content and audiovisual commercial communications will continue to apply,4,2
1899,1899,however this regulation applies to those providers to the extent that the avsmd or other union legal acts such as the proposal for a regulation on addressing the dissemination on terrorist content online do not contain more specific provisions applicable to them,4,1
1900,1900,the framework established in the regulation eu  on promoting fairness and transparency for business users of online intermediation services to ensure that business users of such services and corporate website users in relation to online search engines are granted appropriate transparency fairness and effective redress possibilities will apply as lex specialis,4,7
1901,1901,furthermore the rules set out in the present proposal will be complementary to the consumer protection acquis and specifically with regard to directive eu  amending council directive eec and directives ec ec and eu which establish specific rules to increase transparency as to certain features offered by certain information society services,4,2
1902,1902,this proposal is without prejudice to the regulation eu  the general data protection regulation and other union rules on protection of personal data and privacy of communications,4,2
1903,1903,for example the measures concerning advertising on online platforms complement but do not amend existing rules on consent and the right to object to processing of personal data,4,1
1904,1904,they impose transparency obligations towards users of online platforms and this information will also enable them to make use of their rights as data subjects,4,7
1905,1905,they also enable scrutiny by authorities and vetted researchers on how advertisements are displayed and how they are targeted,4,12
1906,1906,this proposal will be complemented by further actions under the european democracy action plan com  final with the objective of empowering citizens and building more resilient democracies across the union,4,2
1907,1907,in particular the rules on codes of conduct established in this regulation could serve as a basis and be complemented by a revised and strengthened code of practice on disinformation building on the guidance of the commission,4,19
1908,1908,the proposal is also fully consistent and further supports equality strategies adopted by the commission in the context of the union of equality,4,6
1909,1909,the proposal is without prejudice to the commissions initiative aimed at improving the labour conditions of people working through digital platforms,4,23
1910,1910,finally the proposed regulation builds on the recommendation on illegal content of,4,7
1911,1911,it takes account of experiences gained with self regulatory efforts supported by the commission such as the product safety pledge   the memorandum of understanding against counterfeit goods   the code of conduct against illegal hate speech   and the eu internet forum with regard to terrorist content,4,2
1912,1912,legal basis subsidiarity and proportionalitylegal basisthe legal basis for the proposal is article  of the treaty on the functioning of the european union which provides for the establishment of measures to ensure the functioning of the internal market,4,2
1913,1913,the primary objective of this proposal is to ensure the proper functioning of the internal market in particular in relation to the provision of cross border digital services more specifically intermediary services,4,0
1914,1914,in line with this objective the proposal aims to ensure harmonised conditions for innovative cross border services to develop in the union by addressing and preventing the emergence of obstacles to such economic activity resulting from differences in the way national laws develop taking into account that several member states have legislated or intend to legislate on issues such as the removal of illegal content online diligence notice and action procedures and transparency,4,1
1915,1915,at the same time the proposal provides for the appropriate supervision of digital services and cooperation between authorities at union level therefore supporting trust innovation and growth in the internal market,4,23
1916,1916,subsidiaritytaking into account that the internet is by its nature cross border the legislative efforts at national level referred to above hamper the provision and reception of services throughout the union and are ineffective in ensuring the safety and uniform protection of the rights of union citizens and businesses online,4,1
1917,1917,harmonising the conditions for innovative cross border digital services to develop in the union while maintaining a safe online environment can only be served at union level,4,1
1918,1918,union level action provides predictability and legal certainty and reduces compliance costs across the union,4,1
1919,1919,at the same time it fosters the equal protection of all union citizens by ensuring that action against illegal content online by providers of intermediary services is consistent regardless of their place of establishment,4,1
1920,1920,a well coordinated supervisory system reinforced at union level also ensures a coherent approach applicable to providers of intermediary services operating in all member states,4,1
1921,1921,to effectively protect users online and to avoid that union based digital service providers are subject to a competitive disadvantage it is necessary to also cover to relevant service providers established outside of the union whose operate on the internal market,4,0
1922,1922,proportionalitythe proposal seeks to foster responsible and diligent behaviour by providers of intermediary services to ensure a safe online environment which allows union citizens and other parties to freely exercise their fundamental rights in particular the freedom of expression and information,4,7
1923,1923,key features of the proposal limit the regulation to what is strictly necessary to achieve those objectives,4,27
1924,1924,in particular the proposal sets asymmetric due diligence obligations on different types of digital service providers depending on the nature of their services and their size to ensure their services are not misused for illegal activities and providers operate responsibly,4,23
1925,1925,this approach addresses certain identified problems only there where they materialise while not overburdening providers unconcerned by those problems,4,27
1926,1926,certain substantive obligations are limited only to very large online platforms which due to their reach have acquired a central systemic role in facilitating the public debate and economic transactions,4,7
1927,1927,very small providers are exempt from the obligations altogether,4,1
1928,1928,as regards digital service providers established outside of the union offering services in the union the regulation requires the appointment of a legal representative in the union to ensure effective oversight and where necessary enforcement,4,1
1929,1929,proportionate to the obligations and taking into account the cross border nature of digital services the proposal will introduce a cooperation mechanism across member states with enhanced union level oversight of very large online platforms,4,1
1930,1930,additionally the proposal does not amend sector specific legislation or the enforcement and governance mechanisms set thereunder but provides for a horizontal framework to rely on for aspects beyond specific content or subcategories of services regulated in sector specific acts,4,26
1931,1931,by establishing a clear framework accompanied by cooperation between and with member states as well as by self regulation this proposal aims to enhance legal certainty and increase trust levels while staying relevant and effective in the long term because of the flexibility of the cooperation framework,4,1
1932,1932,choice of the instrumentarticle  of the treaty on the functioning of the european union gives the legislator the possibility to adopt regulations and directives,4,2
1933,1933,the commission has decided to put forward a proposal for a regulation to ensure a consistent level of protection throughout the union and to prevent divergences hampering the free provision of the relevant services within the internal market as well as guarantee the uniform protection of rights and uniform obligations for business and consumers across the internal market,4,2
1934,1934,this is necessary to provide legal certainty and transparency for economic operators and consumers alike,4,0
1935,1935,the proposed regulation also ensures consistent monitoring of the rights and obligations and equivalent sanctions in all member states as well as effective cooperation between the supervisory authorities of different member states and at union level,4,1
1936,1936,results of ex post evaluations stakeholder consultations and impact assessmentsex post evaluationsfitness checks of existing legislationthis proposal builds on the evaluation of the e commerce directive conducted as a back to back evaluation with the impact assessment accompanying the proposal,4,17
1937,1937,the specific objectives of the e commerce directive were to ensure  a well functioning internal market for digital services ii the effective removal of illegal content online in full respect of fundamental rights and iii an adequate level of information and transparency for consumers,4,0
1938,1938,as regards the effectiveness of the e commerce directive the evaluation shows that while the e commerce directive has provided an important incentive for the growth of the internal market for digital services and enabled entry and scaling up of new providers of such services the initial objectives have not been fully achieved,4,0
1939,1939,in particular the dynamic growth of the digital economy and the appearance of new types of service providers raises certain new challenges dealt with differently by member states where the initial set of objectives need to be clarified,4,0
1940,1940,in addition these developments put an additional strain on achieving already existing objectives as the increased legal fragmentation shows,4,0
1941,1941,the evaluation also showed that while several new regulatory instruments make valuable contributions to the attainment of some of the policy objectives set out in the e commerce directive they provide only sector specific solutions for some of the underlying problems eg,4,2
1942,1942,in addressing the proliferation of specific types of illegal activity,4,12
1943,1943,they therefore do not address such issues consistently for the entire digital ecosystem as they are limited to certain types of services or certain types of illegal content,4,7
1944,1944,furthermore while self regulatory initiatives have generally shown positive results they cannot be legally enforced nor do they cover all participants in the digital economy,4,8
1945,1945,as regards the efficiency of the e commerce directive the directive imposed only limited additional costs for member states administrations and providers of information society services,4,2
1946,1946,the evaluation has not revealed particularly high or disproportionate costs and no substantial concerns have been raised regarding impacts on small and medium sized enterprises,4,25
1947,1947,the main concern in this regard is related to the lack of clarity in the cooperation mechanism across member states creating burdens and duplication of costs despite the opposite objective of the directive in particular where the supervision of online platforms is concerned,4,7
1948,1948,this has essentially reduced its efficiency in maintaining the functioning of the internal market,4,0
1949,1949,in relation to questions about the continued relevance of the objectives pursued by the e commerce directive the evaluation shows that the objectives of the e commerce directive continue to remain valid while at the same time there are several new developments that are not well reflected in the existing public policy objectives,4,2
1950,1950,first the open public consultation targeted submissions by stakeholders reports issued by the european parliament  as well as council conclusions  confirm that the existing principles and objectives of the e commerce directive remain valid today,4,2
1951,1951,however new information asymmetries and risks have arisen since the entry into force of the directive notably related to the emergence of online platforms in particular very large ones and the scale of the digital transformation,4,7
1952,1952,this is for example the case in the areas of algorithmic decision making with an impact on how information flows are intermediated online or in online advertising systems,4,0
1953,1953,the evaluation showed that the e commerce directive is coherent with other eu interventions that took place since its adoption,4,2
1954,1954,the evaluation also did not identify any internal in coherence of the e commerce directive,4,1
1955,1955,finally at least parts of the actual benefits of the e commerce directive that the evaluation identified could be considered as eu added value,4,2
1956,1956,it is likely that member states would have continued applying their own regulatory systems without any common set of principles and that some member states would have continued to have no horizontal rules in place at all,4,8
1957,1957,in the absence of robust evidence it is however not possible to draw firm conclusions on the extent of this eu added value,4,14
1958,1958,stakeholder consultationsover the past five years the commission has consulted a wide range of different stakeholders including providers of digital services such as online platforms and other intermediary services businesses trading online media publishers brand owners and other businesses social partners users of digital services civil society organisations national authorities academia the technical community international organisations and the general public,4,7
1959,1959,an array of targeted consultation steps have captured thoroughly stakeholder views on issues related to digital services and platforms over the last years,4,23
1960,1960,the open public consultation on the digital services act was open for  weeks between nd of june and th of september and received  responses and around  position papers from a diverse group of stakeholders,4,23
1961,1961,most feedback was submitted by the general public  from union citizens  from non eu citizens companies businesses organizations  business associations  and ngos,4,2
1962,1962,this was followed by public authorities  academicresearch institutions  trade unions  and consumer and environmental organisations,4,24
1963,1963,overall there is a general agreement amongst stakeholders for a need for action both in addressing online safety and in furthering the internal market for digital services,4,7
1964,1964,stakeholders converge on the continued relevance of the main principles of the e commerce directive and agree that they should be maintained including the internal market principle for the supervision of digital services the liability regime and the prohibition of general monitoring obligations,4,0
1965,1965,stakeholders also broadly agree on the need to upgrade the framework in light of todays challenges by establishing clear obligations for service providers harmonised across the eu,4,27
1966,1966,a majority of respondents all categories included indicated that they have encountered both harmful and illegal content goods or services online and specifically noted an alarming spike during the covid  pandemic,4,7
1967,1967,a large share of respondents who say they have notified illegal content or goods to digital service providers expressed their dissatisfaction with the response and the ineffectiveness of reporting mechanisms after the exposure took place,4,7
1968,1968,moreover users perceive there to be a mismatch between providers policies as stated and their concrete actions,4,14
1969,1969,there is broad consensus including among service providers responding to the consultation on the need for simple standardised transparent notice and action obligations harmonised across the internal market,4,27
1970,1970,this is considered as essential to enable rapid responses to illegal content and enhance legal clarity for users of platforms and for small platforms seeking to scale in the internal market,4,7
1971,1971,respondents also agree on the importance of a redress mechanisms,4,22
1972,1972,concerning online marketplaces several stakeholders flagged the need for more targeted measures such as the identification of sellers,4,0
1973,1973,respondents also generally agree that the territorial scope for these obligations should include all players offering goods information or services in the union regardless of their place of establishment,4,1
1974,1974,a large share of respondents also emphasized the importance of these issues in particular where large platforms are concerned,4,7
1975,1975,there is a general agreement among stakeholders that harmful yet not or at least not necessarily illegal content should not be defined in the digital services act and should not be subject to removal obligations as this is a delicate area with severe implications for the protection of freedom of expression,4,1
1976,1976,however the way algorithmic systems shape information flows online is an area of concern among a wide category of stakeholders,4,7
1977,1977,several stakeholders in particular civil society and academics pointed out the need for algorithmic accountability and transparency audits especially with regard to how information is prioritized and targeted,4,16
1978,1978,similarly regarding online advertising stakeholder views echoed the broad concerns around the lack of user empowerment and lack of meaningful oversight and enforcement,4,7
1979,1979,when it comes to enforcement there is a general understanding among stakeholders that cooperation between authorities should be improved both cross border and within each member state,4,1
1980,1980,eu oversight is considered crucial and the majority of respondents seems to favour a unified oversight entity,4,2
1981,1981,collection and use of expertisethe preparatory steps for the proposal rest on an array of studies and expert advice including a number of legal studies commissioned focusing on the implementation of the e commerce directive and the state of legal fragmentation   studies on algorithmic transparency and accountability   as well as internal studies on costs of content moderation liability regimes for intermediaries and cost of non europe with the support of the joint research centre of the european commission,4,7
1982,1982,for gathering the views and perceptions of the general public the commission ran a eurobarometer survey in  with a representative sample of over  respondents from all member states,4,2
1983,1983,the legal analysis also rests on a rich collection of case law in particular by the court of justice of the european union of several provisions of the e commerce directive and related acts such as provisions concerning the interpretation of the notion of information society services  or provisions concerning the liability of intermediary services providers  the commission also gathered expertise and views through targeted consultations and engagement activities including a series of workshops conferences interviews with experts and judges consultations of the expert group on e commerce as well as numerous bilateral meetings and analysis of ad hoc position and research papers from organizations industry representatives civil society and academia,4,2
1984,1984,finally the analysis rests on additional literature review studies and research papers submitted by academics in the open public consultation and other independent studies including the collection of studies carried out for the european parliament,4,2
1985,1985,impact assessmentthe regulatory scrutiny board issued a positive opinion with reservations on the impact assessment including suggestions for improvement,4,17
1986,1986,the impact assessment report was further revised along these lines notably in clarifying the interlinks between the digital services act and the broader regulatory framework providing more detailed descriptions of the policy options and a more detailed analysis of the underlying evidence addressed in the revised impact assessment report,4,17
1987,1987,the importance in our economy and society but also the growing risks brought by digital services will continue to scale,4,0
1988,1988,in the baseline scenario the commission will continue to enforce existing rules including on sector specific issues and will support the self regulatory efforts in place,4,19
1989,1989,however faced with the evolving problems member states will continue to legislate independently,4,1
1990,1990,the legal fragmentation with the resulting patchwork of national measures will not just fail to effectively tackle illegal activities and protect citizens fundamental rights throughout the eu it will also hinder new innovative services from scaling up in the internal market cementing the position of the few players which can afford the additional compliance costs,4,22
1991,1991,this leaves the rule setting and enforcement mostly to very large private companies with ever growing information asymmetry between online services their users and public authorities,4,7
1992,1992,three main policy options were assessed in addition to the baseline,4,22
1993,1993,option  would codify the recommendation of  it would lay down a range of procedural obligations for online platforms to tackle illegal activities conducted by their users,4,21
1994,1994,the obligations would also include the necessary safeguards in order to protect users fundamental rights and ensure transparency,4,11
1995,1995,it would also enhance the administrative cooperation mechanisms for authorities to resolve cross border issues through a digital clearing house facilitating information flows,4,1
1996,1996,option  would in addition to measures in option  remove disincentives for service providers to take voluntary measures against illegal content and introduce measures to enhance transparency around recommender systems and advertising,4,21
1997,1997,the enforcement and cooperation mechanism would be enhanced with the appointment of a central coordinator in each member state,4,1
1998,1998,option  building on the measures outlined in the previous options includes targeted asymmetric measures with stronger obligations for very large online platforms which are prone to the highest levels of risks for the eu society and economy as well as certain limited clarifications of the liability regime for providers of intermediary services and an eu governance system with reinforced oversight and enforcement powers,4,7
1999,1999,the assessment of the economic and social impacts identified and the comparison of its effectiveness efficiency coherence and proportionality showed that option  would most effectively meet the objectives of the intervention by establishing the proportionate framework fit for adapting to emerging challenges in the dynamic digital world,4,17
2000,2000,the components included in option  are also broadly supported by stakeholders including positions from the european parliament and member states,4,2
2001,2001,the preferred option would support the access to the internal market for european union intermediary service providers and their ability to scale up by reducing costs related to the legal fragmentation,4,0
2002,2002,while costs for compliance with due diligence obligations are expected it is estimated this is offset by reducing the current fragmentation through harmonisation,4,17
2003,2003,it is expected to have a positive impact on competitiveness innovation and investment in digital services in particular european union start ups and scale ups offering platform business models but also to varying extents on sectors underpinned and amplified by digital commerce,4,0
2004,2004,the preferred option intends to define the appropriate division of responsibilities between intermediary services their recipients and authorities when fighting against illegal content online,4,21
2005,2005,to do so it introduces an asymmetric approach to the due diligence obligations imposed on very large online platforms this is a supervised risk management approach with an important role of the governance system for enforcement,4,7
2006,2006,the asymmetric obligations are only imposed on very large online platforms which have based on the current data not only the broadest reach but are also the large companies with important turnover,4,7
2007,2007,consequently while the targeted measures are more restrictive than for other companies they are proportionate to the ability of the companies to comply,4,0
2008,2008,for public authorities the proposed option would cut the costs brought by the inefficiencies and duplications in the existing set up for the cooperation of authorities,4,17
2009,2009,while member states would bear the costs of appointing a competent authority new or already established the efficiency gains are expected to outweigh them for the individual authorities through mutualisation of resources better information flows and straight forward processes for interacting with their counterparts across the internal market as well as with service providers,4,1
2010,2010,regulatory fitness and simplificationthe impact assessment accompanying this proposal identifies the sole added value of union intervention addressing the risk of legal fragmentation triggered by divergent regulatory and supervisory approaches hence without accounting for the increased safety and trust on digital services in a possible increase of cross border digital trade of  to  ie,4,8
2011,2011,the equivalent of an increase in turnover generated cross border of eur  billion and up to eur  billion,4,0
2012,2012,with regard to added value in the enforcement of measures the initiative creates important efficiency gains in the cooperation across member states and mutualising some resources for technical assistance at eu level for inspecting and auditing content moderation systems recommender systems and online advertising on very large online platforms,4,17
2013,2013,this in turn leads to an increased effectiveness of enforcement and supervision measures whereas the current system relies to a large extent on the limited capability for supervision in a small number of member states,4,8
2014,2014,fundamental rightsunion citizens and others are exposed to ever increasing risks and harms online  from the spread of illegal content and activities to limitations to express themselves and other societal harms,4,12
2015,2015,the envisaged policy measures in this legislative proposal will substantially improve this situation by providing a modern future proof governance framework effectively safeguarding the rights and legitimate interests of all parties involved most of all union citizens,4,1
2016,2016,the proposal introduces important safeguards to allow citizens to freely express themselves while enhancing user agency in the online environment as well as the exercise of other fundamental rights such as the right to an effective remedy non discrimination rights of the child as well as the protection of personal data and privacy online,4,7
2017,2017,the proposed regulation will mitigate risks of erroneous or unjustified blocking speech address the chilling effects on speech stimulate the freedom to receive information and hold opinions as well as reinforce users redress possibilities,4,1
2018,2018,specific groups or persons may be vulnerable or disadvantaged in their use of online services because of their gender race or ethnic origin religion or belief disability age or sexual orientation,4,6
2019,2019,they can be disproportionately affected by restrictions and removal measures following from unconscious or conscious biases potentially embedded in the notification systems by users and third parties as well as replicated in automated content moderation tools used by platforms,4,17
2020,2020,the proposal will mitigate discriminatory risks particularly for those groups or persons and will contribute to the protection of the rights of the child and the right to human dignity online,4,27
2021,2021,the proposal will only require removal of illegal content and will impose mandatory safeguards when users information is removed including the provision of explanatory information to the user complaint mechanisms supported by the service providers as well as external out of court dispute resolution mechanism,4,11
2022,2022,furthermore it will ensure eu citizens are also protected when using services provided by providers not established in the union but active on the internal market since those providers are covered too,4,2
2023,2023,with regard to service providers freedom to conduct a business the costs incurred on businesses are offset by reducing fragmentation across the internal market,4,0
2024,2024,the proposal introduces safeguards to alleviate the burden on service providers including measures against repeated unjustified notices and prior vetting of trusted flaggers by public authorities,4,27
2025,2025,furthermore certain obligations are targeted to very large online platforms where the most serious risks often occur and which have the capacity absorb the additional burden,4,7
2026,2026,the proposed legislation will preserve the prohibition of general monitoring obligations of the e commerce directive which in itself is crucial to the required fair balance of fundamental rights in the online world,4,2
2027,2027,the new regulation prohibits general monitoring obligations as they could disproportionately limit users freedom of expression and freedom to receive information and could burden service providers excessively and thus unduly interfere with their freedom to conduct a business,4,1
2028,2028,the prohibition also limits incentives for online surveillance and has positive implications for the protection of personal data and privacy,4,12
2029,2029,all measures in the proposal are fully compliant and aligned with the high standard of personal data protection and protection of privacy of communications and private life set in eu legislation,4,2
2030,2030,budgetary implicationsthe budgetary impact of the proposal will be covered by the allocations foreseen in the mff   under the financial envelopes of the digital europe programme and single market programme as detailed in the legislative financial statement accompanying this proposal for a regulationthese implications require also reprogramming of heading  of the financial perspective,4,17
2031,2031,the legislative financial statement accompanying this proposal for a regulation covers the budgetary impacts for the regulation itself,4,17
2032,2032,other elementsimplementation plans and monitoring evaluation and reporting arrangementsthe commission will establish a comprehensive framework for continuously monitoring the output results and impact of this legislative instrument upon the date of its application,4,17
2033,2033,based on the established monitoring programme an evaluation of the instrument is envisaged within five years from its entry into force,4,18
2034,2034,detailed explanation of the specific provisions of the proposalchapter i sets out general provisions including the subject matter and scope of the regulation article  and the definitions of key terms used in the regulation article,4,27
2035,2035,chapter ii contains provisions on the exemption of liability of providers of intermediary services,4,10
2036,2036,more specifically it includes the conditions under which providers of mere conduit article  caching article  and hosting services article  are exempt from liability for the third party information they transmit and store,4,1
2037,2037,it also provides that the liability exemptions should not be disapplied when providers of intermediary services carry out voluntary own initiative investigations or comply with the law article  and it lays down a prohibition of general monitoring or active fact finding obligations for those providers article,4,1
2038,2038,finally it imposes an obligation on providers of intermediary services in respect of orders from national judicial or administrative authorities to act against illegal content article  and to provide information article,4,1
2039,2039,chapter iii sets out the due diligence obligations for a transparent and safe online environment in five different sections,4,7
2040,2040,section  lays down obligations applicable to all providers of intermediary services in particular the obligation to establish a single point of contact to facilitate direct communication with member states authorities the commission and the board article  the obligation to designate a legal representative in the union for providers not established in any member state but offering their services in the union article  the obligation to set out in their terms and conditions any restrictions that they may impose on the use of their services and to act responsibly in applying and enforcing those restrictions article  and transparency reporting obligations in relation to the removal and the disabling of information considered to be illegal content or contrary to the providers terms and conditions article,4,1
2041,2041,section  lays down obligations additional to those under section  applicable to providers of hosting services,4,1
2042,2042,in particular that section obliges those providers to put in place mechanisms to allow third parties to notify the presence of alleged illegal content article,4,1
2043,2043,furthermore if such a provider decides to remove or disable access to specific information provided by a recipient of the service it imposes the obligation to provide that recipient with a statement of reasons article,4,1
2044,2044,section  lays down obligations applicable to all online platforms additional to those under sections  and,4,7
2045,2045,the section specifies that it does not apply to online platforms that are micro or small enterprises within the meaning of the annex to recommendation ec article,4,7
2046,2046,the section lays down the obligation for online platforms to provide an internal complaint handling system in respect of decisions taken in relation to alleged illegal content or information incompatible with their terms and conditions article,4,7
2047,2047,it also obliges online platforms to engage with certified out of court dispute settlement bodies to resolve any dispute with users of their services article,4,7
2048,2048,it further obliges online platforms to ensure that notices submitted by entities granted the status of trusted flaggers are treated with priority article  and sets out the measures online platforms are to adopt against misuse article,4,7
2049,2049,furthermore this section includes a requirement for online platforms to inform competent enforcement authorities in the event they become aware of any information giving rise to a suspicion of serious criminal offences involving a threat to the life or safety of persons article,4,7
2050,2050,the section also obliges online platforms to receive store make reasonable efforts to assess the reliability of and publish specific information on the traders using their services where those online platforms allow consumers to conclude distance contracts with those traders article,4,7
2051,2051,those online platforms are also obliged to organise their interface in a way that enables traders to respect union consumer and product safety law article a,4,7
2052,2052,online platforms are also obliged to publish reports on their activities relating to the removal and the disabling of information considered to be illegal content or contrary to their terms and conditions article,4,7
2053,2053,the section also includes transparency obligations for online platforms in respect of online advertising article,4,7
2054,2054,section  lays down obligations additional to the obligations laid down in sections  to  for very large online platforms as defined by article  to manage systemic risks,4,7
2055,2055,very large online platforms are obliged to conduct risk assessments on the systemic risks brought about by or relating to the functioning and use of their services article  and to take reasonable and effective measures aimed at mitigating those risks article,4,7
2056,2056,they are also to submit themselves to external and independent audits article,4,0
2057,2057,the section includes also a specific obligation in case very large online platforms use recommender systems article  or display online advertising on their online interface article,4,7
2058,2058,furthermore the section sets out the conditions under which very large online platforms provide access to data to the digital services coordinator of establishment or the commission and vetted researchers article  the obligation to appoint one or more compliance officers to ensure compliance with the obligations laid down in the regulation article  and specific additional transparency reporting obligations article,4,7
2059,2059,section  contains transversal provisions concerning due diligence obligations namely the processes for which the commission will support and promote the development and implementation of harmonised european standards article  the framework for the development of codes of conduct article  and the framework for the development of specific codes of conduct for online advertising article,4,7
2060,2060,there is also a provision on crisis protocols to address extraordinary circumstances affecting public security or public health article,4,17
2061,2061,chapter iv contains the provisions concerning the implementation and enforcement of this regulation,4,1
2062,2062,section  lays down provisions concerning national competent authorities including digital services coordinators which are the primary national authorities designated by the member states for the consistent application of this regulation article,4,23
2063,2063,the digital services coordinators like other designated competent authorities are independent and perform their tasks impartially transparently and in a timely manner article,4,23
2064,2064,member states where the main establishment of the provider is located have jurisdiction to enforce this regulation article,4,1
2065,2065,the digital services coordinators are granted specific powers article,4,23
2066,2066,member states are to lay down rules on penalties applicable to breaches of the obligations by providers of intermediary services under this regulation article,4,1
2067,2067,digital services coordinators can receive complaints against providers of intermediary services for breaches of the obligations laid down in this regulation article,4,23
2068,2068,digital services coordinators are required to publish annual reports on their activities article  and to cooperate with digital services coordinators of other member states article,4,23
2069,2069,digital services coordinators can also participate in joint investigations with regard to matters covered by the regulation article,4,23
2070,2070,section  lays down provisions regarding the european board for digital services an independent advisory group of digital services coordinators article,4,23
2071,2071,it also sets out the structure of that board article  and its tasks article,4,19
2072,2072,section  concerns the supervision investigation enforcement and monitoring of very large online platforms,4,7
2073,2073,it provides for enhanced supervision in the event such platforms infringe the provisions of chapter iii section  article,4,19
2074,2074,it also provides the possibility for the commission to intervene vis  vis very large online platforms in case the infringements persist article,4,7
2075,2075,in these cases the commission can carry out investigations including through requests for information article  interviews article  and on site inspections article  can adopt interim measures article  and make binding commitments by very large online platforms article  as well as monitor their compliance with the regulation article,4,19
2076,2076,in case of non compliance the commission can adopt non compliance decisions article  as well as fines article  and periodic penalty payments article  for breaches of the regulation by very large online platforms as well as for supply of incorrect incomplete or misleading information in the context of the investigation,4,7
2077,2077,the regulation sets also a limitation period for the imposition of penalties article  and for their enforcement article,4,1
2078,2078,finally the regulation sets the procedural guarantees in front of the commission in particular the right to be heard and of access to the file article  and the publication of decisions article,4,19
2079,2079,the section also provides for the cooperation of the commission with national courts article  and for the adoption of implementing acts on practical arrangement on the proceedings article,4,19
2080,2080,section  includes the common provisions on enforcement,4,1
2081,2081,it first lays down rules on an information sharing system supporting communications between digital services coordinators the commission and the board article,4,23
2082,2082,it also includes the right of recipients of intermediary services to mandate a body organisation and association to exercise their rights on their behalf article,4,1
2083,2083,section  relates to the adoption of delegated and implementing acts in accordance with articles  and  of the treaty on the functioning of the european union respectively articles  and,4,2
2084,2084,finally chapter v contains the final provisions of this regulation which concern the deletion of articles  to  of the e commerce directive given that they have been incorporated in the regulation article  amendments to directive xxec article  evaluation of the regulation article  and its entry into force and application article,4,1
2085,2085,codproposal for aregulation of the european parliament and of the councilon a single market for digital services digital services act and amending directive ectext with eea relevancethe european parliament and the council of the european unionhaving regard to the treaty on the functioning of the european union and in particular article  thereofhaving regard to the proposal from the european commissionafter transmission of the draft legislative act to the national parliamentshaving regard to the opinion of the european economic and social committee  having regard to the opinion of the committee of the regions  having regard to the opinion of the european data protection supervisor  acting in accordance with the ordinary legislative procedurewhereasinformation society services and especially intermediary services have become an important part of the unions economy and daily life of union citizens,4,2
2086,2086,twenty years after the adoption of the existing legal framework applicable to such services laid down in directive ec of the european parliament and of the council   new and innovative business models and services such as online social networks and marketplaces have allowed business users and consumers to impart and access information and engage in transactions in novel ways,4,2
2087,2087,a majority of union citizens now uses those services on a daily basis,4,22
2088,2088,however the digital transformation and increased use of those services has also resulted in new risks and challenges both for individual users and for society as a whole,4,23
2089,2089,member states are increasingly introducing or are considering introducing national laws on the matters covered by this regulation imposing in particular diligence requirements for providers of intermediary services,4,8
2090,2090,those diverging national laws negatively affect the internal market which pursuant to article  of the treaty comprises an area without internal frontiers in which the free movement of goods and services and freedom of establishment are ensured taking into account the inherently cross border nature of the internet which is generally used to provide those services,4,0
2091,2091,the conditions for the provision of intermediary services across the internal market should be harmonised so as to provide businesses with access to new markets and opportunities to exploit the benefits of the internal market while allowing consumers and other recipients of the services to have increased choice,4,0
2092,2092,responsible and diligent behaviour by providers of intermediary services is essential for a safe predictable and trusted online environment and for allowing union citizens and other persons to exercise their fundamental rights guaranteed in the charter of fundamental rights of the european union charter in particular the freedom of expression and information and the freedom to conduct a business and the right to non discrimination,4,2
2093,2093,therefore in order to safeguard and improve the functioning of the internal market a targeted set of uniform effective and proportionate mandatory rules should be established at union level,4,0
2094,2094,this regulation provides the conditions for innovative digital services to emerge and to scale up in the internal market,4,0
2095,2095,the approximation of national regulatory measures at union level concerning the requirements for providers of intermediary services is necessary in order to avoid and put an end to fragmentation of the internal market and to ensure legal certainty thus reducing uncertainty for developers and fostering interoperability,4,1
2096,2096,by using requirements that are technology neutral innovation should not be hampered but instead be stimulated,4,0
2097,2097,this regulation should apply to providers of certain information society services as defined in directive eu  of the european parliament and of the council   that is any service normally provided for remuneration at a distance by electronic means and at the individual request of a recipient,4,2
2098,2098,specifically this regulation should apply to providers of intermediary services and in particular intermediary services consisting of services known as mere conduit caching and hosting services given that the exponential growth of the use made of those services mainly for legitimate and socially beneficial purposes of all kinds has also increased their role in the intermediation and spread of unlawful or otherwise harmful information and activities,4,1
2099,2099,in practice certain providers of intermediary services intermediate in relation to services that may or may not be provided by electronic means such as remote information technology services transport accommodation or delivery services,4,23
2100,2100,this regulation should apply only to intermediary services and not affect requirements set out in union or national law relating to products or services intermediated through intermediary services including in situations where the intermediary service constitutes an integral part of another service which is not an intermediary service as specified in the case law of the court of justice of the european union,4,1
2101,2101,in order to ensure the effectiveness of the rules laid down in this regulation and a level playing field within the internal market those rules should apply to providers of intermediary services irrespective of their place of establishment or residence in so far as they provide services in the union as evidenced by a substantial connection to the union,4,1
2102,2102,such a substantial connection to the union should be considered to exist where the service provider has an establishment in the union or in its absence on the basis of the existence of a significant number of users in one or more member states or the targeting of activities towards one or more member states,4,1
2103,2103,the targeting of activities towards one or more member states can be determined on the basis of all relevant circumstances including factors such as the use of a language or a currency generally used in that member state or the possibility of ordering products or services or using a national top level domain,4,1
2104,2104,the targeting of activities towards a member state could also be derived from the availability of an application in the relevant national application store from the provision of local advertising or advertising in the language used in that member state or from the handling of customer relations such as by providing customer service in the language generally used in that member state,4,1
2105,2105,a substantial connection should also be assumed where a service provider directs its activities to one or more member state as set out in article  of regulation eu  of the european parliament and of the council,4,2
2106,2106,on the other hand mere technical accessibility of a website from the union cannot on that ground alone be considered as establishing a substantial connection to the union,4,1
2107,2107,this regulation should complement yet not affect the application of rules resulting from other acts of union law regulating certain aspects of the provision of intermediary services in particular directive ec with the exception of those changes introduced by this regulation directive eu of the european parliament and of the council as amended  and regulation eu  of the european parliament and of the council   proposed terrorist content online regulation,4,2
2108,2108,therefore this regulation leaves those other acts which are to be considered lex specialis in relation to the generally applicable framework set out in this regulation unaffected,4,1
2109,2109,however the rules of this regulation apply in respect of issues that are not or not fully addressed by those other acts as well as issues on which those other acts leave member states the possibility of adopting certain measures at national level,4,1
2110,2110,for reasons of clarity it should also be specified that this regulation is without prejudice to regulation eu  of the european parliament and of the council  and regulation eu  of the european parliament and of the council   directive ec of the european parliament and of the council  and regulation  on temporary derogation from certain provisions of directive ec  as well as union law on consumer protection in particular directive ec of the european parliament and of the council   directive eu of the european parliament and of the council  and directive eec of the european parliament and of the council   as amended by directive eu  of the european parliament and of the council   and on the protection of personal data in particular regulation eu  of the european parliament and of the council,4,2
2111,2111,the protection of individuals with regard to the processing of personal data is solely governed by the rules of union law on that subject in particular regulation eu  and directive ec,4,2
2112,2112,this regulation is also without prejudice to the rules of union law on working conditions,4,1
2113,2113,it should be clarified that this regulation is without prejudice to the rules of union law on copyright and related rights which establish specific rules and procedures that should remain unaffected,4,1
2114,2114,in order to achieve the objective of ensuring a safe predictable and trusted online environment for the purpose of this regulation the concept of illegal content should be defined broadly and also covers information relating to illegal content products services and activities,4,7
2115,2115,in particular that concept should be understood to refer to information irrespective of its form that under the applicable law is either itself illegal such as illegal hate speech or terrorist content and unlawful discriminatory content or that relates to activities that are illegal such as the sharing of images depicting child sexual abuse unlawful non consensual sharing of private images online stalking the sale of non compliant or counterfeit products the non authorised use of copyright protected material or activities involving infringements of consumer protection law,4,1
2116,2116,in this regard it is immaterial whether the illegality of the information or activity results from union law or from national law that is consistent with union law and what the precise nature or subject matter is of the law in question,4,1
2117,2117,considering the particular characteristics of the services concerned and the corresponding need to make the providers thereof subject to certain specific obligations it is necessary to distinguish within the broader category of providers of hosting services as defined in this regulation the subcategory of online platforms,4,7
2118,2118,online platforms such as social networks or online marketplaces should be defined as providers of hosting services that not only store information provided by the recipients of the service at their request but that also disseminate that information to the public again at their request,4,7
2119,2119,however in order to avoid imposing overly broad obligations providers of hosting services should not be considered as online platforms where the dissemination to the public is merely a minor and purely ancillary feature of another service and that feature cannot for objective technical reasons be used without that other principal service and the integration of that feature is not a means to circumvent the applicability of the rules of this regulation applicable to online platforms,4,7
2120,2120,for example the comments section in an online newspaper could constitute such a feature where it is clear that it is ancillary to the main service represented by the publication of news under the editorial responsibility of the publisher,4,14
2121,2121,the concept of dissemination to the public as used in this regulation should entail the making available of information to a potentially unlimited number of persons that is making the information easily accessible to users in general without further action by the recipient of the service providing the information being required irrespective of whether those persons actually access the information in question,4,1
2122,2122,the mere possibility to create groups of users of a given service should not in itself be understood to mean that the information disseminated in that manner is not disseminated to the public,4,14
2123,2123,however the concept should exclude dissemination of information within closed groups consisting of a finite number of pre determined persons,4,1
2124,2124,interpersonal communication services as defined in directive eu  of the european parliament and of the council  such as emails or private messaging services fall outside the scope of this regulation,4,2
2125,2125,information should be considered disseminated to the public within the meaning of this regulation only where that occurs upon the direct request by the recipient of the service that provided the information,4,1
2126,2126,where some of the services provided by a provider are covered by this regulation whilst others are not or where the services provided by a provider are covered by different sections of this regulation the relevant provisions of this regulation should apply only in respect of those services that fall within their scope,4,1
2127,2127,the legal certainty provided by the horizontal framework of conditional exemptions from liability for providers of intermediary services laid down in directive ec has allowed many novel services to emerge and scale up across the internal market,4,10
2128,2128,that framework should therefore be preserved,4,26
2129,2129,however in view of the divergences when transposing and applying the relevant rules at national level and for reasons of clarity and coherence that framework should be incorporated in this regulation,4,8
2130,2130,it is also necessary to clarify certain elements of that framework having regard to case law of the court of justice of the european union,4,2
2131,2131,the relevant rules of chapter ii should only establish when the provider of intermediary services concerned cannot be held liable in relation to illegal content provided by the recipients of the service,4,1
2132,2132,those rules should not be understood to provide a positive basis for establishing when a provider can be held liable which is for the applicable rules of union or national law to determine,4,1
2133,2133,furthermore the exemptions from liability established in this regulation should apply in respect of any type of liability as regards any type of illegal content irrespective of the precise subject matter or nature of those laws,4,1
2134,2134,the exemptions from liability established in this regulation should not apply where instead of confining itself to providing the services neutrally by a merely technical and automatic processing of the information provided by the recipient of the service the provider of intermediary services plays an active role of such a kind as to give it knowledge of or control over that information,4,1
2135,2135,those exemptions should accordingly not be available in respect of liability relating to information provided not by the recipient of the service but by the provider of intermediary service itself including where the information has been developed under the editorial responsibility of that provider,4,1
2136,2136,in view of the different nature of the activities of mere conduit caching and hosting and the different position and abilities of the providers of the services in question it is necessary to distinguish the rules applicable to those activities in so far as under this regulation they are subject to different requirements and conditions and their scope differs as interpreted by the court of justice of the european union,4,1
2137,2137,a provider of intermediary services that deliberately collaborates with a recipient of the services in order to undertake illegal activities does not provide its service neutrally and should therefore not be able to benefit from the exemptions from liability provided for in this regulation,4,1
2138,2138,a provider should be able to benefit from the exemptions from liability for mere conduit and for caching services when it is in no way involved with the information transmitted,4,10
2139,2139,this requires among other things that the provider does not modify the information that it transmits,4,9
2140,2140,however this requirement should not be understood to cover manipulations of a technical nature which take place in the course of the transmission as such manipulations do not alter the integrity of the information transmitted,4,25
2141,2141,in order to benefit from the exemption from liability for hosting services the provider should upon obtaining actual knowledge or awareness of illegal content act expeditiously to remove or to disable access to that content,4,1
2142,2142,the removal or disabling of access should be undertaken in the observance of the principle of freedom of expression,4,1
2143,2143,the provider can obtain such actual knowledge or awareness through in particular its own initiative investigations or notices submitted to it by individuals or entities in accordance with this regulation in so far as those notices are sufficiently precise and adequately substantiated to allow a diligent economic operator to reasonably identify assess and where appropriate act against the allegedly illegal content,4,1
2144,2144,in order to ensure the effective protection of consumers when engaging in intermediated commercial transactions online certain providers of hosting services namely online platforms that allow consumers to conclude distance contracts with traders should not be able to benefit from the exemption from liability for hosting service providers established in this regulation in so far as those online platforms present the relevant information relating to the transactions at issue in such a way that it leads consumers to believe that the information was provided by those online platforms themselves or by recipients of the service acting under their authority or control and that those online platforms thus have knowledge of or control over the information even if that may in reality not be the case,4,7
2145,2145,in that regard is should be determined objectively on the basis of all relevant circumstances whether the presentation could lead to such a belief on the side of an average and reasonably well informed consumer,4,14
2146,2146,the exemptions from liability established in this regulation should not affect the possibility of injunctions of different kinds against providers of intermediary services even where they meet the conditions set out as part of those exemptions,4,1
2147,2147,such injunctions could in particular consist of orders by courts or administrative authorities requiring the termination or prevention of any infringement including the removal of illegal content specified in such orders issued in compliance with union law or the disabling of access to it,4,1
2148,2148,in order to create legal certainty and not to discourage activities aimed at detecting identifying and acting against illegal content that providers of intermediary services may undertake on a voluntary basis it should be clarified that the mere fact that providers undertake such activities does not lead to the unavailability of the exemptions from liability set out in this regulation provided those activities are carried out in good faith and in a diligent manner,4,1
2149,2149,in addition it is appropriate to clarify that the mere fact that those providers take measures in good faith to comply with the requirements of union law including those set out in this regulation as regards the implementation of their terms and conditions should not lead to the unavailability of those exemptions from liability,4,1
2150,2150,therefore any such activities and measures that a given provider may have taken should not be taken into account when determining whether the provider can rely on an exemption from liability in particular as regards whether the provider provides its service neutrally and can therefore fall within the scope of the relevant provision without this rule however implying that the provider can necessarily rely thereon,4,1
2151,2151,whilst the rules in chapter ii of this regulation concentrate on the exemption from liability of providers of intermediary services it is important to recall that despite the generally important role played by those providers the problem of illegal content and activities online should not be dealt with by solely focusing on their liability and responsibilities,4,7
2152,2152,where possible third parties affected by illegal content transmitted or stored online should attempt to resolve conflicts relating to such content without involving the providers of intermediary services in question,4,7
2153,2153,recipients of the service should be held liable where the applicable rules of union and national law determining such liability so provide for the illegal content that they provide and may disseminate through intermediary services,4,1
2154,2154,where appropriate other actors such as group moderators in closed online environments in particular in the case of large groups should also help to avoid the spread of illegal content online in accordance with the applicable law,4,7
2155,2155,furthermore where it is necessary to involve information society services providers including providers of intermediary services any requests or orders for such involvement should as a general rule be directed to the actor that has the technical and operational ability to act against specific items of illegal content so as to prevent and minimise any possible negative effects for the availability and accessibility of information that is not illegal content,4,1
2156,2156,since  new technologies have emerged that improve the availability efficiency speed reliability capacity and security of systems for the transmission and storage of data online leading to an increasingly complex online ecosystem,4,11
2157,2157,in this regard it should be recalled that providers of services establishing and facilitating the underlying logical architecture and proper functioning of the internet including technical auxiliary functions can also benefit from the exemptions from liability set out in this regulation to the extent that their services qualify as mere conduits caching or hosting services,4,1
2158,2158,such services include as the case may be wireless local area networks domain name system dns services toplevel domain name registries certificate authorities that issue digital certificates or content delivery networks that enable or improve the functions of other providers of intermediary services,4,23
2159,2159,likewise services used for communications purposes and the technical means of their delivery have also evolved considerably giving rise to online services such as voice over ip messaging services and web based e mail services where the communication is delivered via an internet access service,4,4
2160,2160,those services too can benefit from the exemptions from liability to the extent that they qualify as mere conduit caching or hosting service,4,10
2161,2161,providers of intermediary services should not be subject to a monitoring obligation with respect to obligations of a general nature,4,1
2162,2162,this does not concern monitoring obligations in a specific case and in particular does not affect orders by national authorities in accordance with national legislation in accordance with the conditions established in this regulation,4,1
2163,2163,nothing in this regulation should be construed as an imposition of a general monitoring obligation or active fact finding obligation or as a general obligation for providers to take proactive measures to relation to illegal content,4,1
2164,2164,depending on the legal system of each member state and the field of law at issue national judicial or administrative authorities may order providers of intermediary services to act against certain specific items of illegal content or to provide certain specific items of information,4,1
2165,2165,the national laws on the basis of which such orders are issued differ considerably and the orders are increasingly addressed in cross border situations,4,1
2166,2166,in order to ensure that those orders can be complied with in an effective and efficient manner so that the public authorities concerned can carry out their tasks and the providers are not subject to any disproportionate burdens without unduly affecting the rights and legitimate interests of any third parties it is necessary to set certain conditions that those orders should meet and certain complementary requirements relating to the processing of those orders,4,1
2167,2167,orders to act against illegal content or to provide information should be issued in compliance with union law in particular regulation eu  and the prohibition of general obligations to monitor information or to actively seek facts or circumstances indicating illegal activity laid down in this regulation,4,1
2168,2168,the conditions and requirements laid down in this regulation which apply to orders to act against illegal content are without prejudice to other union acts providing for similar systems for acting against specific types of illegal content such as regulation eu,4,1
2169,2169,proposed regulation addressing the dissemination of terrorist content online or regulation eu  that confers specific powers to order the provision of information on member state consumer law enforcement authorities whilst the conditions and requirements that apply to orders to provide information are without prejudice to other union acts providing for similar relevant rules for specific sectors,4,7
2170,2170,those conditions and requirements should be without prejudice to retention and preservation rules under applicable national law in conformity with union law and confidentiality requests by law enforcement authorities related to the non disclosure of information,4,1
2171,2171,the territorial scope of such orders to act against illegal content should be clearly set out on the basis of the applicable union or national law enabling the issuance of the order and should not exceed what is strictly necessary to achieve its objectives,4,1
2172,2172,in that regard the national judicial or administrative authority issuing the order should balance the objective that the order seeks to achieve in accordance with the legal basis enabling its issuance with the rights and legitimate interests of all third parties that may be affected by the order in particular their fundamental rights under the charter,4,22
2173,2173,in addition where the order referring to the specific information may have effects beyond the territory of the member state of the authority concerned the authority should assess whether the information at issue is likely to constitute illegal content in other member states concerned and where relevant take account of the relevant rules of union law or international law and the interests of international comity,4,1
2174,2174,the orders to provide information regulated by this regulation concern the production of specific information about individual recipients of the intermediary service concerned who are identified in those orders for the purposes of determining compliance by the recipients of the services with applicable union or national rules,4,1
2175,2175,therefore orders about information on a group of recipients of the service who are not specifically identified including orders to provide aggregate information required for statistical purposes or evidence based policy making should remain unaffected by the rules of this regulation on the provision of information,4,1
2176,2176,orders to act against illegal content and to provide information are subject to the rules safeguarding the competence of the member state where the service provider addressed is established and laying down possible derogations from that competence in certain cases set out in article  of directive ec only if the conditions of that article are met,4,1
2177,2177,given that the orders in question relate to specific items of illegal content and information respectively where they are addressed to providers of intermediary services established in another member state they do not in principle restrict those providers freedom to provide their services across borders,4,1
2178,2178,therefore the rules set out in article  of directive ec including those regarding the need to justify measures derogating from the competence of the member state where the service provider is established on certain specified grounds and regarding the notification of such measures do not apply in respect of those orders,4,1
2179,2179,in order to achieve the objectives of this regulation and in particular to improve the functioning of the internal market and ensure a safe and transparent online environment it is necessary to establish a clear and balanced set of harmonised due diligence obligations for providers of intermediary services,4,7
2180,2180,those obligations should aim in particular to guarantee different public policy objectives such as the safety and trust of the recipients of the service including minors and vulnerable users protect the relevant fundamental rights enshrined in the charter to ensure meaningful accountability of those providers and to empower recipients and other affected parties whilst facilitating the necessary oversight by competent authorities,4,27
2181,2181,in that regard it is important that the due diligence obligations are adapted to the type and nature of the intermediary service concerned,4,10
2182,2182,this regulation therefore sets out basic obligations applicable to all providers of intermediary services as well as additional obligations for providers of hosting services and more specifically online platforms and very large online platforms,4,7
2183,2183,to the extent that providers of intermediary services may fall within those different categories in view of the nature of their services and their size they should comply with all of the corresponding obligations of this regulation,4,1
2184,2184,those harmonised due diligence obligations which should be reasonable and non arbitrary are needed to achieve the identified public policy concerns such as safeguarding the legitimate interests of the recipients of the service addressing illegal practices and protecting fundamental rights online,4,27
2185,2185,in order to facilitate smooth and efficient communications relating to matters covered by this regulation providers of intermediary services should be required to establish a single point of contact and to publish relevant information relating to their point of contact including the languages to be used in such communications,4,1
2186,2186,the point of contact can also be used by trusted flaggers and by professional entities which are under a specific relationship with the provider of intermediary services,4,12
2187,2187,in contrast to the legal representative the point of contact should serve operational purposes and should not necessarily have to have a physical location,4,12
2188,2188,providers of intermediary services that are established in a third country that offer services in the union should designate a sufficiently mandated legal representative in the union and provide information relating to their legal representatives so as to allow for the effective oversight and where necessary enforcement of this regulation in relation to those providers,4,1
2189,2189,it should be possible for the legal representative to also function as point of contact provided the relevant requirements of this regulation are complied with,4,27
2190,2190,whilst the freedom of contract of providers of intermediary services should in principle be respected it is appropriate to set certain rules on the content application and enforcement of the terms and conditions of those providers in the interests of transparency the protection of recipients of the service and the avoidance of unfair or arbitrary outcomes,4,1
2191,2191,to ensure an adequate level of transparency and accountability providers of intermediary services should annually report in accordance with the harmonised requirements contained in this regulation on the content moderation they engage in including the measures taken as a result of the application and enforcement of their terms and conditions,4,7
2192,2192,however so as to avoid disproportionate burdens those transparency reporting obligations should not apply to providers that are microor small enterprises as defined in commission recommendation ec,4,1
2193,2193,providers of hosting services play a particularly important role in tackling illegal content online as they store information provided by and at the request of the recipients of the service and typically give other recipients access thereto sometimes on a large scale,4,7
2194,2194,it is important that all providers of hosting services regardless of their size put in place user friendly notice and action mechanisms that facilitate the notification of specific items of information that the notifying party considers to be illegal content to the provider of hosting services concerned notice pursuant to which that provider can decide whether or not it agrees with that assessment and wishes to remove or disable access to that content action,4,7
2195,2195,provided the requirements on notices are met it should be possible for individuals or entities to notify multiple specific items of allegedly illegal content through a single notice,4,12
2196,2196,the obligation to put in place notice and action mechanisms should apply for instance to file storage and sharing services web hosting services advertising servers and paste bins in as far as they qualify as providers of hosting services covered by this regulation,4,1
2197,2197,the rules on such notice and action mechanisms should be harmonised at union level so as to provide for the timely diligent and objective processing of notices on the basis of rules that are uniform transparent and clear and that provide for robust safeguards to protect the right and legitimate interests of all affected parties in particular their fundamental rights guaranteed by the charter irrespective of the member state in which those parties are established or reside and of the field of law at issue,4,1
2198,2198,the fundamental rights include as the case may be the right to freedom of expression and information the right to respect for private and family life the right to protection of personal data the right to non discrimination and the right to an effective remedy of the recipients of the service the freedom to conduct a business including the freedom of contract of service providers as well as the right to human dignity the rights of the child the right to protection of property including intellectual property and the right to non discrimination of parties affected by illegal content,4,12
2199,2199,where a hosting service provider decides to remove or disable information provided by a recipient of the service for instance following receipt of a notice or acting on its own initiative including through the use of automated means that provider should inform the recipient of its decision the reasons for its decision and the available redress possibilities to contest the decision in view of the negative consequences that such decisions may have for the recipient including as regards the exercise of its fundamental right to freedom of expression,4,1
2200,2200,that obligation should apply irrespective of the reasons for the decision in particular whether the action has been taken because the information notified is considered to be illegal content or incompatible with the applicable terms and conditions,4,21
2201,2201,available recourses to challenge the decision of the hosting service provider should always include judicial redress,4,10
2202,2202,to avoid disproportionate burdens the additional obligations imposed on online platforms under this regulation should not apply to micro or small enterprises as defined in recommendation ec of the commission  unless their reach and impact is such that they meet the criteria to qualify as very large online platforms under this regulation,4,7
2203,2203,the consolidation rules laid down in that recommendation help ensure that any circumvention of those additional obligations is prevented,4,27
2204,2204,the exemption of microand small enterprises from those additional obligations should not be understood as affecting their ability to set up on a voluntary basis a system that complies with one or more of those obligations,4,1
2205,2205,recipients of the service should be able to easily and effectively contest certain decisions of online platforms that negatively affect them,4,7
2206,2206,therefore online platforms should be required to provide for internal complaint handling systems which meet certain conditions aimed at ensuring that the systems are easily accessible and lead to swift and fair outcomes,4,7
2207,2207,in addition provision should be made for the possibility of out of court dispute settlement of disputes including those that could not be resolved in satisfactory manner through the internal complaint handling systems by certified bodies that have the requisite independence means and expertise to carry out their activities in a fair swift and cost effective manner,4,10
2208,2208,the possibilities to contest decisions of online platforms thus created should complement yet leave unaffected in all respects the possibility to seek judicial redress in accordance with the laws of the member state concerned,4,7
2209,2209,for contractual consumer to business disputes over the purchase of goods or services directive eu of the european parliament and of the council  ensures that union consumers and businesses in the union have access to quality certified alternative dispute resolution entities,4,2
2210,2210,in this regard it should be clarified that the rules of this regulation on out of court dispute settlement are without prejudice to that directive including the right of consumers under that directive to withdraw from the procedure at any stage if they are dissatisfied with the performance or the operation of the procedure,4,1
2211,2211,action against illegal content can be taken more quickly and reliably where online platforms take the necessary measures to ensure that notices submitted by trusted flaggers through the notice and action mechanisms required by this regulation are treated with priority without prejudice to the requirement to process and decide upon all notices submitted under those mechanisms in a timely diligent and objective manner,4,7
2212,2212,such trusted flagger status should only be awarded to entities and not individuals that have demonstrated among other things that they have particular expertise and competence in tackling illegal content that they represent collective interests and that they work in a diligent and objective manner,4,12
2213,2213,such entities can be public in nature such as for terrorist content internet referral units of national law enforcement authorities or of the european union agency for law enforcement cooperation europol or they can be non governmental organisations and semi public bodies such as the organisations part of the inhope network of hotlines for reporting child sexual abuse material and organisations committed to notifying illegal racist and xenophobic expressions online,4,7
2214,2214,for intellectual property rights organisations of industry and of right holders could be awarded trusted flagger status where they have demonstrated that they meet the applicable conditions,4,0
2215,2215,the rules of this regulation on trusted flaggers should not be understood to prevent online platforms from giving similar treatment to notices submitted by entities or individuals that have not been awarded trusted flagger status under this regulation from otherwise cooperating with other entities in accordance with the applicable law including this regulation and regulation eu  of the european parliament and of the council,4,7
2216,2216,the misuse of services of online platforms by frequently providing manifestly illegal content or by frequently submitting manifestly unfounded notices or complaints under the mechanisms and systems respectively established under this regulation undermines trust and harms the rights and legitimate interests of the parties concerned,4,7
2217,2217,therefore there is a need to put in place appropriate and proportionate safeguards against such misuse,4,12
2218,2218,information should be considered to be manifestly illegal content and notices or complaints should be considered manifestly unfounded where it is evident to a layperson without any substantive analysis that the content is illegal respectively that the notices or complaints are unfounded,4,1
2219,2219,under certain conditions online platforms should temporarily suspend their relevant activities in respect of the person engaged in abusive behaviour,4,7
2220,2220,this is without prejudice to the freedom by online platforms to determine their terms and conditions and establish stricter measures in the case of manifestly illegal content related to serious crimes,4,7
2221,2221,for reasons of transparency this possibility should be set out clearly and in sufficiently detail in the terms and conditions of the online platforms,4,7
2222,2222,redress should always be open to the decisions taken in this regard by online platforms and they should be subject to oversight by the competent digital services coordinator,4,23
2223,2223,the rules of this regulation on misuse should not prevent online platforms from taking other measures to address the provision of illegal content by recipients of their service or other misuse of their services in accordance with the applicable union and national law,4,1
2224,2224,those rules are without prejudice to any possibility to hold the persons engaged in misuse liable including for damages provided for in union or national law,4,10
2225,2225,an online platform may in some instances become aware such as through a notice by a notifying party or through its own voluntary measures of information relating to certain activity of a recipient of the service such as the provision of certain types of illegal content that reasonably justify having regard to all relevant circumstances of which the online platform is aware the suspicion that the recipient may have committed may be committing or is likely to commit a serious criminal offence involving a threat to the life or safety of person such as offences specified in directive eu of the european parliament and of the council,4,7
2226,2226,in such instances the online platform should inform without delay the competent law enforcement authorities of such suspicion providing all relevant information available to it including where relevant the content in question and an explanation of its suspicion,4,7
2227,2227,this regulation does not provide the legal basis for profiling of recipients of the services with a view to the possible identification of criminal offences by online platforms,4,12
2228,2228,online platforms should also respect other applicable rules of union or national law for the protection of the rights and freedoms of individuals when informing law enforcement authorities,4,7
2229,2229,in order to contribute to a safe trustworthy and transparent online environment for consumers as well as for other interested parties such as competing traders and holders of intellectual property rights and to deter traders from selling products or services in violation of the applicable rules online platforms allowing consumers to conclude distance contracts with traders should ensure that such traders are traceable,4,7
2230,2230,the trader should therefore be required to provide certain essential information to the online platform including for purposes of promoting messages on or offering products,4,7
2231,2231,that requirement should also be applicable to traders that promote messages on products or services on behalf of brands based on underlying agreements,4,0
2232,2232,those online platforms should store all information in a secure manner for a reasonable period of time that does not exceed what is necessary so that it can be accessed in accordance with the applicable law including on the protection of personal data by public authorities and private parties with a legitimate interest including through the orders to provide information referred to in this regulation,4,7
2233,2233,to ensure an efficient and adequate application of that obligation without imposing any disproportionate burdens the online platforms covered should make reasonable efforts to verify the reliability of the information provided by the traders concerned in particular by using freely available official online databases and online interfaces such as national trade registers and the vat information exchange system   or by requesting the traders concerned to provide trustworthy supporting documents such as copies of identity documents certified bank statements company certificates and trade register certificates,4,7
2234,2234,they may also use other sources available for use at a distance which offer a similar degree of reliability for the purpose of complying with this obligation,4,25
2235,2235,however the online platforms covered should not be required to engage in excessive or costly online fact finding exercises or to carry out verifications on the spot,4,7
2236,2236,nor should such online platforms which have made the reasonable efforts required by this regulation be understood as guaranteeing the reliability of the information towards consumer or other interested parties,4,7
2237,2237,such online platforms should also design and organise their online interface in a way that enables traders to comply with their obligations under union law in particular the requirements set out in articles  and  of directive eu of the european parliament and of the council   article  of directive ec of the european parliament and of the council  and article  of directive ec of the european parliament and of the council,4,7
2238,2238,in view of the particular responsibilities and obligations of online platforms they should be made subject to transparency reporting obligations which apply in addition to the transparency reporting obligations applicable to all providers of intermediary services under this regulation,4,7
2239,2239,for the purposes of determining whether online platforms may be very large online platforms that are subject to certain additional obligations under this regulation the transparency reporting obligations for online platforms should include certain obligations relating to the publication and communication of information on the average monthly active recipients of the service in the union,4,7
2240,2240,online advertisement plays an important role in the online environment including in relation to the provision of the services of online platforms,4,7
2241,2241,however online advertisement can contribute to significant risks ranging from advertisement that is itself illegal content to contributing to financial incentives for the publication or amplification of illegal or otherwise harmful content and activities online or the discriminatory display of advertising with an impact on the equal treatment and opportunities of citizens,4,7
2242,2242,in addition to the requirements resulting from article  of directive ec online platforms should therefore be required to ensure that the recipients of the service have certain individualised information necessary for them to understand when and on whose behalf the advertisement is displayed,4,7
2243,2243,in addition recipients of the service should have information on the main parameters used for determining that specific advertising is to be displayed to them providing meaningful explanations of the logic used to that end including when this is based on profiling,4,0
2244,2244,the requirements of this regulation on the provision of information relating to advertisement is without prejudice to the application of the relevant provisions of regulation eu  in particular those regarding the right to object automated individual decision making including profiling and specifically the need to obtain consent of the data subject prior to the processing of personal data for targeted advertising,4,1
2245,2245,similarly it is without prejudice to the provisions laid down in directive ec in particular those regarding the storage of information in terminal equipment and the access to information stored therein,4,1
2246,2246,given the importance of very large online platforms due to their reach in particular as expressed in number of recipients of the service in facilitating public debate economic transactions and the dissemination of information opinions and ideas and in influencing how recipients obtain and communicate information online it is necessary to impose specific obligations on those platforms in addition to the obligations applicable to all online platforms,4,7
2247,2247,those additional obligations on very large online platforms are necessary to address those public policy concerns there being no alternative and less restrictive measures that would effectively achieve the same result,4,7
2248,2248,very large online platforms may cause societal risks different in scope and impact from those caused by smaller platforms,4,7
2249,2249,once the number of recipients of a platform reaches a significant share of the union population the systemic risks the platform poses have a disproportionately negative impact in the union,4,6
2250,2250,such significant reach should be considered to exist where the number of recipients exceeds an operational threshold set at  million that is a number equivalent to  of the union population,4,22
2251,2251,the operational threshold should be kept up to date through amendments enacted by delegated acts where necessary,4,22
2252,2252,such very large online platforms should therefore bear the highest standard of due diligence obligations proportionate to their societal impact and means,4,7
2253,2253,in view of the network effects characterising the platform economy the user base of an online platform may quickly expand and reach the dimension of a very large online platform with the related impact on the internal market,4,7
2254,2254,this may be the case in the event of exponential growth experienced in short periods of time or by a large global presence and turnover allowing the online platform to fully exploit network effects and economies of scale and of scope,4,7
2255,2255,a high annual turnover or market capitalisation can in particular be an indication of fast scalability in terms of user reach,4,0
2256,2256,in those cases the digital services coordinator should be able to request more frequent reporting from the platform on the user base to be able to timely identify the moment at which that platform should be designated as a very large online platform for the purposes of this regulation,4,7
2257,2257,very large online platforms are used in a way that strongly influences safety online the shaping of public opinion and discourse as well as on online trade,4,7
2258,2258,the way they design their services is generally optimised to benefit their often advertising driven business models and can cause societal concerns,4,0
2259,2259,in the absence of effective regulation and enforcement they can set the rules of the game without effectively identifying and mitigating the risks and the societal and economic harm they can cause,4,8
2260,2260,under this regulation very large online platforms should therefore assess the systemic risks stemming from the functioning and use of their service as well as by potential misuses by the recipients of the service and take appropriate mitigating measures,4,7
2261,2261,three categories of systemic risks should be assessed in depth,4,3
2262,2262,a first category concerns the risks associated with the misuse of their service through the dissemination of illegal content such as the dissemination of child sexual abuse material or illegal hate speech and the conduct of illegal activities such as the sale of products or services prohibited by union or national law including counterfeit products,4,12
2263,2263,for example and without prejudice to the personal responsibility of the recipient of the service of very large online platforms for possible illegality of his or her activity under the applicable law such dissemination or activities may constitute a significant systematic risk where access to such content may be amplified through accounts with a particularly wide reach,4,7
2264,2264,a second category concerns the impact of the service on the exercise of fundamental rights as protected by the charter of fundamental rights including the freedom of expression and information the right to private life the right to non discrimination and the rights of the child,4,12
2265,2265,such risks may arise for example in relation to the design of the algorithmic systems used by the very large online platform or the misuse of their service through the submission of abusive notices or other methods for silencing speech or hampering competition,4,7
2266,2266,a third category of risks concerns the intentional and oftentimes coordinated manipulation of the platforms service with a foreseeable impact on health civic discourse electoral processes public security and protection of minors having regard to the need to safeguard public order protect privacy and fight fraudulent and deceptive commercial practices,4,7
2267,2267,such risks may arise for example through the creation of fake accounts the use of bots and other automated or partially automated behaviours which may lead to the rapid and widespread dissemination of information that is illegal content or incompatible with an online platforms terms and conditions,4,7
2268,2268,very large online platforms should deploy the necessary means to diligently mitigate the systemic risks identified in the risk assessment,4,7
2269,2269,very large online platforms should under such mitigating measures consider for example enhancing or otherwise adapting the design and functioning of their content moderation algorithmic recommender systems and online interfaces so that they discourage and limit the dissemination of illegal content adapting their decision making processes or adapting their terms and conditions,4,7
2270,2270,they may also include corrective measures such as discontinuing advertising revenue for specific content or other actions such as improving the visibility of authoritative information sources,4,1
2271,2271,very large online platforms may reinforce their internal processes or supervision of any of their activities in particular as regards the detection of systemic risks,4,7
2272,2272,they may also initiate or increase cooperation with trusted flaggers organise training sessions and exchanges with trusted flagger organisations and cooperate with other service providers including by initiating or joining existing codes of conduct or other self regulatory measures,4,1
2273,2273,any measures adopted should respect the due diligence requirements of this regulation and be effective and appropriate for mitigating the specific risks identified in the interest of safeguarding public order protecting privacy and fighting fraudulent and deceptive commercial practices and should be proportionate in light of the very large online platforms economic capacity and the need to avoid unnecessary restrictions on the use of their service taking due account of potential negative effects on the fundamental rights of the recipients of the service,4,7
2274,2274,very large online platforms should where appropriate conduct their risk assessments and design their risk mitigation measures with the involvement of representatives of the recipients of the service representatives of groups potentially impacted by their services independent experts and civil society organisations,4,7
2275,2275,given the need to ensure verification by independent experts very large online platforms should be accountable through independent auditing for their compliance with the obligations laid down by this regulation and where relevant any complementary commitments undertaking pursuant to codes of conduct and crises protocols,4,7
2276,2276,they should give the auditor access to all relevant data necessary to perform the audit properly,4,9
2277,2277,auditors should also be able to make use of other sources of objective information including studies by vetted researchers,4,19
2278,2278,auditors should guarantee the confidentiality security and integrity of the information such as trade secrets that they obtain when performing their tasks and have the necessary expertise in the area of risk management and technical competence to audit algorithms,4,0
2279,2279,auditors should be independent so as to be able to perform their tasks in an adequate and trustworthy manner,4,16
2280,2280,if their independence is not beyond doubt they should resign or abstain from the audit engagement,4,0
2281,2281,the audit report should be substantiated so as to give a meaningful account of the activities undertaken and the conclusions reached,4,18
2282,2282,it should help inform and where appropriate suggest improvements to the measures taken by the very large online platform to comply with their obligations under this regulation,4,7
2283,2283,the report should be transmitted to the digital services coordinator of establishment and the board without delay together with the risk assessment and the mitigation measures as well as the platforms plans for addressing the audits recommendations,4,23
2284,2284,the report should include an audit opinion based on the conclusions drawn from the audit evidence obtained,4,0
2285,2285,a positive opinion should be given where all evidence shows that the very large online platform complies with the obligations laid down by this regulation or where applicable any commitments it has undertaken pursuant to a code of conduct or crisis protocol in particular by identifying evaluating and mitigating the systemic risks posed by its system and services,4,7
2286,2286,a positive opinion should be accompanied by comments where the auditor wishes to include remarks that do not have a substantial effect on the outcome of the audit,4,14
2287,2287,a negative opinion should be given where the auditor considers that the very large online platform does not comply with this regulation or the commitments undertaken,4,7
2288,2288,a core part of a very large online platforms business is the manner in which information is prioritised and presented on its online interface to facilitate and optimise access to information for the recipients of the service,4,7
2289,2289,this is done for example by algorithmically suggesting ranking and prioritising information distinguishing through text or other visual representations or otherwise curating information provided by recipients,4,13
2290,2290,such recommender systems can have a significant impact on the ability of recipients to retrieve and interact with information online,4,7
2291,2291,they also play an important role in the amplification of certain messages the viral dissemination of information and the stimulation of online behaviour,4,4
2292,2292,consequently very large online platforms should ensure that recipients are appropriately informed and can influence the information presented to them,4,7
2293,2293,they should clearly present the main parameters for such recommender systems in an easily comprehensible manner to ensure that the recipients understand how information is prioritised for them,4,14
2294,2294,they should also ensure that the recipients enjoy alternative options for the main parameters including options that are not based on profiling of the recipient,4,21
2295,2295,advertising systems used by very large online platforms pose particular risks and require further public and regulatory supervision on account of their scale and ability to target and reach recipients of the service based on their behaviour within and outside that platforms online interface,4,7
2296,2296,very large online platforms should ensure public access to repositories of advertisements displayed on their online interfaces to facilitate supervision and research into emerging risks brought about by the distribution of advertising online for example in relation to illegal advertisements or manipulative techniques and disinformation with a real and foreseeable negative impact on public health public security civil discourse political participation and equality,4,7
2297,2297,repositories should include the content of advertisements and related data on the advertiser and the delivery of the advertisement in particular where targeted advertising is concerned,4,13
2298,2298,in order to appropriately supervise the compliance of very large online platforms with the obligations laid down by this regulation the digital services coordinator of establishment or the commission may require access to or reporting of specific data,4,7
2299,2299,such a requirement may include for example the data necessary to assess the risks and possible harms brought about by the platforms systems data on the accuracy functioning and testing of algorithmic systems for content moderation recommender systems or advertising systems or data on processes and outputs of content moderation or of internal complaint handling systems within the meaning of this regulation,4,7
2300,2300,investigations by researchers on the evolution and severity of online systemic risks are particularly important for bridging information asymmetries and establishing a resilient system of risk mitigation informing online platforms digital services coordinators other competent authorities the commission and the public,4,7
2301,2301,this regulation therefore provides a framework for compelling access to data from very large online platforms to vetted researchers,4,7
2302,2302,all requirements for access to data under that framework should be proportionate and appropriately protect the rights and legitimate interests including trade secrets and other confidential information of the platform and any other parties concerned including the recipients of the service,4,11
2303,2303,given the complexity of the functioning of the systems deployed and the systemic risks they present to society very large online platforms should appoint compliance officers which should have the necessary qualifications to operationalise measures and monitor the compliance with this regulation within the platforms organisation,4,7
2304,2304,very large online platforms should ensure that the compliance officer is involved properly and in a timely manner in all issues which relate to this regulation,4,7
2305,2305,in view of the additional risks relating to their activities and their additional obligations under this regulation the other transparency requirements set out in this regulation should be complemented by additional transparency requirements applicable specifically to very large online platforms notably to report on the risk assessments performed and subsequent measures adopted as provided by this regulation,4,7
2306,2306,to facilitate the effective and consistent application of the obligations in this regulation that may require implementation through technological means it is important to promote voluntary industry standards covering certain technical procedures where the industry can help develop standardised means to comply with this regulation such as allowing the submission of notices including through application programming interfaces or about the interoperability of advertisement repositories,4,27
2307,2307,such standards could in particular be useful for relatively small providers of intermediary services,4,27
2308,2308,the standards could distinguish between different types of illegal content or different types of intermediary services as appropriate,4,1
2309,2309,the commission and the board should encourage the drawing up of codes of conduct to contribute to the application of this regulation,4,19
2310,2310,while the implementation of codes of conduct should be measurable and subject to public oversight this should not impair the voluntary nature of such codes and the freedom of interested parties to decide whether to participate,4,25
2311,2311,in certain circumstances it is important that very large online platforms cooperate in the drawing up and adhere to specific codes of conduct,4,7
2312,2312,nothing in this regulation prevents other service providers from adhering to the same standards of due diligence adopting best practices and benefitting from the guidance provided by the commission and the board by participating in the same codes of conduct,4,1
2313,2313,it is appropriate that this regulation identify certain areas of consideration for such codes of conduct,4,1
2314,2314,in particular risk mitigation measures concerning specific types of illegal content should be explored via selfand co regulatory agreements,4,1
2315,2315,another area for consideration is the possible negative impacts of systemic risks on society and democracy such as disinformation or manipulative and abusive activities,4,3
2316,2316,this includes coordinated operations aimed at amplifying information including disinformation such as the use of bots or fake accounts for the creation of fake or misleading information sometimes with a purpose of obtaining economic gain which are particularly harmful for vulnerable recipients of the service such as children,4,7
2317,2317,in relation to such areas adherence to and compliance with a given code of conduct by a very large online platform may be considered as an appropriate risk mitigating measure,4,7
2318,2318,the refusal without proper explanations by an online platform of the commissions invitation to participate in the application of such a code of conduct could be taken into account where relevant when determining whether the online platform has infringed the obligations laid down by this regulation,4,7
2319,2319,the rules on codes of conduct under this regulation could serve as a basis for already established self regulatory efforts at union level including the product safety pledge the memorandum of understanding against counterfeit goods the code of conduct against illegal hate speech as well as the code of practice on disinformation,4,1
2320,2320,in particular for the latter the commission will issue guidance for strengthening the code of practice on disinformation as announced in the european democracy action plan,4,19
2321,2321,the provision of online advertising generally involves several actors including intermediary services that connect publishers of advertising with advertisers,4,7
2322,2322,codes of conducts should support and complement the transparency obligations relating to advertisement for online platforms and very large online platforms set out in this regulation in order to provide for flexible and effective mechanisms to facilitate and enhance the compliance with those obligations notably as concerns the modalities of the transmission of the relevant information,4,7
2323,2323,the involvement of a wide range of stakeholders should ensure that those codes of conduct are widely supported technically sound effective and offer the highest levels of user friendliness to ensure that the transparency obligations achieve their objectives,4,27
2324,2324,in case of extraordinary circumstances affecting public security or public health the commission may initiate the drawing up of crisis protocols to coordinate a rapid collective and cross border response in the online environment,4,12
2325,2325,extraordinary circumstances may entail any unforeseeable event such as earthquakes hurricanes pandemics and other serious cross border threats to public health war and acts of terrorism where for example online platforms may be misused for the rapid spread of illegal content or disinformation or where the need arises for rapid dissemination of reliable information,4,7
2326,2326,in light of the important role of very large online platforms in disseminating information in our societies and across borders such platforms should be encouraged in drawing up and applying specific crisis protocols,4,7
2327,2327,such crisis protocols should be activated only for a limited period of time and the measures adopted should also be limited to what is strictly necessary to address the extraordinary circumstance,4,22
2328,2328,those measures should be consistent with this regulation and should not amount to a general obligation for the participating very large online platforms to monitor the information which they transmit or store nor actively to seek facts or circumstances indicating illegal content,4,7
2329,2329,the task of ensuring adequate oversight and enforcement of the obligations laid down in this regulation should in principle be attributed to the member states,4,1
2330,2330,to this end they should appoint at least one authority with the task to apply and enforce this regulation,4,8
2331,2331,member states should however be able to entrust more than one competent authority with specific supervisory or enforcement tasks and competences concerning the application of this regulation for example for specific sectors such as electronic communications regulators media regulators or consumer protection authorities reflecting their domestic constitutional organisational and administrative structure,4,1
2332,2332,given the cross border nature of the services at stake and the horizontal range of obligations introduced by this regulation the authority appointed with the task of supervising the application and where necessary enforcing this regulation should be identified as a digital services coordinator in each member state,4,23
2333,2333,where more than one competent authority is appointed to apply and enforce this regulation only one authority in that member state should be identified as a digital services coordinator,4,23
2334,2334,the digital services coordinator should act as the single contact point with regard to all matters related to the application of this regulation for the commission the board the digital services coordinators of other member states as well as for other competent authorities of the member state in question,4,23
2335,2335,in particular where several competent authorities are entrusted with tasks under this regulation in a given member state the digital services coordinator should coordinate and cooperate with those authorities in accordance with the national law setting their respective tasks and should ensure effective involvement of all relevant authorities in the supervision and enforcement at union level,4,23
2336,2336,the digital services coordinator as well as other competent authorities designated under this regulation play a crucial role in ensuring the effectiveness of the rights and obligations laid down in this regulation and the achievement of its objectives,4,23
2337,2337,accordingly it is necessary to ensure that those authorities act in complete independence from private and public bodies without the obligation or possibility to seek or receive instructions including from the government and without prejudice to the specific duties to cooperate with other competent authorities the digital services coordinators the board and the commission,4,23
2338,2338,on the other hand the independence of these authorities should not mean that they cannot be subject in accordance with national constitutions and without endangering the achievement of the objectives of this regulation to national control or monitoring mechanisms regarding their financial expenditure or to judicial review or that they should not have the possibility to consult other national authorities including law enforcement authorities or crisis management authorities where appropriate,4,26
2339,2339,member states can designate an existing national authority with the function of the digital services coordinator or with specific tasks to apply and enforce this regulation provided that any such appointed authority complies with the requirements laid down in this regulation such as in relation to its independence,4,23
2340,2340,moreover member states are in principle not precluded from merging functions within an existing authority in accordance with union law,4,1
2341,2341,the measures to that effect may include inter alia the preclusion to dismiss the president or a board member of a collegiate body of an existing authority before the expiry of their terms of office on the sole ground that an institutional reform has taken place involving the merger of different functions within one authority in the absence of any rules guaranteeing that such dismissals do not jeopardise the independence and impartiality of such members,4,22
2342,2342,in the absence of a general requirement for providers of intermediary services to ensure a physical presence within the territory of one of the member states there is a need to ensure clarity under which member states jurisdiction those providers fall for the purposes of enforcing the rules laid down in chapters iii and iv by the national competent authorities,4,1
2343,2343,a provider should be under the jurisdiction of the member state where its main establishment is located that is where the provider has its head office or registered office within which the principal financial functions and operational control are exercised,4,1
2344,2344,in respect of providers that do not have an establishment in the union but that offer services in the union and therefore fall within the scope of this regulation the member state where those providers appointed their legal representative should have jurisdiction considering the function of legal representatives under this regulation,4,1
2345,2345,in the interest of the effective application of this regulation all member states should however have jurisdiction in respect of providers that failed to designate a legal representative provided that the principle of ne bis in idem is respected,4,1
2346,2346,to that aim each member state that exercises jurisdiction in respect of such providers should without undue delay inform all other member states of the measures they have taken in the exercise of that jurisdiction,4,1
2347,2347,member states should provide the digital services coordinator and any other competent authority designated under this regulation with sufficient powers and means to ensure effective investigation and enforcement,4,23
2348,2348,digital services coordinators should in particular be able to search for and obtain information which is located in its territory including in the context of joint investigations with due regard to the fact that oversight and enforcement measures concerning a provider under the jurisdiction of another member state should be adopted by the digital services coordinator of that other member state where relevant in accordance with the procedures relating to cross border cooperation,4,23
2349,2349,member states should set out in their national law in accordance with union law and in particular this regulation and the charter the detailed conditions and limits for the exercise of the investigatory and enforcement powers of their digital services coordinators and other competent authorities where relevant under this regulation,4,23
2350,2350,in the course of the exercise of those powers the competent authorities should comply with the applicable national rules regarding procedures and matters such as the need for a prior judicial authorisation to enter certain premises and legal professional privilege,4,19
2351,2351,those provisions should in particular ensure respect for the fundamental rights to an effective remedy and to a fair trial including the rights of defence and the right to respect for private life,4,12
2352,2352,in this regard the guarantees provided for in relation to the proceedings of the commission pursuant to this regulation could serve as an appropriate point of reference,4,19
2353,2353,a prior fair and impartial procedure should be guaranteed before taking any final decision including the right to be heard of the persons concerned and the right to have access to the file while respecting confidentiality and professional and business secrecy as well as the obligation to give meaningful reasons for the decisions,4,21
2354,2354,this should not preclude the taking of measures however in duly substantiated cases of urgency and subject to appropriate conditions and procedural arrangements,4,22
2355,2355,the exercise of powers should also be proportionate to inter alia the nature and the overall actual or potential harm caused by the infringement or suspected infringement,4,10
2356,2356,the competent authorities should in principle take all relevant facts and circumstances of the case into account including information gathered by competent authorities in other member states,4,10
2357,2357,member states should ensure that violations of the obligations laid down in this regulation can be sanctioned in a manner that is effective proportionate and dissuasive taking into account the nature gravity recurrence and duration of the violation in view of the public interest pursued the scope and kind of activities carried out as well as the economic capacity of the infringer,4,1
2358,2358,in particular penalties should take into account whether the provider of intermediary services concerned systematically or recurrently fails to comply with its obligations stemming from this regulation as well as where relevant whether the provider is active in several member states,4,1
2359,2359,in order to ensure effective enforcement of this regulation individuals or representative organisations should be able to lodge any complaint related to compliance with this regulation with the digital services coordinator in the territory where they received the service without prejudice to this regulations rules on jurisdiction,4,23
2360,2360,complaints should provide a faithful overview of concerns related to a particular intermediary service providers compliance and could also inform the digital services coordinator of any more cross cutting issues,4,23
2361,2361,the digital services coordinator should involve other national competent authorities as well as the digital services coordinator of another member state and in particular the one of the member state where the provider of intermediary services concerned is established if the issue requires cross border cooperation,4,23
2362,2362,member states should ensure that digital services coordinators can take measures that are effective in addressing and proportionate to certain particularly serious and persistent infringements,4,23
2363,2363,especially where those measures can affect the rights and interests of third parties as may be the case in particular where the access to online interfaces is restricted it is appropriate to require that the measures be ordered by a competent judicial authority at the digital service coordinators request and are subject to additional safeguards,4,1
2364,2364,in particular third parties potentially affected should be afforded the opportunity to be heard and such orders should only be issued when powers to take such measures as provided by other acts of union law or by national law for instance to protect collective interests of consumers to ensure the prompt removal of web pages containing or disseminating child pornography or to disable access to services are being used by a third party to infringe an intellectual property right are not reasonably available,4,1
2365,2365,such an order to restrict access should not go beyond what is necessary to achieve its objective,4,22
2366,2366,for that purpose it should be temporary and be addressed in principle to a provider of intermediary services such as the relevant hosting service provider internet service provider or domain registry or registrar which is in a reasonable position to achieve that objective without unduly restricting access to lawful information,4,7
2367,2367,the digital services coordinator should regularly publish a report on the activities carried out under this regulation,4,23
2368,2368,given that the digital services coordinator is also made aware of orders to take action against illegal content or to provide information regulated by this regulation through the common information sharing system the digital services coordinator should include in its annual report the number and categories of these orders addressed to providers of intermediary services issued by judicial and administrative authorities in its member state,4,23
2369,2369,where a digital services coordinator requests another digital services coordinator to take action the requesting digital services coordinator or the board in case it issued a recommendation to assess issues involving more than three member states should be able to refer the matter to the commission in case of any disagreement as to the assessments or the measures taken or proposed or a failure to adopt any measures,4,23
2370,2370,the commission on the basis of the information made available by the concerned authorities should accordingly be able to request the competent digital services coordinator to re assess the matter and take the necessary measures to ensure compliance within a defined time period,4,23
2371,2371,this possibility is without prejudice to the commissions general duty to oversee the application of and where necessary enforce union law under the control of the court of justice of the european union in accordance with the treaties,4,2
2372,2372,a failure by the digital services coordinator of establishment to take any measures pursuant to such a request may also lead to the commissions intervention under section  of chapter iv of this regulation where the suspected infringer is a very large online platformin order to facilitate cross border supervision and investigations involving several member states the digital services coordinators should be able to participate on a permanent or temporary basis in joint oversight and investigation activities concerning matters covered by this regulation,4,23
2373,2373,those activities may include other competent authorities and may cover a variety of issues ranging from coordinated data gathering exercises to requests for information or inspections of premises within the limits and scope of powers available to each participating authority,4,19
2374,2374,the board may be requested to provide advice in relation to those activities for example by proposing roadmaps and timelines for activities or proposing ad hoc task forces with participation of the authorities involved,4,19
2375,2375,in view of the particular challenges that may emerge in relation to assessing and ensuring a very large online platforms compliance for instance relating to the scale or complexity of a suspected infringement or the need for particular expertise or capabilities at union level digital services coordinators should have the possibility to request on a voluntary basis the commission to intervene and exercise its investigatory and enforcement powers under this regulation,4,7
2376,2376,in order to ensure a consistent application of this regulation it is necessary to set up an independent advisory group at union level which should support the commission and help coordinate the actions of digital services coordinators,4,23
2377,2377,that european board for digital services should consist of the digital services coordinators without prejudice to the possibility for digital services coordinators to invite in its meetings or appoint ad hoc delegates from other competent authorities entrusted with specific tasks under this regulation where that is required pursuant to their national allocation of tasks and competences,4,23
2378,2378,in case of multiple participants from one member state the voting right should remain limited to one representative per member state,4,6
2379,2379,the board should contribute to achieving a common union perspective on the consistent application of this regulation and to cooperation among competent authorities including by advising the commission and the digital services coordinators about appropriate investigation and enforcement measures in particular vis  vis very large online platforms,4,7
2380,2380,the board should also contribute to the drafting of relevant templates and codes of conduct and analyse emerging general trends in the development of digital services in the union,4,23
2381,2381,for that purpose the board should be able to adopt opinions requests and recommendations addressed to digital services coordinators or other competent national authorities,4,23
2382,2382,while not legally binding the decision to deviate therefrom should be properly explained and could be taken into account by the commission in assessing the compliance of the member state concerned with this regulation,4,19
2383,2383,the board should bring together the representatives of the digital services coordinators and possible other competent authorities under the chairmanship of the commission with a view to ensuring an assessment of matters submitted to it in a fully european dimension,4,23
2384,2384,in view of possible cross cutting elements that may be of relevance for other regulatory frameworks at union level the board should be allowed to cooperate with other union bodies offices agencies and advisory groups with responsibilities in fields such as equality including equality between women and men and non discrimination data protection electronic communications audiovisual services detection and investigation of frauds against the eu budget as regards custom duties or consumer protection as necessary for the performance of its tasks,4,2
2385,2385,the commission through the chair should participate in the board without voting rights,4,19
2386,2386,through the chair the commission should ensure that the agenda of the meetings is set in accordance with the requests of the members of the board as laid down in the rules of procedure and in compliance with the duties of the board laid down in this regulation,4,19
2387,2387,in view of the need to ensure support for the boards activities the board should be able to rely on the expertise and human resources of the commission and of the competent national authorities,4,19
2388,2388,the specific operational arrangements for the internal functioning of the board should be further specified in the rules of procedure of the board,4,19
2389,2389,given the importance of very large online platforms in view of their reach and impact their failure to comply with the specific obligations applicable to them may affect a substantial number of recipients of the services across different member states and may cause large societal harms while such failures may also be particularly complex to identify and address,4,7
2390,2390,in order to address those public policy concerns it is therefore necessary to provide for a common system of enhanced supervision and enforcement at union level,4,1
2391,2391,once an infringement of one of the provisions that solely apply to very large online platforms has been identified for instance pursuant to individual or joint investigations auditing or complaints the digital services coordinator of establishment upon its own initiative or upon the boards advice should monitor any subsequent measure taken by the very large online platform concerned as set out in its action plan,4,7
2392,2392,that digital services coordinator should be able to ask where appropriate for an additional specific audit to be carried out on a voluntary basis to establish whether those measures are sufficient to address the infringement,4,23
2393,2393,at the end of that procedure it should inform the board the commission and the platform concerned of its views on whether or not that platform addressed the infringement specifying in particular the relevant conduct and its assessment of any measures taken,4,19
2394,2394,the digital services coordinator should perform its role under this common system in a timely manner and taking utmost account of any opinions and other advice of the board,4,23
2395,2395,where the infringement of the provision that solely applies to very large online platforms is not effectively addressed by that platform pursuant to the action plan only the commission may on its own initiative or upon advice of the board decide to further investigate the infringement concerned and the measures that the platform has subsequently taken to the exclusion of the digital services coordinator of establishment,4,7
2396,2396,after having conducted the necessary investigations the commission should be able to issue decisions finding an infringement and imposing sanctions in respect of very large online platforms where that is justified,4,7
2397,2397,it should also have such a possibility to intervene in cross border situations where the digital services coordinator of establishment did not take any measures despite the commissions request or in situations where the digital services coordinator of establishment itself requested for the commission to intervene in respect of an infringement of any other provision of this regulation committed by a very large online platform,4,23
2398,2398,the commission should remain free to decide whether or not it wishes to intervene in any of the situations where it is empowered to do so under this regulation,4,19
2399,2399,once the commission initiated the proceedings the digital services coordinators of establishment concerned should be precluded from exercising their investigatory and enforcement powers in respect of the relevant conduct of the very large online platform concerned so as to avoid duplication inconsistencies and risks from the viewpoint of the principle of ne bis in idem,4,23
2400,2400,however in the interest of effectiveness those digital services coordinators should not be precluded from exercising their powers either to assist the commission at its request in the performance of its supervisory tasks or in respect of other conduct including conduct by the same very large online platform that is suspected to constitute a new infringement,4,23
2401,2401,those digital services coordinators as well as the board and other digital services coordinators where relevant should provide the commission with all necessary information and assistance to allow it to perform its tasks effectively whilst conversely the commission should keep them informed on the exercise of its powers as appropriate,4,23
2402,2402,in that regard the commission should where appropriate take account of any relevant assessments carried out by the board or by the digital services coordinators concerned and of any relevant evidence and information gathered by them without prejudice to the commissions powers and responsibility to carry out additional investigations as necessary,4,19
2403,2403,in view of both the particular challenges that may arise in seeking to ensure compliance by very large online platforms and the importance of doing so effectively considering their size and impact and the harms that they may cause the commission should have strong investigative and enforcement powers to allow it to investigate enforce and monitor certain of the rules laid down in this regulation in full respect of the principle of proportionality and the rights and interests of the affected parties,4,7
2404,2404,in particular the commission should have access to any relevant documents data and information necessary to open and conduct investigations and to monitor the compliance with the relevant obligations laid down in this regulation irrespective of who possesses the documents data or information in question and regardless of their form or format their storage medium or the precise place where they are stored,4,19
2405,2405,the commission should be able to directly require that the very large online platform concerned or relevant third parties or than individuals provide any relevant evidence data and information,4,7
2406,2406,in addition the commission should be able to request any relevant information from any public authority body or agency within the member state or from any natural person or legal person for the purpose of this regulation,4,19
2407,2407,the commission should be empowered to require access to and explanations relating to data bases and algorithms of relevant persons and to interview with their consent any persons who may be in possession of useful information and to record the statements made,4,19
2408,2408,the commission should also be empowered to undertake such inspections as are necessary to enforce the relevant provisions of this regulation,4,19
2409,2409,those investigatory powers aim to complement the commissions possibility to ask digital services coordinators and other member states authorities for assistance for instance by providing information or in the exercise of those powerscompliance with the relevant obligations imposed under this regulation should be enforceable by means of fines and periodic penalty payments,4,8
2410,2410,to that end appropriate levels of fines and periodic penalty payments should also be laid down for non compliance with the obligations and breach of the procedural rules subject to appropriate limitation periods,4,22
2411,2411,the very large online platforms concerned and other persons subject to the exercise of the commissions powers whose interests may be affected by a decision should be given the opportunity of submitting their observations beforehand and the decisions taken should be widely publicised,4,7
2412,2412,while ensuring the rights of defence of the parties concerned in particular the right of access to the file it is essential that confidential information be protected,4,11
2413,2413,furthermore while respecting the confidentiality of the information the commission should ensure that any information relied on for the purpose of its decision is disclosed to an extent that allows the addressee of the decision to understand the facts and considerations that lead up to the decision,4,19
2414,2414,in the interest of effectiveness and efficiency in addition to the general evaluation of the regulation to be performed within five years of entry into force after the initial start up phase and on the basis of the first three years of application of this regulation the commission should also perform an evaluation of the activities of the board and on its structure,4,19
2415,2415,in order to ensure uniform conditions for the implementation of this regulation implementing powers should be conferred on the commission,4,19
2416,2416,those powers should be exercised in accordance with regulation eu no  of the european parliament and of the council,4,2
2417,2417,in order to fulfil the objectives of this regulation the power to adopt acts in accordance with article  of the treaty should be delegated to the commission to supplement this regulation,4,19
2418,2418,in particular delegated acts should be adopted in respect of criteria for identification of very large online platforms and of technical specifications for access requests,4,7
2419,2419,it is of particular importance that the commission carries out appropriate consultations and that those consultations be conducted in accordance with the principles laid down in the interinstitutional agreement on better law making of  april,4,19
2420,2420,in particular to ensure equal participation in the preparation of delegated acts the european parliament and the council receive all documents at the same time as member states experts and their experts systematically have access to meetings of commission expert groups dealing with the preparation of delegated acts,4,19
2421,2421,this regulation respects the fundamental rights recognised by the charter and the fundamental rights constituting general principles of union law,4,1
2422,2422,accordingly this regulation should be interpreted and applied in accordance with those fundamental rights including the freedom of expression and information as well as the freedom and pluralism of the media,4,1
2423,2423,when exercising the powers set out in this regulation all public authorities involved should achieve in situations where the relevant fundamental rights conflict a fair balance between the rights concerned in accordance with the principle of proportionality,4,16
2424,2424,since the objective of this regulation namely the proper functioning of the internal market and to ensure a safe predictable and trusted online environment in which the fundamental rights enshrined in the charter are duly protected cannot be sufficiently achieved by the member states because they cannot achieve the necessary harmonisation and cooperation by acting alone but can rather by reason of its territorial and personal scope be better achieved at the union level the union may adopt measures in accordance with the principle of subsidiarity as set out in article  of the treaty on european union,4,2
2425,2425,in accordance with the principle of proportionality as set out in that article this regulation does not go beyond what is necessary in order to achieve that objectivehave adopted this regulationchapter i  general provisionsarticle subject matter and scopethis regulation lays down harmonised rules on the provision of intermediary services in the internal market,4,1
2426,2426,in particular it establishesa framework for the conditional exemption from liability of providers of intermediary servicesrules on specific due diligence obligations tailored to certain specific categories of providers of intermediary servicesrules on the implementation and enforcement of this regulation including as regards the cooperation of and coordination between the competent authorities,4,10
2427,2427,the aims of this regulation are tocontribute to the proper functioning of the internal market for intermediary servicesset out uniform rules for a safe predictable and trusted online environment where fundamental rights enshrined in the charter are effectively protected,4,1
2428,2428,this regulation shall apply to intermediary services provided to recipients of the service that have their place of establishment or residence in the union irrespective of the place of establishment of the providers of those services,4,1
2429,2429,this regulation shall not apply to any service that is not an intermediary service or to any requirements imposed in respect of such a service irrespective of whether the service is provided through the use of an intermediary service,4,1
2430,2430,this regulation is without prejudice to the rules laid down by the followingdirective ecdirective ecunion law on copyright and related rightsregulation eu,4,1
2431,2431,on preventing the dissemination of terrorist content online tco once adoptedregulation eu on european production and preservation orders for electronic evidence in criminal matters and directive eu laying down harmonised rules on the appointment of legal representatives for the purpose of gathering evidence in criminal proceedings e evidence once adoptedregulation eu regulation eu union law on consumer protection and product safety including regulation eu union law on the protection of personal data in particular regulation eu  and directive ec,4,2
2432,2432,article definitionsfor the purpose of this regulation the following definitions shall applyinformation society services means services within the meaning of article  of directive eu recipient of the service means any natural or legal person who uses the relevant intermediary serviceconsumer means any natural person who is acting for purposes which are outside his or her trade business or professionto offer services in the union means enabling legal or natural persons in one or more member states to use the services of the provider of information society services which has a substantial connection to the union such a substantial connection is deemed to exist where the provider has an establishment in the union in the absence of such an establishment the assessment of a substantial connection is based on specific factual criteria such asa significant number of users in one or more member states orthe targeting of activities towards one or more member states,4,2
2433,2433,trader means any natural person or any legal person irrespective of whether privately or publicly owned who is acting including through any person acting in his or her name or on his or her behalf for purposes relating to his or her trade business craft or professionintermediary service means one of the following servicesa mere conduit service that consists of the transmission in a communication network of information provided by a recipient of the service or the provision of access to a communication networka caching service that consists of the transmission in a communication network of information provided by a recipient of the service involving the automatic intermediate and temporary storage of that information for the sole purpose of making more efficient the informations onward transmission to other recipients upon their requesta hosting service that consists of the storage of information provided by and at the request of a recipient of the serviceillegal content means any information which in itself or by its reference to an activity including the sale of products or provision of services is not in compliance with union law or the law of a member state irrespective of the precise subject matter or nature of that law online platform means a provider of a hosting service which at the request of a recipient of the service stores and disseminates to the public information unless that activity is a minor and purely ancillary feature of another service and for objective and technical reasons cannot be used without that other service and the integration of the feature into the other service is not a means to circumvent the applicability of this regulation,4,0
2434,2434,dissemination to the public means making information available at the request of the recipient of the service who provided the information to a potentially unlimited number of third partiesdistance contract means a contract within the meaning of article  of directive euonline interface means any software including a website or a part thereof and applications including mobile applicationsdigital services coordinator of establishment means the digital services coordinator of the member state where the provider of an intermediary service is established or its legal representative resides or is establisheddigital services coordinator of destination means the digital services coordinator of a member state where the intermediary service is providedadvertisement means information designed to promote the message of a legal or natural person irrespective of whether to achieve commercial or non commercial purposes and displayed by an online platform on its online interface against remuneration specifically for promoting that informationrecommender system means a fully or partially automated system used by an online platform to suggest in its online interface specific information to recipients of the service including as a result of a search initiated by the recipient or otherwise determining the relative order or prominence of information displayedpcontent moderation means the activities undertaken by providers of intermediary services aimed at detecting identifying and addressing illegal content or information incompatible with their terms and conditions provided by recipients of the service including measures taken that affect the availability visibility and accessibility of that illegal content or that information such as demotion disabling of access to or removal thereof or the recipients ability to provide that information such as the termination or suspension of a recipients accountqterms and conditions means all terms and conditions or specifications irrespective of their name or form which govern the contractual relationship between the provider of intermediary services and the recipients of the services,4,1
2435,2435,chapter ii  liability of providers of intermediary servicesarticle mere conduitwhere an information society service is provided that consists of the transmission in a communication network of information provided by a recipient of the service or the provision of access to a communication network the service provider shall not be liable for the information transmitted on condition that the providerdoes not initiate the transmissiondoes not select the receiver of the transmission anddoes not select or modify the information contained in the transmission,4,23
2436,2436,the acts of transmission and of provision of access referred to in paragraph  include the automatic intermediate and transient storage of the information transmitted in so far as this takes place for the sole purpose of carrying out the transmission in the communication network and provided that the information is not stored for any period longer than is reasonably necessary for the transmission,4,11
2437,2437,this article shall not affect the possibility for a court or administrative authority in accordance with member states legal systems of requiring the service provider to terminate or prevent an infringement,4,1
2438,2438,article cachingwhere an information society service is provided that consists of the transmission in a communication network of information provided by a recipient of the service the service provider shall not be liable for the automatic intermediate and temporary storage of that information performed for the sole purpose of making more efficient the informations onward transmission to other recipients of the service upon their request on condition thatthe provider does not modify the informationthe provider complies with conditions on access to the informationthe provider complies with rules regarding the updating of the information specified in a manner widely recognised and used by industrythe provider does not interfere with the lawful use of technology widely recognised and used by industry to obtain data on the use of the information andthe provider acts expeditiously to remove or to disable access to the information it has stored upon obtaining actual knowledge of the fact that the information at the initial source of the transmission has been removed from the network or access to it has been disabled or that a court or an administrative authority has ordered such removal or disablement,4,23
2439,2439,this article shall not affect the possibility for a court or administrative authority in accordance with member states legal systems of requiring the service provider to terminate or prevent an infringement,4,1
2440,2440,article hostingwhere an information society service is provided that consists of the storage of information provided by a recipient of the service the service provider shall not be liable for the information stored at the request of a recipient of the service on condition that the providerdoes not have actual knowledge of illegal activity or illegal content and as regards claims for damages is not aware of facts or circumstances from which the illegal activity or illegal content is apparent orupon obtaining such knowledge or awareness acts expeditiously to remove or to disable access to the illegal content,4,23
2441,2441,paragraph  shall not apply where the recipient of the service is acting under the authority or the control of the provider,4,1
2442,2442,paragraph  shall not apply with respect to liability under consumer protection law of online platforms allowing consumers to conclude distance contracts with traders where such an online platform presents the specific item of information or otherwise enables the specific transaction at issue in a way that would lead an average and reasonably well informed consumer to believe that the information or the product or service that is the object of the transaction is provided either by the online platform itself or by a recipient of the service who is acting under its authority or control,4,7
2443,2443,this article shall not affect the possibility for a court or administrative authority in accordance with member states legal systems of requiring the service provider to terminate or prevent an infringement,4,1
2444,2444,article voluntary own initiative investigations and legal complianceproviders of intermediary services shall not be deemed ineligible for the exemptions from liability referred to in articles   and  solely because they carry out voluntary own initiative investigations or other activities aimed at detecting identifying and removing or disabling of access to illegal content or take the necessary measures to comply with the requirements of union law including those set out in this regulation,4,1
2445,2445,article no general monitoring or active fact finding obligationsno general obligation to monitor the information which providers of intermediary services transmit or store nor actively to seek facts or circumstances indicating illegal activity shall be imposed on those providers,4,1
2446,2446,article orders to act against illegal contentproviders of intermediary services shall upon the receipt of an order to act against a specific item of illegal content issued by the relevant national judicial or administrative authorities on the basis of the applicable union or national law in conformity with union law inform the authority issuing the order of the effect given to the orders without undue delay specifying the action taken and the moment when the action was taken,4,1
2447,2447,member states shall ensure that the orders referred to in paragraph  meet the following conditionsthe orders contains the following elementsa statement of reasons explaining why the information is illegal content by reference to the specific provision of union or national law infringedone or more exact uniform resource locators and where necessary additional information enabling the identification of the illegal content concernedinformation about redress available to the provider of the service and to the recipient of the service who provided the contentthe territorial scope of the order on the basis of the applicable rules of union and national law including the charter and where relevant general principles of international law does not exceed what is strictly necessary to achieve its objectivethe order is drafted in the language declared by the provider and is sent to the point of contact appointed by the provider in accordance with article,4,1
2448,2448,the digital services coordinator from the member state of the judicial or administrative authority issuing the order shall without undue delay transmit a copy of the orders referred to in paragraph  to all other digital services coordinators through the system established in accordance with article,4,23
2449,2449,the conditions and requirements laid down in this article shall be without prejudice to requirements under national criminal procedural law in conformity with union law,4,10
2450,2450,article orders to provide informationproviders of intermediary services shall upon receipt of an order to provide a specific item of information about one or more specific individual recipients of the service issued by the relevant national judicial or administrative authorities on the basis of the applicable union or national law in conformity with union law inform without undue delay the authority of issuing the order of its receipt and the effect given to the order,4,1
2451,2451,member states shall ensure that orders referred to in paragraph  meet the following conditionsthe order contains the following elementsa statement of reasons explaining the objective for which the information is required and why the requirement to provide the information is necessary and proportionate to determine compliance by the recipients of the intermediary services with applicable union or national rules unless such a statement cannot be provided for reasons related to the prevention investigation detection and prosecution of criminal offencesinformation about redress available to the provider and to the recipients of the service concernedthe order only requires the provider to provide information already collected for the purposes of providing the service and which lies within its controlthe order is drafted in the language declared by the provider and is sent to the point of contact appointed by that provider in accordance with article the digital services coordinator from the member state of the national judicial or administrative authority issuing the order shall without undue delay transmit a copy of the order referred to in paragraph  to all digital services coordinators through the system established in accordance with article,4,1
2452,2452,the conditions and requirements laid down in this article shall be without prejudice to requirements under national criminal procedural law in conformity with union law,4,10
2453,2453,chapter iiidue diligence obligations for a transparent and safe online environmentsection provisions applicable to all providers of intermediary servicesarticle points of contactproviders of intermediary services shall establish a single point of contact allowing for direct communication by electronic means with member states authorities the commission and the board referred to in article  for the application of this regulation,4,7
2454,2454,providers of intermediary services shall make public the information necessary to easily identify and communicate with their single points of contact,4,1
2455,2455,providers of intermediary services shall specify in the information referred to in paragraph  the official language or languages of the union which can be used to communicate with their points of contact and which shall include at least one of the official languages of the member state in which the provider of intermediary services has its main establishment or where its legal representative resides or is established,4,1
2456,2456,article legal representativesproviders of intermediary services which do not have an establishment in the union but which offer services in the union shall designate in writing a legal or natural person as their legal representative in one of the member states where the provider offers its services,4,1
2457,2457,providers of intermediary services shall mandate their legal representatives to be addressed in addition to or instead of the provider by the member states authorities the commission and the board on all issues necessary for the receipt of compliance with and enforcement of decisions issued in relation to this regulation,4,1
2458,2458,providers of intermediary services shall provide their legal representative with the necessary powers and resource to cooperate with the member states authorities the commission and the board and comply with those decisions,4,1
2459,2459,the designated legal representative can be held liable for non compliance with obligations under this regulation without prejudice to the liability and legal actions that could be initiated against the provider of intermediary services,4,1
2460,2460,providers of intermediary services shall notify the name address the electronic mail address and telephone number of their legal representative to the digital service coordinator in the member state where that legal representative resides or is established,4,23
2461,2461,they shall ensure that that information is up to date,4,18
2462,2462,the designation of a legal representative within the union pursuant to paragraph  shall not amount to an establishment in the union,4,1
2463,2463,article terms and conditionsproviders of intermediary services shall include information on any restrictions that they impose in relation to the use of their service in respect of information provided by the recipients of the service in their terms and conditions,4,1
2464,2464,that information shall include information on any policies procedures measures and tools used for the purpose of content moderation including algorithmic decision making and human review,4,1
2465,2465,it shall be set out in clear and unambiguous language and shall be publicly available in an easily accessible format,4,1
2466,2466,providers of intermediary services shall act in a diligent objective and proportionate manner in applying and enforcing the restrictions referred to in paragraph  with due regard to the rights and legitimate interests of all parties involved including the applicable fundamental rights of the recipients of the service as enshrined in the charter,4,1
2467,2467,article transparency reporting obligations for providers of intermediary servicesproviders of intermediary services shall publish at least once a year clear easily comprehensible and detailed reports on any content moderation they engaged in during the relevant period,4,7
2468,2468,those reports shall include in particular information on the following as applicable the number of orders received from member states authorities categorised by the type of illegal content concerned including orders issued in accordance with articles  and  and the average time needed for taking the action specified in those ordersthe number of notices submitted in accordance with article  categorised by the type of alleged illegal content concerned any action taken pursuant to the notices by differentiating whether the action was taken on the basis of the law or the terms and conditions of the provider and the average time needed for taking the actionthe content moderation engaged in at the providers own initiative including the number and type of measures taken that affect the availability visibility and accessibility of information provided by the recipients of the service and the recipients ability to provide information categorised by the type of reason and basis for taking those measuresthe number of complaints received through the internal complaint handling system referred to in article  the basis for those complaints decisions taken in respect of those complaints the average time needed for taking those decisions and the number of instances where those decisions were reversed,4,24
2469,2469,paragraph  shall not apply to providers of intermediary services that qualify as micro or small enterprises within the meaning of the annex to recommendation ec,4,1
2470,2470,section additional provisions applicable to providers of hosting services including online platformsarticle notice and action mechanismsproviders of hosting services shall put mechanisms in place to allow any individual or entity to notify them of the presence on their service of specific items of information that the individual or entity considers to be illegal content,4,7
2471,2471,those mechanisms shall be easy to access user friendly and allow for the submission of notices exclusively by electronic means,4,15
2472,2472,the mechanisms referred to in paragraph  shall be such as to facilitate the submission of sufficiently precise and adequately substantiated notices on the basis of which a diligent economic operator can identify the illegality of the content in question,4,19
2473,2473,to that end the providers shall take the necessary measures to enable and facilitate the submission of notices containing all of the following elementsan explanation of the reasons why the individual or entity considers the information in question to be illegal contenta clear indication of the electronic location of that information in particular the exact url or urls and where necessary additional information enabling the identification of the illegal contentthe name and an electronic mail address of the individual or entity submitting the notice except in the case of information considered to involve one of the offences referred to in articles  to  of directive eua statement confirming the good faith belief of the individual or entity submitting the notice that the information and allegations contained therein are accurate and complete,4,2
2474,2474,notices that include the elements referred to in paragraph  shall be considered to give rise to actual knowledge or awareness for the purposes of article  in respect of the specific item of information concerned,4,14
2475,2475,where the notice contains the name and an electronic mail address of the individual or entity that submitted it the provider of hosting services shall promptly send a confirmation of receipt of the notice to that individual or entity,4,1
2476,2476,the provider shall also without undue delay notify that individual or entity of its decision in respect of the information to which the notice relates providing information on the redress possibilities in respect of that decision,4,1
2477,2477,providers of hosting services shall process any notices that they receive under the mechanisms referred to in paragraph  and take their decisions in respect of the information to which the notices relate in a timely diligent and objective manner,4,1
2478,2478,where they use automated means for that processing or decision making they shall include information on such use in the notification referred to in paragraph,4,15
2479,2479,article statement of reasonswhere a provider of hosting services decides to remove or disable access to specific items of information provided by the recipients of the service irrespective of the means used for detecting identifying or removing or disabling access to that information and of the reason for its decision it shall inform the recipient at the latest at the time of the removal or disabling of access of the decision and provide a clear and specific statement of reasons for that decision,4,14
2480,2480,the statement of reasons referred to in paragraph  shall at least contain the following informationwhether the decision entails either the removal of or the disabling of access to the information and where relevant the territorial scope of the disabling of accessthe facts and circumstances relied on in taking the decision including where relevant whether the decision was taken pursuant to a notice submitted in accordance with article where applicable information on the use made of automated means in taking the decision including where the decision was taken in respect of content detected or identified using automated meanswhere the decision concerns allegedly illegal content a reference to the legal ground relied on and explanations as to why the information is considered to be illegal content on that groundwhere the decision is based on the alleged incompatibility of the information with the terms and conditions of the provider a reference to the contractual ground relied on and explanations as to why the information is considered to be incompatible with that groundinformation on the redress possibilities available to the recipient of the service in respect of the decision in particular through internal complaint handling mechanisms out of court dispute settlement and judicial redress,4,1
2481,2481,the information provided by the providers of hosting services in accordance with this article shall be clear and easily comprehensible and as precise and specific as reasonably possible under the given circumstances,4,7
2482,2482,the information shall in particular be such as to reasonably allow the recipient of the service concerned to effectively exercise the redress possibilities referred to in point  of paragraph,4,1
2483,2483,providers of hosting services shall publish the decisions and the statements of reasons referred to in paragraph  in a publicly accessible database managed by the commission,4,19
2484,2484,that information shall not contain personal data,4,9
2485,2485,section additional provisions applicable to online platformsarticle exclusion for micro and small enterprisesthis section shall not apply to online platforms that qualify as micro or small enterprises within the meaning of the annex to recommendation ec,4,7
2486,2486,article internal complaint handling systemonline platforms shall provide recipients of the service for a period of at least six months following the decision referred to in this paragraph the access to an effective internal complaint handling system which enables the complaints to be lodged electronically and free of charge against the following decisions taken by the online platform on the ground that the information provided by the recipients is illegal content or incompatible with its terms and conditionsdecisions to remove or disable access to the informationdecisions to suspend or terminate the provision of the service in whole or in part to the recipientsdecisions to suspend or terminate the recipients account,4,7
2487,2487,online platforms shall ensure that their internal complaint handling systems are easy to access user friendly and enable and facilitate the submission of sufficiently precise and adequately substantiated complaints,4,7
2488,2488,online platforms shall handle complaints submitted through their internal complaint handling system in a timely diligent and objective manner,4,7
2489,2489,where a complaint contains sufficient grounds for the online platform to consider that the information to which the complaint relates is not illegal and is not incompatible with its terms and conditions or contains information indicating that the complainants conduct does not warrant the suspension or termination of the service or the account it shall reverse its decision referred to in paragraph  without undue delay,4,7
2490,2490,online platforms shall inform complainants without undue delay of the decision they have taken in respect of the information to which the complaint relates and shall inform complainants of the possibility of out of court dispute settlement provided for in article  and other available redress possibilities,4,7
2491,2491,online platforms shall ensure that the decisions referred to in paragraph  are not solely taken on the basis of automated means,4,7
2492,2492,article out of court dispute settlementrecipients of the service addressed by the decisions referred to in article  shall be entitled to select any out of court dispute that has been certified in accordance with paragraph  in order to resolve disputes relating to those decisions including complaints that could not be resolved by means of the internal complaint handling system referred to in that article,4,10
2493,2493,online platforms shall engage in good faith with the body selected with a view to resolving the dispute and shall be bound by the decision taken by the body,4,7
2494,2494,the first subparagraph is without prejudice to the right of the recipient concerned to redress against the decision before a court in accordance with the applicable law,4,10
2495,2495,the digital services coordinator of the member state where the out of court dispute settlement body is established shall at the request of that body certify the body where the body has demonstrated that it meets all of the following conditionsit is impartial and independent of online platforms and recipients of the service provided by the online platformsit has the necessary expertise in relation to the issues arising in one or more particular areas of illegal content or in relation to the application and enforcement of terms and conditions of one or more types of online platforms allowing the body to contribute effectively to the settlement of a disputethe dispute settlement is easily accessible through electronic communication technologyit is capable of settling dispute in a swift efficient and cost effective manner and in at least one official language of the unionthe dispute settlement takes place in accordance with clear and fair rules of procedure,4,23
2496,2496,the digital services coordinator shall where applicable specify in the certificate the particular issues to which the bodys expertise relates and the official language or languages of the union in which the body is capable of settling disputes as referred to in points  and  of the first subparagraph respectively,4,23
2497,2497,if the body decides the dispute in favour of the recipient of the service the online platform shall reimburse the recipient for any fees and other reasonable expenses that the recipient has paid or is to pay in relation to the dispute settlement,4,7
2498,2498,if the body decides the dispute in favour of the online platform the recipient shall not be required to reimburse any fees or other expenses that the online platform paid or is to pay in relation to the dispute settlement,4,7
2499,2499,the fees charged by the body for the dispute settlement shall be reasonable and shall in any event not exceed the costs thereof,4,10
2500,2500,certified out of court dispute settlement bodies shall make the fees or the mechanisms used to determine the fees known to the recipient of the services and the online platform concerned before engaging in the dispute settlement,4,10
2501,2501,member states may establish out of court dispute settlement bodies for the purposes of paragraph  or support the activities of some or all out of court dispute settlement bodies that they have certified in accordance with paragraph,4,10
2502,2502,member states shall ensure that any of their activities undertaken under the first subparagraph do not affect the ability of their digital services coordinators to certify the bodies concerned in accordance with paragraph,4,23
2503,2503,digital services coordinators shall notify to the commission the out of court dispute settlement bodies that they have certified in accordance with paragraph  including where applicable the specifications referred to in the second subparagraph of that paragraph,4,23
2504,2504,the commission shall publish a list of those bodies including those specifications on a dedicated website and keep it updated,4,19
2505,2505,this article is without prejudice to directive eu and alternative dispute resolution procedures and entities for consumers established under that directive,4,2
2506,2506,article trusted flaggersonline platforms shall take the necessary technical and organisational measures to ensure that notices submitted by trusted flaggers through the mechanisms referred to in article  are processed and decided upon with priority and without delay,4,1
2507,2507,the status of trusted flaggers under this regulation shall be awarded upon application by any entities by the digital services coordinator of the member state in which the applicant is established where the applicant has demonstrated to meet all of the following conditionsit has particular expertise and competence for the purposes of detecting identifying and notifying illegal contentit represents collective interests and is independent from any online platformit carries out its activities for the purposes of submitting notices in a timely diligent and objective manner,4,12
2508,2508,digital services coordinators shall communicate to the commission and the board the names addresses and electronic mail addresses of the entities to which they have awarded the status of the trusted flagger in accordance with paragraph,4,23
2509,2509,the commission shall publish the information referred to in paragraph  in a publicly available database and keep the database updated,4,19
2510,2510,where an online platform has information indicating that a trusted flagger submitted a significant number of insufficiently precise or inadequately substantiated notices through the mechanisms referred to in article  including information gathered in connection to the processing of complaints through the internal complaint handling systems referred to in article  it shall communicate that information to the digital services coordinator that awarded the status of trusted flagger to the entity concerned providing the necessary explanations and supporting documents,4,7
2511,2511,the digital services coordinator that awarded the status of trusted flagger to an entity shall revoke that status if it determines following an investigation either on its own initiative or on the basis information received by third parties including the information provided by an online platform pursuant to paragraph  that the entity no longer meets the conditions set out in paragraph,4,23
2512,2512,before revoking that status the digital services coordinator shall afford the entity an opportunity to react to the findings of its investigation and its intention to revoke the entitys status as trusted flaggerthe commission after consulting the board may issue guidance to assist online platforms and digital services coordinators in the application of paragraphs  and,4,23
2513,2513,article measures and protection against misuseonline platforms shall suspend for a reasonable period of time and after having issued a prior warning the provision of their services to recipients of the service that frequently provide manifestly illegal content,4,7
2514,2514,online platforms shall suspend for a reasonable period of time and after having issued a prior warning the processing of notices and complaints submitted through the notice and action mechanisms and internal complaints handling systems referred to in articles  and  respectively by individuals or entities or by complainants that frequently submit notices or complaints that are manifestly unfounded,4,7
2515,2515,online platforms shall assess on a case by case basis and in a timely diligent and objective manner whether a recipient individual entity or complainant engages in the misuse referred to in paragraphs  and  taking into account all relevant facts and circumstances apparent from the information available to the online platform,4,7
2516,2516,those circumstances shall include at least the followingthe absolute numbers of items of manifestly illegal content or manifestly unfounded notices or complaints submitted in the past yearthe relative proportion thereof in relation to the total number of items of information provided or notices submitted in the past yearthe gravity of the misuses and its consequencesthe intention of the recipient individual entity or complainant,4,24
2517,2517,online platforms shall set out in a clear and detailed manner their policy in respect of the misuse referred to in paragraphs  and  in their terms and conditions including as regards the facts and circumstances that they take into account when assessing whether certain behaviour constitutes misuse and the duration of the suspension,4,7
2518,2518,article notification of suspicions of criminal offenceswhere an online platform becomes aware of any information giving rise to a suspicion that a serious criminal offence involving a threat to the life or safety of persons has taken place is taking place or is likely to take place it shall promptly inform the law enforcement or judicial authorities of the member state or member states concerned of its suspicion and provide all relevant information available,4,12
2519,2519,where the online platform cannot identify with reasonable certainty the member state concerned it shall inform the law enforcement authorities of the member state in which it is established or has its legal representative or inform europol,4,7
2520,2520,for the purpose of this article the member state concerned shall be the member state where the offence is suspected to have taken place be taking place and likely to take place or the member state where the suspected offender resides or is located or the member state where the victim of the suspected offence resides or is located,4,12
2521,2521,article traceability of traderswhere an online platform allows consumers to conclude distance contracts with traders it shall ensure that traders can only use its services to promote messages on or to offer products or services to consumers located in the union if prior to the use of its services the online platform has obtained the following informationthe name address telephone number and electronic mail address of the tradera copy of the identification document of the trader or any other electronic identification as defined by article  of regulation eu no  of the european parliament and of the council  the bank account details of the trader where the trader is a natural personthe name address telephone number and electronic mail address of the economic operator within the meaning of article  and article  of regulation eu  of the european parliament and the council  or any relevant act of union lawwhere the trader is registered in a trade register or similar public register the trade register in which the trader is registered and its registration number or equivalent means of identification in that registera self certification by the trader committing to only offer products or services that comply with the applicable rules of union law,4,7
2522,2522,the online platform shall upon receiving that information make reasonable efforts to assess whether the information referred to in points   and  of paragraph  is reliable through the use of any freely accessible official online database or online interface made available by a member states or the union or through requests to the trader to provide supporting documents from reliable sources,4,7
2523,2523,where the online platform obtains indications that any item of information referred to in paragraph  obtained from the trader concerned is inaccurate or incomplete that platform shall request the trader to correct the information in so far as necessary to ensure that all information is accurate and complete without delay or within the time period set by union and national law,4,7
2524,2524,where the trader fails to correct or complete that information the online platform shall suspend the provision of its service to the trader until the request is complied with,4,7
2525,2525,the online platform shall store the information obtained pursuant to paragraph  and  in a secure manner for the duration of their contractual relationship with the trader concerned,4,7
2526,2526,they shall subsequently delete the information,4,22
2527,2527,without prejudice to paragraph  the platform shall only disclose the information to third parties where so required in accordance with the applicable law including the orders referred to in article  and any orders issued by member states competent authorities or the commission for the performance of their tasks under this regulation,4,1
2528,2528,the online platform shall make the information referred to in points    and  of paragraph  available to the recipients of the service in a clear easily accessible and comprehensible manner,4,7
2529,2529,the online platform shall design and organise its online interface in a way that enables traders to comply with their obligations regarding pre contractual information and product safety information under applicable union law,4,7
2530,2530,article transparency reporting obligations for providers of online platformsin addition to the information referred to in article  online platforms shall include in the reports referred to in that article information on the followingthe number of disputes submitted to the out of court dispute settlement bodies referred to in article  the outcomes of the dispute settlement and the average time needed for completing the dispute settlement proceduresthe number of suspensions imposed pursuant to article  distinguishing between suspensions enacted for the provision of manifestly illegal content the submission of manifestly unfounded notices and the submission of manifestly unfounded complaintsany use made of automatic means for the purpose of content moderation including a specification of the precise purposes indicators of the accuracy of the automated means in fulfilling those purposes and any safeguards applied,4,7
2531,2531,online platforms shall publish at least once every six months information on the average monthly active recipients of the service in each member state calculated as an average over the period of the past six months in accordance with the methodology laid down in the delegated acts adopted pursuant to article,4,7
2532,2532,online platforms shall communicate to the digital services coordinator of establishment upon its request the information referred to in paragraph  updated to the moment of such request,4,23
2533,2533,that digital services coordinator may require the online platform to provide additional information as regards the calculation referred to in that paragraph including explanations and substantiation in respect of the data used,4,23
2534,2534,that information shall not include personal data,4,9
2535,2535,the commission may adopt implementing acts to lay down templates concerning the form content and other details of reports pursuant to paragraph,4,19
2536,2536,article online advertising transparencyonline platforms that display advertising on their online interfaces shall ensure that the recipients of the service can identify for each specific advertisement displayed to each individual recipient in a clear and unambiguous manner and in real timethat the information displayed is an advertisementthe natural or legal person on whose behalf the advertisement is displayedmeaningful information about the main parameters used to determine the recipient to whom the advertisement is displayed,4,7
2537,2537,section additional obligations for very large online platforms to manage systemic risksarticle very large online platformsthis section shall apply to online platforms which provide their services to a number of average monthly active recipients of the service in the union equal to or higher than  million calculated in accordance with the methodology set out in the delegated acts referred to in paragraph,4,7
2538,2538,the commission shall adopt delegated acts in accordance with article  to adjust the number of average monthly recipients of the service in the union referred to in paragraph  where the unions population increases or decreases at least with   in relation to its population in  or after adjustment by means of a delegated act of its population in the year in which the latest delegated act was adopted,4,22
2539,2539,in that case it shall adjust the number so that it corresponds to  of the unions population in the year in which it adopts the delegated act rounded up or down to allow the number to be expressed in millions,4,22
2540,2540,the commission shall adopt delegated acts in accordance with article  after consulting the board to lay down a specific methodology for calculating the number of average monthly active recipients of the service in the union for the purposes of paragraph,4,19
2541,2541,the methodology shall specify in particular how to determine the unions population and criteria to determine the average monthly active recipients of the service in the union taking into account different accessibility features,4,6
2542,2542,the digital services coordinator of establishment shall verify at least every six months whether the number of average monthly active recipients of the service in the union of online platforms under their jurisdiction is equal to or higher than the number referred to in paragraph,4,23
2543,2543,on the basis of that verification it shall adopt a decision designating the online platform as a very large online platform for the purposes of this regulation or terminating that designation and communicate that decision without undue delay to the online platform concerned and to the commission,4,7
2544,2544,the commission shall ensure that the list of designated very large online platforms is published in the official journal of the european union and keep that list updated,4,7
2545,2545,the obligations of this section shall apply or cease to apply to the very large online platforms concerned from four months after that publication,4,7
2546,2546,article risk assessmentvery large online platforms shall identify analyse and assess from the date of application referred to in the second subparagraph of article  at least once a year thereafter any significant systemic risks stemming from the functioning and use made of their services in the union,4,3
2547,2547,this risk assessment shall be specific to their services and shall include the following systemic risksthe dissemination of illegal content through their servicesany negative effects for the exercise of the fundamental rights to respect for private and family life freedom of expression and information the prohibition of discrimination and the rights of the child as enshrined in articles    and  of the charter respectivelyintentional manipulation of their service including by means of inauthentic use or automated exploitation of the service with an actual or foreseeable negative effect on the protection of public health minors civic discourse or actual or foreseeable effects related to electoral processes and public security,4,12
2548,2548,when conducting risk assessments very large online platforms shall take into account in particular how their content moderation systems recommender systems and systems for selecting and displaying advertisement influence any of the systemic risks referred to in paragraph  including the potentially rapid and wide dissemination of illegal content and of information that is incompatible with their terms and conditions,4,7
2549,2549,article mitigation of risksvery large online platforms shall put in place reasonable proportionate and effective mitigation measures tailored to the specific systemic risks identified pursuant to article,4,7
2550,2550,such measures may include where applicableadapting content moderation or recommender systems their decision making processes the features or functioning of their services or their terms and conditionstargeted measures aimed at limiting the display of advertisements in association with the service they providereinforcing the internal processes or supervision of any of their activities in particular as regards detection of systemic riskinitiating or adjusting cooperation with trusted flaggers in accordance with article initiating or adjusting cooperation with other online platforms through the codes of conduct and the crisis protocols referred to in article  and  respectively,4,7
2551,2551,the board in cooperation with the commission shall publish comprehensive reports once a year which shall include the followingidentification and assessment of the most prominent and recurrent systemic risks reported by very large online platforms or identified through other information sources in particular those provided in compliance with article  and best practices for very large online platforms to mitigate the systemic risks identified,4,7
2552,2552,the commission in cooperation with the digital services coordinators may issue general guidelines on the application of paragraph  in relation to specific risks in particular to present best practices and recommend possible measures having due regard to the possible consequences of the measures on fundamental rights enshrined in the charter of all parties involved,4,23
2553,2553,when preparing those guidelines the commission shall organise public consultations,4,19
2554,2554,article independent auditvery large online platforms shall be subject at their own expense and at least once a year to audits to assess compliance with the followingthe obligations set out in chapter iiiany commitments undertaken pursuant to the codes of conduct referred to in articles  and  and the crisis protocols referred to in article,4,7
2555,2555,audits performed pursuant to paragraph  shall be performed by organisations whichare independent from the very large online platform concernedhave proven expertise in the area of risk management technical competence and capabilitieshave proven objectivity and professional ethics based in particular on adherence to codes of practice or appropriate standards,4,7
2556,2556,the organisations that perform the audits shall establish an audit report for each audit,4,0
2557,2557,the report shall be in writing and include at least the followingthe name address and the point of contact of the very large online platform subject to the audit and the period coveredthe name and address of the organisation performing the audita description of the specific elements audited and the methodology applieda description of the main findings drawn from the auditan audit opinion on whether the very large online platform subject to the audit complied with the obligations and with the commitments referred to in paragraph  either positive positive with comments or negativewhere the audit opinion is not positive operational recommendations on specific measures to achieve compliance,4,7
2558,2558,very large online platforms receiving an audit report that is not positive shall take due account of any operational recommendations addressed to them with a view to take the necessary measures to implement them,4,7
2559,2559,they shall within one month from receiving those recommendations adopt an audit implementation report setting out those measures,4,0
2560,2560,where they do not implement the operational recommendations they shall justify in the audit implementation report the reasons for not doing so and set out any alternative measures they may have taken to address any instances of non compliance identified,4,1
2561,2561,article recommender systemsvery large online platforms that use recommender systems shall set out in their terms and conditions in a clear accessible and easily comprehensible manner the main parameters used in their recommender systems as well as any options for the recipients of the service to modify or influence those main parameters that they may have made available including at least one option which is not based on profiling within the meaning of article   of regulation eu,4,7
2562,2562,where several options are available pursuant to paragraph  very large online platforms shall provide an easily accessible functionality on their online interface allowing the recipient of the service to select and to modify at any time their preferred option for each of the recommender systems that determines the relative order of information presented to them,4,7
2563,2563,article additional online advertising transparencyvery large online platforms that display advertising on their online interfaces shall compile and make publicly available through application programming interfaces a repository containing the information referred to in paragraph  until one year after the advertisement was displayed for the last time on their online interfaces,4,7
2564,2564,they shall ensure that the repository does not contain any personal data of the recipients of the service to whom the advertisement was or could have been displayed,4,9
2565,2565,the repository shall include at least all of the following informationthe content of the advertisementthe natural or legal person on whose behalf the advertisement is displayedthe period during which the advertisement was displayedwhether the advertisement was intended to be displayed specifically to one or more particular groups of recipients of the service and if so the main parameters used for that purposethe total number of recipients of the service reached and where applicable aggregate numbers for the group or groups of recipients to whom the advertisement was targeted specifically,4,24
2566,2566,article data access and scrutinyvery large online platforms shall provide the digital services coordinator of establishment or the commission upon their reasoned request and within a reasonable period specified in the request access to data that are necessary to monitor and assess compliance with this regulation,4,7
2567,2567,that digital services coordinator and the commission shall only use that data for those purposes,4,23
2568,2568,upon a reasoned request from the digital services coordinator of establishment or the commission very large online platforms shall within a reasonable period as specified in the request provide access to data to vetted researchers who meet the requirements in paragraphs  of this article for the sole purpose of conducting research that contributes to the identification and understanding of systemic risks as set out in article,4,7
2569,2569,very large online platforms shall provide access to data pursuant to paragraphs  and  through online databases or application programming interfaces as appropriate,4,7
2570,2570,in order to be vetted researchers shall be affiliated with academic institutions be independent from commercial interests have proven records of expertise in the fields related to the risks investigated or related research methodologies and shall commit and be in a capacity to preserve the specific data security and confidentiality requirements corresponding to each request,4,19
2571,2571,the commission shall after consulting the board adopt delegated acts laying down the technical conditions under which very large online platforms are to share data pursuant to paragraphs  and  and the purposes for which the data may be used,4,19
2572,2572,those delegated acts shall lay down the specific conditions under which such sharing of data with vetted researchers can take place in compliance with regulation eu  taking into account the rights and interests of the very large online platforms and the recipients of the service concerned including the protection of confidential information in particular trade secrets and maintaining the security of their service,4,7
2573,2573,within  days following receipt of a request as referred to in paragraph  and  a very large online platform may request the digital services coordinator of establishment or the commission as applicable to amend the request where it considers that it is unable to give access to the data requested because one of following two reasonsit does not have access to the datagiving access to the data will lead to significant vulnerabilities for the security of its service or the protection of confidential information in particular trade secrets,4,23
2574,2574,requests for amendment pursuant to point  of paragraph  shall contain proposals for one or more alternative means through which access may be provided to the requested data or other data which are appropriate and sufficient for the purpose of the request,4,2
2575,2575,the digital services coordinator of establishment or the commission shall decide upon the request for amendment within  days and communicate to the very large online platform its decision and where relevant the amended request and the new time period to comply with the request,4,23
2576,2576,article compliance officersvery large online platforms shall appoint one or more compliance officers responsible for monitoring their compliance with this regulation,4,7
2577,2577,very large online platforms shall only designate as compliance officers persons who have the professional qualifications knowledge experience and ability necessary to fulfil the tasks referred to in paragraph,4,7
2578,2578,compliance officers may either be staff members of or fulfil those tasks on the basis of a contract with the very large online platform concerned,4,7
2579,2579,compliance officers shall have the following taskscooperating with the digital services coordinator of establishment and the commission for the purpose of this regulationorganising and supervising the very large online platforms activities relating to the independent audit pursuant to article informing and advising the management and employees of the very large online platform about relevant obligations under this regulationmonitoring the very large online platforms compliance with its obligations under this regulation,4,7
2580,2580,very large online platforms shall take the necessary measures to ensure that the compliance officers can perform their tasks in an independent manner,4,7
2581,2581,very large online platforms shall communicate the name and contact details of the compliance officer to the digital services coordinator of establishment and the commission,4,7
2582,2582,very large online platforms shall support the compliance officer in the performance of his or her tasks and provide him or her with the resources necessary to adequately carry out those tasks,4,7
2583,2583,the compliance officer shall directly report to the highest management level of the platform,4,19
2584,2584,article transparency reporting obligations for very large online platformsvery large online platforms shall publish the reports referred to in article  within six months from the date of application referred to in article  and thereafter every six months,4,7
2585,2585,in addition to the reports provided for in article  very large online platforms shall make publicly available and transmit to the digital services coordinator of establishment and the commission at least once a year and within  days following the adoption of the audit implementing report provided for in article a report setting out the results of the risk assessment pursuant to article the related risk mitigation measures identified and implemented pursuant to article the audit report provided for in article the audit implementation report provided for in article,4,7
2586,2586,where a very large online platform considers that the publication of information pursuant to paragraph  may result in the disclosure of confidential information of that platform or of the recipients of the service may cause significant vulnerabilities for the security of its service may undermine public security or may harm recipients the platform may remove such information from the reports,4,7
2587,2587,in that case that platform shall transmit the complete reports to the digital services coordinator of establishment and the commission accompanied by a statement of the reasons for removing the information from the public reports,4,23
2588,2588,section other provisions concerning due diligence obligationsarticle standardsthe commission shall support and promote the development and implementation of voluntary industry standards set by relevant european and international standardisation bodies at least for the followingelectronic submission of notices under article electronic submission of notices by trusted flaggers under article  including through application programming interfacesspecific interfaces including application programming interfaces to facilitate compliance with the obligations set out in articles  and auditing of very large online platforms pursuant to article interoperability of the advertisement repositories referred to in article transmission of data between advertising intermediaries in support of transparency obligations pursuant to points  and  of article,4,7
2589,2589,the commission shall support the update of the standards in the light of technological developments and the behaviour of the recipients of the services in question,4,19
2590,2590,article codes of conductthe commission and the board shall encourage and facilitate the drawing up of codes of conduct at union level to contribute to the proper application of this regulation taking into account in particular the specific challenges of tackling different types of illegal content and systemic risks in accordance with union law in particular on competition and the protection of personal data,4,19
2591,2591,where significant systemic risk within the meaning of article  emerge and concern several very large online platforms the commission may invite the very large online platforms concerned other very large online platforms other online platforms and other providers of intermediary services as appropriate as well as civil society organisations and other interested parties to participate in the drawing up of codes of conduct including by setting out commitments to take specific risk mitigation measures as well as a regular reporting framework on any measures taken and their outcomes,4,7
2592,2592,when giving effect to paragraphs  and  the commission and the board shall aim to ensure that the codes of conduct clearly set out their objectives contain key performance indicators to measure the achievement of those objectives and take due account of the needs and interests of all interested parties including citizens at union level,4,19
2593,2593,the commission and the board shall also aim to ensure that participants report regularly to the commission and their respective digital service coordinators of establishment on any measures taken and their outcomes as measured against the key performance indicators that they contain,4,19
2594,2594,the commission and the board shall assess whether the codes of conduct meet the aims specified in paragraphs  and  and shall regularly monitor and evaluate the achievement of their objectives,4,19
2595,2595,they shall publish their conclusions,4,19
2596,2596,the board shall regularly monitor and evaluate the achievement of the objectives of the codes of conduct having regard to the key performance indicators that they may contain,4,19
2597,2597,article codes of conduct for online advertisingthe commission shall encourage and facilitate the drawing up of codes of conduct at union level between online platforms and other relevant service providers such as providers of online advertising intermediary services or organisations representing recipients of the service and civil society organisations or relevant authorities to contribute to further transparency in online advertising beyond the requirements of articles  and,4,7
2598,2598,the commission shall aim to ensure that the codes of conduct pursue an effective transmission of information in full respect for the rights and interests of all parties involved and a competitive transparent and fair environment in online advertising in accordance with union and national law in particular on competition and the protection of personal data,4,19
2599,2599,the commission shall aim to ensure that the codes of conduct address at leastthe transmission of information held by providers of online advertising intermediaries to recipients of the service with regard to requirements set in points  and  of article the transmission of information held by providers of online advertising intermediaries to the repositories pursuant to article,4,7
2600,2600,the commission shall encourage the development of the codes of conduct within one year following the date of application of this regulation and their application no later than six months after that date,4,19
2601,2601,article crisis protocolsthe board may recommend the commission to initiate the drawing up in accordance with paragraphs   and  of crisis protocols for addressing crisis situations strictly limited to extraordinary circumstances affecting public security or public health,4,19
2602,2602,the commission shall encourage and facilitate very large online platforms and where appropriate other online platforms with the involvement of the commission to participate in the drawing up testing and application of those crisis protocols which include one or more of the following measuresdisplaying prominent information on the crisis situation provided by member states authorities or at union levelensuring that the point of contact referred to in article  is responsible for crisis managementwhere applicable adapt the resources dedicated to compliance with the obligations set out in articles     and  to the needs created by the crisis situation,4,7
2603,2603,the commission may involve as appropriate member states authorities and union bodies offices and agencies in drawing up testing and supervising the application of the crisis protocols,4,19
2604,2604,the commission may where necessary and appropriate also involve civil society organisations or other relevant organisations in drawing up the crisis protocols,4,19
2605,2605,the commission shall aim to ensure that the crisis protocols set out clearly all of the followingthe specific parameters to determine what constitutes the specific extraordinary circumstance the crisis protocol seeks to address and the objectives it pursuesthe role of each participant and the measures they are to put in place in preparation and once the crisis protocol has been activateda clear procedure for determining when the crisis protocol is to be activateda clear procedure for determining the period during which the measures to be taken once the crisis protocol has been activated are to be taken which is strictly limited to what is necessary for addressing the specific extraordinary circumstances concernedsafeguards to address any negative effects on the exercise of the fundamental rights enshrined in the charter in particular the freedom of expression and information and the right to non discriminationa process to publicly report on any measures taken their duration and their outcomes upon the termination of the crisis situation,4,19
2606,2606,if the commission considers that a crisis protocol fails to effectively address the crisis situation or to safeguard the exercise of fundamental rights as referred to in point  of paragraph  it may request the participants to revise the crisis protocol including by taking additional measures,4,19
2607,2607,chapter ivimplementation cooperation sanctions and enforcementsection competent authorities and national digital services coordinatorsarticle competent authorities and digital services coordinatorsmember states shall designate one or more competent authorities as responsible for the application and enforcement of this regulation competent authorities,4,23
2608,2608,member states shall designate one of the competent authorities as their digital services coordinator,4,23
2609,2609,the digital services coordinator shall be responsible for all matters relating to application and enforcement of this regulation in that member state unless the member state concerned has assigned certain specific tasks or sectors to other competent authorities,4,23
2610,2610,the digital services coordinator shall in any event be responsible for ensuring coordination at national level in respect of those matters and for contributing to the effective and consistent application and enforcement of this regulation throughout the union,4,23
2611,2611,for that purpose digital services coordinators shall cooperate with each other other national competent authorities the board and the commission without prejudice to the possibility for member states to provide for regular exchanges of views with other authorities where relevant for the performance of the tasks of those other authorities and of the digital services coordinator,4,23
2612,2612,where a member state designates more than one competent authority in addition to the digital services coordinator it shall ensure that the respective tasks of those authorities and of the digital services coordinator are clearly defined and that they cooperate closely and effectively when performing their tasks,4,23
2613,2613,the member state concerned shall communicate the name of the other competent authorities as well as their respective tasks to the commission and the board,4,1
2614,2614,member states shall designate the digital services coordinators within two months from the date of entry into force of this regulation,4,23
2615,2615,member states shall make publicly available and communicate to the commission and the board the name of their competent authority designated as digital services coordinator and information on how it can be contacted,4,23
2616,2616,the requirements applicable to digital services coordinators set out in articles   and  shall also apply to any other competent authorities that the member states designate pursuant to paragraph,4,23
2617,2617,article requirements for digital services coordinatorsmember states shall ensure that their digital services coordinators perform their tasks under this regulation in an impartial transparent and timely manner,4,23
2618,2618,member states shall ensure that their digital services coordinators have adequate technical financial and human resources to carry out their tasks,4,23
2619,2619,when carrying out their tasks and exercising their powers in accordance with this regulation the digital services coordinators shall act with complete independence,4,23
2620,2620,they shall remain free from any external influence whether direct or indirect and shall neither seek nor take instructions from any other public authority or any private party,4,1
2621,2621,paragraph  is without prejudice to the tasks of digital services coordinators within the system of supervision and enforcement provided for in this regulation and the cooperation with other competent authorities in accordance with article,4,23
2622,2622,paragraph  shall not prevent supervision of the authorities concerned in accordance with national constitutional law,4,1
2623,2623,article jurisdictionthe member state in which the main establishment of the provider of intermediary services is located shall have jurisdiction for the purposes of chapters iii and iv of this regulation,4,1
2624,2624,a provider of intermediary services which does not have an establishment in the union but which offers services in the union shall for the purposes of chapters iii and iv be deemed to be under the jurisdiction of the member state where its legal representative resides or is established,4,1
2625,2625,where a provider of intermediary services fails to appoint a legal representative in accordance with article  all member states shall have jurisdiction for the purposes of chapters iii and iv,4,1
2626,2626,where a member state decides to exercise jurisdiction under this paragraph it shall inform all other member states and ensure that the principle of ne bis in idem is respected,4,1
2627,2627,paragraphs   and  are without prejudice to the second subparagraph of article  and the second subparagraph of article  and the tasks and powers of the commission under section,4,19
2628,2628,article powers of digital services coordinatorswhere needed for carrying out their tasks digital services coordinators shall have at least the following powers of investigation in respect of conduct by providers of intermediary services under the jurisdiction of their member statethe power to require those providers as well as any other persons acting for purposes related to their trade business craft or profession that may reasonably be aware of information relating to a suspected infringement of this regulation including organisations performing the audits referred to in articles  and  to provide such information within a reasonable time periodthe power to carry out on site inspections of any premises that those providers or those persons use for purposes related to their trade business craft or profession or to request other public authorities to do so in order to examine seize take or obtain copies of information relating to a suspected infringement in any form irrespective of the storage mediumthe power to ask any member of staff or representative of those providers or those persons to give explanations in respect of any information relating to a suspected infringement and to record the answers,4,23
2629,2629,where needed for carrying out their tasks digital services coordinators shall have at least the following enforcement powers in respect of providers of intermediary services under the jurisdiction of their member statethe power to accept the commitments offered by those providers in relation to their compliance with this regulation and to make those commitments bindingthe power to order the cessation of infringements and where appropriate to impose remedies proportionate to the infringement and necessary to bring the infringement effectively to an endthe power to impose fines in accordance with article  for failure to comply with this regulation including with any of the orders issued pursuant to paragraph the power to impose a periodic penalty payment in accordance with article  to ensure that an infringement is terminated in compliance with an order issued pursuant to point  of this paragraph or for failure to comply with any of the orders issued pursuant to paragraph the power to adopt interim measures to avoid the risk of serious harm,4,23
2630,2630,as regards points  and  of the first subparagraph digital services coordinators shall also have the enforcement powers set out in those points in respect of the other persons referred to in paragraph  for failure to comply with any of the orders issued to them pursuant to that paragraph,4,23
2631,2631,they shall only exercise those enforcement powers after having provided those others persons in good time with all relevant information relating to such orders including the applicable time period the fines or periodic payments that may be imposed for failure to comply and redress possibilities,4,22
2632,2632,where needed for carrying out their tasks digital services coordinators shall also have in respect of providers of intermediary services under the jurisdiction of their member state where all other powers pursuant to this article to bring about the cessation of an infringement have been exhausted the infringement persists and causes serious harm which cannot be avoided through the exercise of other powers available under union or national law the power to take the following measuresrequire the management body of the providers within a reasonable time period to examine the situation adopt and submit an action plan setting out the necessary measures to terminate the infringement ensure that the provider takes those measures and report on the measures takenwhere the digital services coordinator considers that the provider has not sufficiently complied with the requirements of the first indent that the infringement persists and causes serious harm and that the infringement entails a serious criminal offence involving a threat to the life or safety of persons request the competent judicial authority of that member state to order the temporary restriction of access of recipients of the service concerned by the infringement or only where that is not technically feasible to the online interface of the provider of intermediary services on which the infringement takes place,4,23
2633,2633,the digital services coordinator shall except where it acts upon the commissions request referred to in article  prior to submitting the request referred to in point  of the first subparagraph invite interested parties to submit written observations within a time period that shall not be less than two weeks describing the measures that it intends to request and identifying the intended addressee or addressees thereof,4,23
2634,2634,the provider the intended addressee or addressees and any other third party demonstrating a legitimate interest shall be entitled to participate in the proceedings before the competent judicial authority,4,12
2635,2635,any measure ordered shall be proportionate to the nature gravity recurrence and duration of the infringement without unduly restricting access to lawful information by recipients of the service concerned,4,22
2636,2636,the restriction shall be for a period of four weeks subject to the possibility for the competent judicial authority in its order to allow the digital services coordinator to extend that period for further periods of the same lengths subject to a maximum number of extensions set by that judicial authority,4,23
2637,2637,the digital services coordinator shall only extend the period where it considers having regard to the rights and interests of all parties affected by the restriction and all relevant circumstances including any information that the provider the addressee or addressees and any other third party that demonstrated a legitimate interest may provide to it that both of the following conditions have been metthe provider has failed to take the necessary measures to terminate the infringementthe temporary restriction does not unduly restrict access to lawful information by recipients of the service having regard to the number of recipients affected and whether any adequate and readily accessible alternatives exist,4,23
2638,2638,where the digital services coordinator considers that those two conditions have been met but it cannot further extend the period pursuant to the third subparagraph it shall submit a new request to the competent judicial authority as referred to in point  of the first subparagraph,4,23
2639,2639,the powers listed in paragraphs   and  are without prejudice to section,4,26
2640,2640,the measures taken by the digital services coordinators in the exercise of their powers listed in paragraphs   and  shall be effective dissuasive and proportionate having regard in particular to the nature gravity recurrence and duration of the infringement or suspected infringement to which those measures relate as well as the economic technical and operational capacity of the provider of the intermediary services concerned where relevant,4,23
2641,2641,member states shall ensure that any exercise of the powers pursuant to paragraphs   and  is subject to adequate safeguards laid down in the applicable national law in conformity with the charter and with the general principles of union law,4,1
2642,2642,in particular those measures shall only be taken in accordance with the right to respect for private life and the rights of defence including the rights to be heard and of access to the file and subject to the right to an effective judicial remedy of all affected parties,4,12
2643,2643,article penaltiesmember states shall lay down the rules on penalties applicable to infringements of this regulation by providers of intermediary services under their jurisdiction and shall take all the necessary measures to ensure that they are implemented in accordance with article,4,1
2644,2644,penalties shall be effective proportionate and dissuasive,4,22
2645,2645,member states shall notify the commission of those rules and of those measures and shall notify it without delay of any subsequent amendments affecting them,4,1
2646,2646,member states shall ensure that the maximum amount of penalties imposed for a failure to comply with the obligations laid down in this regulation shall not exceed   of the annual income or turnover of the provider of intermediary services concerned,4,1
2647,2647,penalties for the supply of incorrect incomplete or misleading information failure to reply or rectify incorrect incomplete or misleading information and to submit to an on site inspection shall not exceed  of the annual income or turnover of the provider concerned,4,10
2648,2648,member states shall ensure that the maximum amount of a periodic penalty payment shall not exceed   of the average daily turnover of the provider of intermediary services concerned in the preceding financial year per day calculated from the date specified in the decision concerned,4,22
2649,2649,article right to lodge a complaintrecipients of the service shall have the right to lodge a complaint against providers of intermediary services alleging an infringement of this regulation with the digital services coordinator of the member state where the recipient resides or is established,4,23
2650,2650,the digital services coordinator shall assess the complaint and where appropriate transmit it to the digital services coordinator of establishment,4,23
2651,2651,where the complaint falls under the responsibility of another competent authority in its member state the digital service coordinator receiving the complaint shall transmit it to that authority,4,23
2652,2652,article activity reportsdigital services coordinators shall draw up an annual report on their activities under this regulation,4,1
2653,2653,they shall make the annual reports available to the public and shall communicate them to the commission and to the board,4,19
2654,2654,the annual report shall include at least the following informationthe number and subject matter of orders to act against illegal content and orders to provide information issued in accordance with articles  and  by any national judicial or administrative authority of the member state of the digital services coordinator concernedthe effects given to those orders as communicated to the digital services coordinator pursuant to articles  and,4,24
2655,2655,where a member state has designated several competent authorities pursuant to article  it shall ensure that the digital services coordinator draws up a single report covering the activities of all competent authorities and that the digital services coordinator receives all relevant information and support needed to that effect from the other competent authorities concerned,4,23
2656,2656,article cross border cooperation among digital services coordinatorswhere a digital services coordinator has reasons to suspect that a provider of an intermediary service not under the jurisdiction of the member state concerned infringed this regulation it shall request the digital services coordinator of establishment to assess the matter and take the necessary investigatory and enforcement measures to ensure compliance with this regulation,4,23
2657,2657,where the board has reasons to suspect that a provider of intermediary services infringed this regulation in a manner involving at least three member states it may recommend the digital services coordinator of establishment to assess the matter and take the necessary investigatory and enforcement measures to ensure compliance with this regulation,4,23
2658,2658,a request or recommendation pursuant to paragraph  shall at least indicatethe point of contact of the provider of the intermediary services concerned as provided for in article a description of the relevant facts the provisions of this regulation concerned and the reasons why the digital services coordinator that sent the request or the board suspects that the provider infringed this regulationany other information that the digital services coordinator that sent the request or the board considers relevant including where appropriate information gathered on its own initiative or suggestions for specific investigatory or enforcement measures to be taken including interim measures,4,23
2659,2659,the digital services coordinator of establishment shall take into utmost account the request or recommendation pursuant to paragraph,4,23
2660,2660,where it considers that it has insufficient information to act upon the request or recommendation and has reasons to consider that the digital services coordinator that sent the request or the board could provide additional information it may request such information,4,23
2661,2661,the time period laid down in paragraph  shall be suspended until that additional information is provided,4,22
2662,2662,the digital services coordinator of establishment shall without undue delay and in any event not later than two months following receipt of the request or recommendation communicate to the digital services coordinator that sent the request or the board its assessment of the suspected infringement or that of any other competent authority pursuant to national law where relevant and an explanation of any investigatory or enforcement measures taken or envisaged in relation thereto to ensure compliance with this regulation,4,23
2663,2663,where the digital services coordinator that sent the request or where appropriate the board did not receive a reply within the time period laid down in paragraph  or where it does not agree with the assessment of the digital services coordinator of establishment it may refer the matter to the commission providing all relevant information,4,23
2664,2664,that information shall include at least the request or recommendation sent to the digital services coordinator of establishment any additional information provided pursuant to paragraph  and the communication referred to in paragraph,4,23
2665,2665,the commission shall assess the matter within three months following the referral of the matter pursuant to paragraph  after having consulted the digital services coordinator of establishment and unless it referred the matter itself the board,4,23
2666,2666,where pursuant to paragraph  the commission concludes that the assessment or the investigatory or enforcement measures taken or envisaged pursuant to paragraph  are incompatible with this regulation it shall request the digital service coordinator of establishment to further assess the matter and take the necessary investigatory or enforcement measures to ensure compliance with this regulation and to inform it about those measures taken within two months from that request,4,23
2667,2667,article joint investigations and requests for commission interventiondigital services coordinators may participate in joint investigations which may be coordinated with the support of the board with regard to matters covered by this regulation concerning providers of intermediary services operating in several member states,4,23
2668,2668,such joint investigations are without prejudice to the tasks and powers of the participating digital coordinators and the requirements applicable to the performance of those tasks and exercise of those powers provided in this regulation,4,23
2669,2669,the participating digital services coordinators shall make the results of the joint investigations available to other digital services coordinators the commission and the board through the system provided for in article  for the fulfilment of their respective tasks under this regulation,4,23
2670,2670,where a digital services coordinator of establishment has reasons to suspect that a very large online platform infringed this regulation it may request the commission to take the necessary investigatory and enforcement measures to ensure compliance with this regulation in accordance with section,4,23
2671,2671,such a request shall contain all information listed in article  and set out the reasons for requesting the commission to intervene,4,19
2672,2672,section european board for digital servicesarticle european board for digital servicesan independent advisory group of digital services coordinators on the supervision of providers of intermediary services named european board for digital services the board is established,4,23
2673,2673,the board shall advise the digital services coordinators and the commission in accordance with this regulation to achieve the following objectivescontributing to the consistent application of this regulation and effective cooperation of the digital services coordinators and the commission with regard to matters covered by this regulationcoordinating and contributing to guidance and analysis of the commission and digital services coordinators and other competent authorities on emerging issues across the internal market with regard to matters covered by this regulationassisting the digital services coordinators and the commission in the supervision of very large online platforms,4,23
2674,2674,article structure of the board the board shall be composed of the digital services coordinators who shall be represented by high level officials,4,23
2675,2675,where provided for by national law other competent authorities entrusted with specific operational responsibilities for the application and enforcement of this regulation alongside the digital services coordinator shall participate in the board,4,23
2676,2676,other national authorities may be invited to the meetings where the issues discussed are of relevance for them,4,1
2677,2677,each member state shall have one vote,4,1
2678,2678,the commission shall not have voting rights,4,19
2679,2679,the board shall adopt its acts by simple majority,4,19
2680,2680,the board shall be chaired by the commission,4,19
2681,2681,the commission shall convene the meetings and prepare the agenda in accordance the tasks of the board pursuant to this regulation and with its rules of procedure,4,19
2682,2682,the commission shall provide administrative and analytical support for the activities of the board pursuant to this regulation,4,19
2683,2683,the board may invite experts and observers to attend its meetings and may cooperate with other union bodies offices agencies and advisory groups as well as external experts as appropriate,4,1
2684,2684,the board shall make the results of this cooperation publicly available,4,19
2685,2685,the board shall adopt its rules of procedure following the consent of the commission,4,19
2686,2686,article tasks of the boardwhere necessary to meet the objectives set out in article  the board shall in particularsupport the coordination of joint investigationssupport the competent authorities in the analysis of reports and results of audits of very large online platforms to be transmitted pursuant to this regulationissue opinions recommendations or advice to digital services coordinators in accordance with this regulationadvise the commission to take the measures referred to in article  and where requested by the commission adopt opinions on draft commission measures concerning very large online platforms in accordance with this regulationsupport and promote the development and implementation of european standards guidelines reports templates and code of conducts as provided for in this regulation as well as the identification of emerging issues with regard to matters covered by this regulation,4,7
2687,2687,digital services coordinators and other national competent authorities that do not follow the opinions requests or recommendations addressed to them adopted by the board shall provide the reasons for this choice when reporting pursuant to this regulation or when adopting their relevant decisions as appropriate,4,23
2688,2688,section supervision investigation enforcement and monitoring in respect of very large online platformsarticle enhanced supervision for very large online platformswhere the digital services coordinator of establishment adopts a decision finding that a very large online platform has infringed any of the provisions of section  of chapter iii it shall make use of the enhanced supervision system laid down in this article,4,7
2689,2689,it shall take utmost account of any opinion and recommendation of the commission and the board pursuant to this article,4,19
2690,2690,the commission acting on its own initiative or the board acting on its own initiative or upon request of at least three digital services coordinators of destination may where it has reasons to suspect that a very large online platform infringed any of those provisions recommend the digital services coordinator of establishment to investigate the suspected infringement with a view to that digital services coordinator adopting such a decision within a reasonable time period,4,23
2691,2691,when communicating the decision referred to in the first subparagraph of paragraph  to the very large online platform concerned the digital services coordinator of establishment shall request it to draw up and communicate to the digital services coordinator of establishment the commission and the board within one month from that decision an action plan specifying how that platform intends to terminate or remedy the infringement,4,23
2692,2692,the measures set out in the action plan may include where appropriate participation in a code of conduct as provided for in article,4,1
2693,2693,within one month following receipt of the action plan the board shall communicate its opinion on the action plan to the digital services coordinator of establishment,4,23
2694,2694,within one month following receipt of that opinion that digital services coordinator shall decide whether the action plan is appropriate to terminate or remedy the infringement,4,23
2695,2695,where the digital services coordinator of establishment has concerns on the ability of the measures to terminate or remedy the infringement it may request the very large online platform concerned to subject itself to an additional independent audit to assess the effectiveness of those measures in terminating or remedying the infringement,4,23
2696,2696,in that case that platform shall send the audit report to that digital services coordinator the commission and the board within four months from the decision referred to in the first subparagraph,4,19
2697,2697,when requesting such an additional audit the digital services coordinator may specify a particular audit organisation that is to carry out the audit at the expense of the platform concerned selected on the basis of criteria set out in article,4,23
2698,2698,the digital services coordinator of establishment shall communicate to the commission the board and the very large online platform concerned its views as to whether the very large online platform has terminated or remedied the infringement and the reasons thereof,4,23
2699,2699,it shall do so within the following time periods as applicablewithin one month from the receipt of the audit report referred to in the second subparagraph of paragraph  where such an audit was performedwithin three months from the decision on the action plan referred to in the first subparagraph of paragraph  where no such audit was performedimmediately upon the expiry of the time period set out in paragraph  where that platform failed to communicate the action plan within that time period,4,22
2700,2700,pursuant to that communication the digital services coordinator of establishment shall no longer be entitled to take any investigatory or enforcement measures in respect of the relevant conduct by the very large online platform concerned without prejudice to article  or any other measures that it may take at the request of the commission,4,23
2701,2701,article intervention by the commission and opening of proceedingsthe commission acting either upon the boards recommendation or on its own initiative after consulting the board may initiate proceedings in view of the possible adoption of decisions pursuant to articles  and  in respect of the relevant conduct by the very large online platform thatis suspected of having infringed any of the provisions of this regulation and the digital services coordinator of establishment did not take any investigatory or enforcement measures pursuant to the request of the commission referred to in article  upon the expiry of the time period set in that requestis suspected of having infringed any of the provisions of this regulation and the digital services coordinator of establishment requested the commission to intervene in accordance with article  upon the reception of that requesthas been found to have infringed any of the provisions of section  of chapter iii upon the expiry of the relevant time period for the communication referred to in article,4,19
2702,2702,where the commission decides to initiate proceedings pursuant to paragraph  it shall notify all digital services coordinators the board and the very large online platform concerned,4,19
2703,2703,as regards points  and  of paragraph  pursuant to that notification the digital services coordinator of establishment concerned shall no longer be entitled to take any investigatory or enforcement measures in respect of the relevant conduct by the very large online platform concerned without prejudice to article  or any other measures that it may take at the request of the commission,4,23
2704,2704,the digital services coordinator referred to in articles   and  as applicable shall without undue delay upon being informed transmit to the commissionany information that that digital services coordinator exchanged relating to  the infringement or the suspected infringement as applicable with the board and with the very large online platform concernedthe case file of that digital services coordinator relating to the infringement or the suspected infringement as applicableany other information in the possession of that digital services coordinator that may be relevant to the proceedings initiated by the commission,4,23
2705,2705,the board and the digital services coordinators making the request referred to in article  shall without undue delay upon being informed transmit to the commission any information in their possession that may be relevant to the proceedings initiated by the commission,4,23
2706,2706,article requests for informationin order to carry out the tasks assigned to it under this section the commission may by simple request or by decision require the very large online platforms concerned as well as any other persons acting for purposes related to their trade business craft or profession that may be reasonably be aware of information relating to the suspected infringement or the infringement as applicable including organisations performing the audits referred to in articles  and  to provide such information within a reasonable time period,4,19
2707,2707,when sending a simple request for information to the very large online platform concerned or other person referred to in article  the commission shall state the legal basis and the purpose of the request specify what information is required and set the time period within which the information is to be provided and the penalties provided for in article  for supplying incorrect or misleading information,4,7
2708,2708,where the commission requires the very large online platform concerned or other person referred to in article  to supply information by decision it shall state the legal basis and the purpose of the request specify what information is required and set the time period within which it is to be provided,4,7
2709,2709,it shall also indicate the penalties provided for in article  and indicate or impose the periodic penalty payments provided for in article,4,22
2710,2710,it shall further indicate the right to have the decision reviewed by the court of justice of the european union,4,2
2711,2711,the owners of the very large online platform concerned or other person referred to in article  or their representatives and in the case of legal persons companies or firms or where they have no legal personality the persons authorised to represent them by law or by their constitution shall supply the information requested on behalf of the very large online platform concerned or other person referred to in article,4,7
2712,2712,lawyers duly authorised to act may supply the information on behalf of their clients,4,10
2713,2713,the latter shall remain fully responsible if the information supplied is incomplete incorrect or misleading,4,10
2714,2714,at the request of the commission the digital services coordinators and other competent authorities shall provide the commission with all necessary information to carry out the tasks assigned to it under this section,4,23
2715,2715,article power to take interviews and statementsin order to carry out the tasks assigned to it under this section the commission may interview any natural or legal person which consents to being interviewed for the purpose of collecting information relating to the subject matter of an investigation in relation to the suspected infringement or infringement as applicable,4,19
2716,2716,article power to conduct on site inspectionsin order to carry out the tasks assigned to it under this section the commission may conduct on site inspections at the premises of the very large online platform concerned or other person referred to in article,4,19
2717,2717,on site inspections may also be carried out with the assistance of auditors or experts appointed by the commission pursuant to article,4,19
2718,2718,during on site inspections the commission and auditors or experts appointed by it may require the very large online platform concerned or other person referred to in article  to provide explanations on its organisation functioning it system algorithms data handling and business conducts,4,7
2719,2719,the commission and auditors or experts appointed by it may address questions to key personnel of the very large online platform concerned or other person referred to in article,4,7
2720,2720,the very large online platform concerned or other person referred to in article  is required to submit to an on site inspection ordered by decision of the commission,4,7
2721,2721,the decision shall specify the subject matter and purpose of the visit set the date on which it is to begin and indicate the penalties provided for in articles  and  and the right to have the decision reviewed by the court of justice of the european union,4,2
2722,2722,article interim measuresin the context of proceedings which may lead to the adoption of a decision of non compliance pursuant to article  where there is an urgency due to the risk of serious damage for the recipients of the service the commission may by decision order interim measures against the very large online platform concerned on the basis of a prima facie finding of an infringement,4,7
2723,2723,a decision under paragraph  shall apply for a specified period of time and may be renewed in so far this is necessary and appropriate,4,22
2724,2724,article commitmentsif during proceedings under this section the very large online platform concerned offers commitments to ensure compliance with the relevant provisions of this regulation the commission may by decision make those commitments binding on the very large online platform concerned and declare that there are no further grounds for action,4,7
2725,2725,the commission may upon request or on its own initiative reopen the proceedingswhere there has been a material change in any of the facts on which the decision was basedwhere the very large online platform concerned acts contrary to its commitments orwhere the decision was based on incomplete incorrect or misleading information provided by the very large online platform concerned or other person referred to in article,4,19
2726,2726,where the commission considers that the commitments offered by the very large online platform concerned are unable to ensure effective compliance with the relevant provisions of this regulation it shall reject those commitments in a reasoned decision when concluding the proceedings,4,7
2727,2727,article monitoring actionsfor the purposes of carrying out the tasks assigned to it under this section the commission may take the necessary actions to monitor the effective implementation and compliance with this regulation by the very large online platform concerned,4,7
2728,2728,the commission may also order that platform to provide access to and explanations relating to its databases and algorithms,4,19
2729,2729,the actions pursuant to paragraph  may include the appointment of independent external experts and auditors to assist the commission in monitoring compliance with the relevant provisions of this regulation and to provide specific expertise or knowledge to the commission,4,19
2730,2730,article non compliancethe commission shall adopt a non compliance decision where it finds that the very large online platform concerned does not comply with one or more of the followingthe relevant provisions of this regulationinterim measures ordered pursuant to article commitments made binding pursuant to article before adopting the decision pursuant to paragraph  the commission shall communicate its preliminary findings to the very large online platform concerned,4,7
2731,2731,in the preliminary findings the commission shall explain the measures that it considers taking or that it considers that the very large online platform concerned should take in order to effectively address the preliminary findings,4,19
2732,2732,in the decision adopted pursuant to paragraph  the commission shall order the very large online platform concerned to take the necessary measures to ensure compliance with the decision pursuant to paragraph  within a reasonable time period and to provide information on the measures that that platform intends to take to comply with the decision,4,7
2733,2733,the very large online platform concerned shall provide the commission with a description of the measures it has taken to ensure compliance with the decision pursuant to paragraph  upon their implementation,4,7
2734,2734,where the commission finds that the conditions of paragraph  are not met it shall close the investigation by a decision,4,19
2735,2735,article finesin the decision pursuant to article  the commission may impose on the very large online platform concerned fines not exceeding  of its total turnover in the preceding financial year where it finds that that platform intentionally or negligentlyinfringes the relevant provisions of this regulationfails to comply with a decision ordering interim measures under article  orfails to comply with a voluntary measure made binding by a decision pursuant to articles,4,7
2736,2736,the commission may by decision impose on the very large online platform concerned or other person referred to in article  fines not exceeding  of the total turnover in the preceding financial year where they intentionally or negligentlysupply incorrect incomplete or misleading information in response to a request pursuant to article  or when the information is requested by decision fail to reply to the request within the set time periodfail to rectify within the time period set by the commission incorrect incomplete or misleading information given by a member of staff or fail or refuse to provide complete informationrefuse to submit to an on site inspection pursuant to article,4,7
2737,2737,before adopting the decision pursuant to paragraph  the commission shall communicate its preliminary findings to the very large online platform concerned or other person referred to in article,4,19
2738,2738,in fixing the amount of the fine the commission shall have regard to the nature gravity duration and recurrence of the infringement and for fines imposed pursuant to paragraph  the delay caused to the proceedings,4,10
2739,2739,article periodic penalty paymentsthe commission may by decision impose on the very large online platform concerned or other person referred to in article  as applicable periodic penalty payments not exceeding   of the average daily turnover in the preceding financial year per day calculated from the date appointed by the decision in order to compel them tosupply correct and complete information in response to a decision requiring information pursuant to article submit to an on site inspection which it has ordered by decision pursuant to article comply with a decision ordering interim measures pursuant to article comply with commitments made legally binding by a decision pursuant to article comply with a decision pursuant to article,4,7
2740,2740,where the very large online platform concerned or other person referred to in article  has satisfied the obligation which the periodic penalty payment was intended to enforce the commission may fix the definitive amount of the periodic penalty payment at a figure lower than that which would arise under the original decision,4,7
2741,2741,article limitation period for the imposition of penaltiesthe powers conferred on the commission by articles  and  shall be subject to a limitation period of five years,4,22
2742,2742,time shall begin to run on the day on which the infringement is committed,4,22
2743,2743,however in the case of continuing or repeated infringements time shall begin to run on the day on which the infringement ceases,4,22
2744,2744,any action taken by the commission or by the digital services coordinator for the purpose of the investigation or proceedings in respect of an infringement shall interrupt the limitation period for the imposition of fines or periodic penalty payments,4,23
2745,2745,actions which interrupt the limitation period shall include in particular the followingrequests for information by the commission or by a digital services coordinatoron site inspectionthe opening of a proceeding by the commission pursuant to article,4,19
2746,2746,each interruption shall start time running afresh,4,22
2747,2747,however the limitation period for the imposition of fines or periodic penalty payments shall expire at the latest on the day on which a period equal to twice the limitation period has elapsed without the commission having imposed a fine or a periodic penalty payment,4,22
2748,2748,that period shall be extended by the time during which the limitation period is suspended pursuant to paragraph,4,22
2749,2749,the limitation period for the imposition of fines or periodic penalty payments shall be suspended for as long as the decision of the commission is the subject of proceedings pending before the court of justice of the european union,4,22
2750,2750,article limitation period for the enforcement of penaltiesthe power of the commission to enforce decisions taken pursuant to articles  and  shall be subject to a limitation period of five years,4,22
2751,2751,time shall begin to run on the day on which the decision becomes final,4,22
2752,2752,the limitation period for the enforcement of penalties shall be interruptedby notification of a decision varying the original amount of the fine or periodic penalty payment or refusing an application for variationby any action of the commission or of a member state acting at the request of the commission designed to enforce payment of the fine or periodic penalty payment,4,22
2753,2753,each interruption shall start time running afresh,4,22
2754,2754,the limitation period for the enforcement of penalties shall be suspended for so long astime to pay is allowedenforcement of payment is suspended pursuant to a decision of the court of justice of the european union,4,22
2755,2755,article right to be heard and access to the filebefore adopting a decision pursuant to articles   or  the commission shall give the very large online platform concerned or other person referred to in article  the opportunity of being heard on preliminary findings of the commission including any matter to which the commission has taken objections andmeasures that the commission may intend to take in view of the preliminary findings referred to point,4,19
2756,2756,the very large online platform concerned or other person referred to in article  may submit their observations on the commissions preliminary findings within a reasonable time period set by the commission in its preliminary findings which may not be less than  days,4,19
2757,2757,the commission shall base its decisions only on objections on which the parties concerned have been able to comment,4,19
2758,2758,the rights of defence of the parties concerned shall be fully respected in the proceedings,4,12
2759,2759,they shall be entitled to have access to the commissions file under the terms of a negotiated disclosure subject to the legitimate interest of the very large online platform concerned or other person referred to in article  in the protection of their business secrets,4,7
2760,2760,the right of access to the file shall not extend to confidential information and internal documents of the commission or member states authorities,4,19
2761,2761,in particular the right of access shall not extend to correspondence between the commission and those authorities,4,19
2762,2762,nothing in this paragraph shall prevent the commission from disclosing and using information necessary to prove an infringement,4,19
2763,2763,the information collected pursuant to articles   and  shall be used only for the purpose of this regulation,4,1
2764,2764,without prejudice to the exchange and to the use of information referred to in articles  and  the commission the board member states authorities and their respective officials servants and other persons working under their supervision and any other natural or legal person involved including auditors and experts appointed pursuant to article  shall not disclose information acquired or exchanged by them pursuant to this section and of the kind covered by the obligation of professional secrecy,4,1
2765,2765,article publication of decisionsthe commission shall publish the decisions it adopts pursuant to articles     and,4,19
2766,2766,such publication shall state the names of the parties and the main content of the decision including any penalties imposed,4,19
2767,2767,the publication shall have regard to the rights and legitimate interests of the very large online platform concerned any other person referred to in article  and any third parties in the protection of their confidential information,4,7
2768,2768,article requests for access restrictions and cooperation with national courtswhere all powers pursuant to this article to bring about the cessation of an infringement of this regulation have been exhausted the infringement persists and causes serious harm which cannot be avoided through the exercise of other powers available under union or national law the commission may request the digital services coordinator of establishment of the very large online platform concerned to act pursuant to article,4,7
2769,2769,prior to making such request to the digital services coordinator the commission shall invite interested parties to submit written observations within a time period that shall not be less than two weeks describing the measures it intends to request and identifying the intended addressee or addressees thereof,4,23
2770,2770,where the coherent application of this regulation so requires the commission acting on its own initiative may submit written observations to the competent judicial authority referred to article,4,19
2771,2771,with the permission of the judicial authority in question it may also make oral observations,4,19
2772,2772,for the purpose of the preparation of its observations only the commission may request that judicial authority to transmit or ensure the transmission to it of any documents necessary for the assessment of the case,4,19
2773,2773,article implementing acts relating to commission interventionin relation to the commission intervention covered by this section the commission may adopt implementing acts concerning the practical arrangements forthe proceedings pursuant to articles  and the hearings provided for in article the negotiated disclosure of information provided for in article,4,19
2774,2774,those implementing acts shall be adopted in accordance with the advisory procedure referred to in article,4,19
2775,2775,before the adoption of any measures pursuant to paragraph  the commission shall publish a draft thereof and invite all interested parties to submit their comments within the time period set out therein which shall not be less than one month,4,19
2776,2776,section common provisions on enforcementarticle information sharing systemthe commission shall establish and maintain a reliable and secure information sharing system supporting communications between digital services coordinators the commission and the board,4,23
2777,2777,the digital services coordinators the commission and the board shall use the information sharing system for all communications pursuant to this regulation,4,23
2778,2778,the commission shall adopt implementing acts laying down the practical and operational arrangements for the functioning of the information sharing system and its interoperability with other relevant systems,4,19
2779,2779,those implementing acts shall be adopted in accordance with the advisory procedure referred to in article,4,19
2780,2780,article representationwithout prejudice to directive xxeu of the european parliament and of the council   recipients of intermediary services shall have the right to mandate a body organisation or association to exercise the rights referred to in articles   and  on their behalf provided the body organisation or association meets all of the following conditionsit operates on a not for profit basisit has been properly constituted in accordance with the law of a member stateits statutory objectives include a legitimate interest in ensuring that this regulation is complied with,4,1
2781,2781,section delegated actsarticle exercise of the delegationthe power to adopt delegated acts is conferred on the commission subject to the conditions laid down in this article,4,19
2782,2782,the delegation of power referred to in articles   and  shall be conferred on the commission for an indeterminate period of time from date of expected adoption of the regulation,4,19
2783,2783,the delegation of power referred to in articles   and  may be revoked at any time by the european parliament or by the council,4,2
2784,2784,a decision of revocation shall put an end to the delegation of power specified in that decision,4,22
2785,2785,it shall take effect the day following that of its publication in the official journal of the european union or at a later date specified therein,4,2
2786,2786,it shall not affect the validity of any delegated acts already in force,4,22
2787,2787,as soon as it adopts a delegated act the commission shall notify it simultaneously to the european parliament and to the council,4,2
2788,2788,a delegated act adopted pursuant to articles   and  shall enter into force only if no objection has been expressed by either the european parliament or the council within a period of three months of notification of that act to the european parliament and the council or if before the expiry of that period the european parliament and the council have both informed the commission that they will not object,4,2
2789,2789,that period shall be extended by three months at the initiative of the european parliament or of the council,4,2
2790,2790,article committeethe commission shall be assisted by the digital services committee,4,23
2791,2791,that committee shall be a committee within the meaning of regulation  eu no,4,2
2792,2792,where reference is made to this article article  of regulation  eu no  shall apply,4,2
2793,2793,chapter vfinal provisionsarticle deletion of certain provisions of directive ecarticles  to  of directive ec shall be deleted,4,22
2794,2794,references to articles  to  of directive ec shall be construed as references to articles    and  of this regulation respectively,4,1
2795,2795,article amendments to directive xxec on representative actions for the protection of the collective interests of consumersthe following is added to annex ix regulation of the european parliament and of the council on a single market for digital services digital services act and amending directive ecarticle evaluationby five years after the entry into force of this regulation at the latest and every five years thereafter the commission shall evaluate this regulation and report to the european parliament the council and the european economic and social committee,4,2
2796,2796,for the purpose of paragraph  member states and the board shall send information on the request of the commission,4,19
2797,2797,in carrying out the evaluations referred to in paragraph  the commission shall take into account the positions and findings of the european parliament the council and other relevant bodies or sources,4,19
2798,2798,by three years from the date of application of this regulation at the latest the commission after consulting the board shall carry out an assessment of the functioning of the board and shall report it to the european parliament the council and the european economic and social committee taking into account the first years of application of the regulation,4,2
2799,2799,on the basis of the findings and taking into utmost account the opinion of the board that report shall where appropriate be accompanied by a proposal for amendment of this regulation with regard to the structure of the board,4,19
2800,2800,this explanatory memorandum accompanies the proposal for a regulation laying down harmonised rules on artificial intelligence artificial intelligence act,5,2
2801,2801,artificial intelligence ai is a fast evolving family of technologies that can bring a wide array of economic and societal benefits across the entire spectrum of industries and social activities,5,4
2802,2802,by improving prediction optimising operations and resource allocation and personalising service delivery the use of artificial intelligence can support socially and environmentally beneficial outcomes and provide key competitive advantages to companies and the european economy,5,0
2803,2803,such action is especially needed in high impact sectors including climate change environment and health the public sector finance mobility home affairs and agriculture,5,17
2804,2804,however the same elements and techniques that power the socio economic benefits of ai can also bring about new risks or negative consequences for individuals or the society,5,0
2805,2805,in light of the speed of technological change and possible challenges the eu is committed to strive for a balanced approach,5,2
2806,2806,it is in the union interest to preserve the eus technological leadership and to ensure that europeans can benefit from new technologies developed and functioning according to union values fundamental rights and principles,5,2
2807,2807,this proposal delivers on the political commitment by president von der leyen who announced in her political guidelines for the   commission a union that strives for more that the commission would put forward legislation for a coordinated european approach on the human and ethical implications of ai,5,2
2808,2808,following on that announcement on  february  the commission published the white paper on ai a european approach to excellence and trust,5,2
2809,2809,the white paper sets out policy options on how to achieve the twin objective of promoting the uptake of ai and of addressing the risks associated with certain uses of such technology,5,2
2810,2810,this proposal aims to implement the second objective for the development of an ecosystem of trust by proposing a legal framework for trustworthy ai,5,2
2811,2811,the proposal is based on eu values and fundamental rights and aims to give people and other users the confidence to embrace ai based solutions while encouraging businesses to develop them,5,2
2812,2812,ai should be a tool for people and be a force for good in society with the ultimate aim of increasing human well being,5,4
2813,2813,rules for ai available in the union market or otherwise affecting people in the union should therefore be human centric so that people can trust that the technology is used in a way that is safe and compliant with the law including the respect of fundamental rights,5,2
2814,2814,following the publication of the white paper the commission launched a broad stakeholder consultation which was met with a great interest by a large number of stakeholders who were largely supportive of regulatory intervention to address the challenges and concerns raised by the increasing use of ai,5,19
2815,2815,the proposal also responds to explicit requests from the european parliament ep and the european council which have repeatedly expressed calls for legislative action to ensure a well functioning internal market for artificial intelligence systems ai systems where both benefits and risks of ai are adequately addressed at union level,5,2
2816,2816,it supports the objective of the union being a global leader in the development of secure trustworthy and ethical artificial intelligence as stated by the european council and ensures the protection of ethical principles as specifically requested by the european parliament,5,2
2817,2817,in  the european council called for a sense of urgency to address emerging trends including issues such as artificial intelligence  while at the same time ensuring a high level of data protection digital rights and ethical standards,5,2
2818,2818,in its  conclusions on the coordinated plan on the development and use of artificial intelligence made in europe the council further highlighted the importance of ensuring that european citizens rights are fully respected and called for a review of the existing relevant legislation to make it fit for purpose for the new opportunities and challenges raised by ai,5,2
2819,2819,the european council has also called for a clear determination of the ai applications that should be considered high risk,5,2
2820,2820,the most recent conclusions from  october  further called for addressing the opacity complexity bias a certain degree of unpredictability and partially autonomous behaviour of certain ai systems to ensure their compatibility with fundamental rights and to facilitate the enforcement of legal rules,5,4
2821,2821,the european parliament has also undertaken a considerable amount of work in the area of ai,5,2
2822,2822,in october  it adopted a number of resolutions related to ai including on ethics liability and copyright,5,2
2823,2823,in  those were followed by resolutions on ai in criminal matters and in education culture and the audio visual sector,5,4
2824,2824,the ep resolution on a framework of ethical aspects of artificial intelligence robotics and related technologies specifically recommends to the commission to propose legislative action to harness the opportunities and benefits of ai but also to ensure protection of ethical principles,5,2
2825,2825,the resolution includes a text of the legislative proposal for a regulation on ethical principles for the development deployment and use of ai robotics and related technologies,5,2
2826,2826,in accordance with the political commitment made by president von der leyen in her political guidelines as regards resolutions adopted by the european parliament under article  tfeu this proposal takes into account the aforementioned resolution of the european parliament in full respect of proportionality subsidiarity and better law making principles,5,2
2827,2827,against this political context the commission puts forward the proposed regulatory framework on artificial intelligence with the following specific objectives ensure that ai systems placed on the union market and used are safe and respect existing law on fundamental rights and union valuesensure legal certainty to facilitate investment and innovation in aienhance governance and effective enforcement of existing law on fundamentalrights and safety requirements applicable to ai systemsfacilitate the development of a single market for lawful safe and trustworthy ai applications and prevent market fragmentation,5,8
2828,2828,to achieve those objectives this proposal presents a balanced and proportionate horizontal regulatory approach to ai that is limited to the minimum necessary requirements to address the risks and problems linked to ai without unduly constraining or hindering technological development or otherwise disproportionately increasing the cost of placing ai solutions on the market,5,8
2829,2829,the proposal sets a robust and flexible legal framework,5,26
2830,2830,on the one hand it is comprehensive and future proof in its fundamental regulatory choices including the principle based requirements that ai systems should comply with,5,5
2831,2831,on the other hand it puts in place a proportionate regulatory system centred on a well defined risk based regulatory approach that does not create unnecessary restrictions to trade whereby legal intervention is tailored to those concrete situations where there is a justified cause for concern or where such concern can reasonably be anticipated in the near future,5,8
2832,2832,at the same time the legal framework includes flexible mechanisms that enable it to be dynamically adapted as the technology evolves and new concerning situations emerge,5,8
2833,2833,the proposal sets harmonised rules for the development placement on the market and use of ai systems in the union following a proportionate risk based approach,5,5
2834,2834,it proposes a single future proof definition of ai,5,2
2835,2835,certain particularly harmful ai practices are prohibited as contravening union values while specific restrictions and safeguards are proposed in relation to certain uses of remote biometric identification systems for the purpose of law enforcement,5,12
2836,2836,the proposal lays down a solid risk methodology to define high risk ai systems that pose significant risks to the health and safety or fundamental rights of persons,5,5
2837,2837,those ai systems will have to comply with a set of horizontal mandatory requirements for trustworthy ai and follow conformity assessment procedures before those systems can be placed on the union market,5,5
2838,2838,predictable proportionate and clear obligations are also placed on providers and users of those systems to ensure safety and respect of existing legislation protecting fundamental rights throughout the whole ai systems lifecycle,5,5
2839,2839,for some specific ai systems only minimum transparency obligations are proposed in particular when chatbots or deep fakes are used,5,25
2840,2840,the proposed rules will be enforced through a governance system at member states level building on already existing structures and a cooperation mechanism at union level with the establishment of a european artificial intelligence board,5,2
2841,2841,additional measures are also proposed to support innovation in particular through ai regulatory sandboxes and other measures to reduce the regulatory burden and to support small and medium sized enterprises smes and start ups,5,8
2842,2842,the horizontal nature of the proposal requires full consistency with existing union legislation applicable to sectors where high risk ai systems are already used or likely to be used in the near future,5,5
2843,2843,consistency is also ensured with the eu charter of fundamental rights and the existing secondary union legislation on data protection consumer protection non discrimination and gender equality,5,2
2844,2844,the proposal is without prejudice and complements the general data protection regulation regulation eu  and the law enforcement directive directive eu  with a set of harmonised rules applicable to the design development and use of certain high risk ai systems and restrictions on certain uses of remote biometric identification systems,5,12
2845,2845,furthermore the proposal complements existing union law on non discrimination with specific requirements that aim to minimise the risk of algorithmic discrimination in particular in relation to the design and the quality of data sets used for the development of ai systems complemented with obligations for testing risk management documentation and human oversight throughout the ai systems lifecycle,5,6
2846,2846,the proposal is without prejudice to the application of union competition law,5,6
2847,2847,as regards high risk ai systems which are safety components of products this proposal will be integrated into the existing sectoral safety legislation to ensure consistency avoid duplications and minimise additional burdens,5,5
2848,2848,in particular as regards high risk ai systems related to products covered by the new legislative framework nlf legislation eg,5,5
2849,2849,machinery medical devices toys the requirements for ai systems set out in this proposal will be checked as part of the existing conformity assessment procedures under the relevant nlf legislation,5,27
2850,2850,with regard to the interplay of requirements while the safety risks specific to ai systems are meant to be covered by the requirements of this proposal nlf legislation aims at ensuring the overall safety of the final product and therefore may contain specific requirements regarding the safe integration of an ai system into the final product,5,5
2851,2851,the proposal for a machinery regulation which is adopted on the same day as this proposal fully reflects this approach,5,13
2852,2852,as regards high risk ai systems related to products covered by relevant old approach legislation eg,5,5
2853,2853,aviation cars this proposal would not directly apply,5,25
2854,2854,however the ex ante essential requirements for high risk ai systems set out in this proposal will have to be taken into account when adopting relevant implementing or delegated legislation under those acts,5,5
2855,2855,as regards ai systems provided or used by regulated credit institutions the authorities responsible for the supervision of the unions financial services legislation should be designated as competent authorities for supervising the requirements in this proposal to ensure a coherent enforcement of the obligations under this proposal and the unions financial services legislation where ai systems are to some extent implicitly regulated in relation to the internal governance system of credit institutions,5,0
2856,2856,to further enhance consistency the conformity assessment procedure and some of the providers procedural obligations under this proposal are integrated into the procedures under directive eu on access to the activity of credit institutions and the prudential supervision,5,18
2857,2857,this proposal is also consistent with the applicable union legislation on services including on intermediary services regulated by the e commerce directive ec and the commissions recent proposal for the digital services act dsa,5,2
2858,2858,in relation to ai systems that are components of large scale it systems in the area of freedom security and justice managed by the european union agency for the operational management of large scale it systems eu lisa the proposal will not apply to those ai systems that have been placed on the market or put into service before one year has elapsed from the date of application of this regulation unless the replacement or amendment of those legal acts leads to a significant change in the design or intended purpose of the ai system or ai systems concerned,5,5
2859,2859,the proposal is part of a wider comprehensive package of measures that address problems posed by the development and use of ai as examined in the white paper on ai,5,4
2860,2860,consistency and complementarity is therefore ensured with other ongoing or planned initiatives of the commission that also aim to address those problems including the revision of sectoral product legislation eg,5,19
2861,2861,the machinery directive the general product safety directive and initiatives that address liability issues related to new technologies including ai systems,5,15
2862,2862,those initiatives will build on and complement this proposal in order to bring legal clarity and foster the development of an ecosystem of trust in ai in europe,5,2
2863,2863,the proposal is also coherent with the commissions overall digital strategy in its contribution to promoting technology that works for people one of the three main pillars of the policy orientation and objectives announced in the communication shaping europes digital future,5,2
2864,2864,it lays down a coherent effective and proportionate framework to ensure ai is developed in ways that respect peoples rights and earn their trust making europe fit for the digital age and turning the next ten years into the digital decade,5,2
2865,2865,furthermore the promotion of ai driven innovation is closely linked to the data governance act the open data directive and other initiatives under the eu strategy for data which will establish trusted mechanisms and services for the re use sharing and pooling of data that are essential for the development of data driven ai models of high quality,5,2
2866,2866,the proposal also strengthens significantly the unions role to help shape global norms and standards and promote trustworthy ai that is consistent with union values and interests,5,1
2867,2867,it provides the union with a powerful basis to engage further with its external partners including third countries and at international fora on issues relating to ai,5,1
2868,2868,the legal basis for the proposal is in the first place article  of the treaty on the functioning of the european union tfeu which provides for the adoption of measures to ensure the establishment and functioning of the internal market,5,2
2869,2869,this proposal constitutes a core part of the eu digital single market strategy,5,2
2870,2870,the primary objective of this proposal is to ensure the proper functioning of the internal market by setting harmonised rules in particular on the development placing on the union market and the use of products and services making use of ai technologies or provided as stand alone ai systems,5,0
2871,2871,some member states are already considering national rules to ensure that ai is safe and is developed and used in compliance with fundamental rights obligations,5,8
2872,2872,this will likely lead to two main problems i a fragmentation of the internal market on essential elements regarding in particular the requirements for the ai products and services their marketing their use the liability and the supervision by public authorities and ii the substantial diminishment of legal certainty for both providers and users of ai systems on how existing and new rules will apply to those systems in the union,5,0
2873,2873,given the wide circulation of products and services across borders these two problems can be best solved through eu harmonizing legislation,5,2
2874,2874,indeed the proposal defines common mandatory requirements applicable to the design and development of certain ai systems before they are placed on the market that will be further operationalised through harmonised technical standards,5,5
2875,2875,the proposal also addresses the situation after ai systems have been placed on the market by harmonising the way in which ex post controls are conducted,5,5
2876,2876,in addition considering that this proposal contains certain specific rules on the protection of individuals with regard to the processing of personal data notably restrictions of the use of ai systems for real time remote biometric identification in publicly accessible spaces for the purpose of law enforcement it is appropriate to base this regulation in as far as those specific rules are concerned on article  of the tfeu,5,12
2877,2877,the nature of ai which often relies on large and varied datasets and which may be embedded in any product or service circulating freely within the internal market entails that the objectives of this proposal cannot be effectively achieved by member states alone,5,0
2878,2878,furthermore an emerging patchwork of potentially divergent national rules will hamper the seamless circulation of products and services related to ai systems across the eu and will be ineffective in ensuring the safety and protection of fundamental rights and union values across the different member states,5,2
2879,2879,national approaches in addressing the problems will only create additional legal uncertainty and barriers and will slow market uptake of ai,5,8
2880,2880,the objectives of this proposal can be better achieved at union level to avoid a further fragmentation of the single market into potentially contradictory national frameworks preventing the free circulation of goods and services embedding ai,5,0
2881,2881,a solid european regulatory framework for trustworthy ai will also ensure a level playing field and protect all people while strengthening europes competitiveness and industrial basis in ai,5,2
2882,2882,only common action at union level can also protect the unions digital sovereignty and leverage its tools and regulatory powers to shape global rules and standards,5,1
2883,2883,the proposal builds on existing legal frameworks and is proportionate and necessary to achieve its objectives since it follows a risk based approach and imposes regulatory burdens only when an ai system is likely to pose high risks to fundamental rights and safety,5,5
2884,2884,for other non high risk ai systems only very limited transparency obligations are imposed for example in terms of the provision of information to flag the use of an ai system when interacting with humans,5,5
2885,2885,for high risk ai systems the requirements of high quality data documentation and traceability transparency human oversight accuracy and robustness are strictly necessary to mitigate the risks to fundamental rights and safety posed by ai and that are not covered by other existing legal frameworks,5,5
2886,2886,harmonised standards and supporting guidance and compliance tools will assist providers and users in complying with the requirements laid down by the proposal and minimise their costs,5,27
2887,2887,the costs incurred by operators are proportionate to the objectives achieved and the economic and reputational benefits that operators can expect from this proposal,5,0
2888,2888,the choice of a regulation as a legal instrument is justified by the need for a uniform application of the new rules such as definition of ai the prohibition of certain harmful aienabled practices and the classification of certain ai systems,5,8
2889,2889,the direct applicability of a regulation in accordance with article  tfeu will reduce legal fragmentation and facilitate the development of a single market for lawful safe and trustworthy ai systems,5,8
2890,2890,it will do so in particular by introducing a harmonised set of core requirements with regard to ai systems classified as high risk and obligations for providers and users of those systems improving the protection of fundamental rights and providing legal certainty for operators and consumers alike,5,5
2891,2891,at the same time the provisions of the regulation are not overly prescriptive and leave room for different levels of member state action for elements that do not undermine the objectives of the initiative in particular the internal organisation of the market surveillance system and the uptake of measures to foster innovation,5,1
2892,2892,this proposal is the result of extensive consultation with all major stakeholders in which the general principles and minimum standards for consultation of interested parties by the commission were applied,5,27
2893,2893,an online public consultation was launched on  february  along with the publication of the white paper on artificial intelligence and ran until  june,5,2
2894,2894,the objective of that consultation was to collect views and opinions on the white paper,5,19
2895,2895,it targeted all interested stakeholders from the public and private sectors including governments local authorities commercial and non commercial organisations social partners experts academics and citizens,5,17
2896,2896,after analysing all the responses received the commission published a summary outcome and the individual responses on its website,5,19
2897,2897,in total  contributions were received of which  were from companies or business organisationsassociations  from individuals  individuals from eu   on behalf of academicresearch institutions and  from public authorities,5,24
2898,2898,civil societys voices were represented by  respondents among which  consumers organisations  nongovernmental organisations and  trade unions  respondents contributed as others,5,6
2899,2899,of the  business and industry representatives  were companies and business representatives  of which were micro small and medium sized enterprises,5,0
2900,2900,the rest were business associations,5,0
2901,2901,overall  of business and industry replies came from the eu,5,2
2902,2902,depending on the question between  and  of the respondents used the free text option to insert comments,5,14
2903,2903,over  position papers were submitted through the eu survey website either in addition to questionnaire answers over  or as stand alone contributions over,5,2
2904,2904,overall there is a general agreement amongst stakeholders on a need for action,5,27
2905,2905,a large majority of stakeholders agree that legislative gaps exist or that new legislation is needed,5,26
2906,2906,however several stakeholders warn the commission to avoid duplication conflicting obligations and overregulation,5,19
2907,2907,there were many comments underlining the importance of a technology neutral and proportionate regulatory framework,5,8
2908,2908,stakeholders mostly requested a narrow clear and precise definition for ai,5,14
2909,2909,stakeholders also highlighted that besides the clarification of the term of ai it is important to define risk high risk low risk remote biometric identification and harm,5,12
2910,2910,most of the respondents are explicitly in favour of the risk based approach,5,3
2911,2911,using a risk based framework was considered a better option than blanket regulation of all ai systems,5,3
2912,2912,the types of risks and threats should be based on a sector by sector and case by case approach,5,3
2913,2913,risks also should be calculated taking into account the impact on rights and safety,5,3
2914,2914,regulatory sandboxes could be very useful for the promotion of ai and are welcomed by certain stakeholders especially the business associations,5,8
2915,2915,among those who formulated their opinion on the enforcement models more than  especially from the business associations were in favour of a combination of an ex ante risk self assessment and an ex post enforcement for high risk ai systems,5,0
2916,2916,the proposal builds on two years of analysis and close involvement of stakeholders including academics businesses social partners non governmental organisations member states and citizens,5,2
2917,2917,the preparatory work started in  with the setting up of a high level expert group on ai hleg which had an inclusive and broad composition of  well known experts tasked to advise the commission on the implementation of the commissions strategy on artificial intelligence,5,19
2918,2918,in april  the commission supported the key requirements set out in the hleg ethics guidelines for trustworthy ai which had been revised to take into account more than  submissions from stakeholders,5,2
2919,2919,the key requirements reflect a widespread and common approach as evidenced by a plethora of ethical codes and principles developed by many private and public organisations in europe and beyond that ai development and use should be guided by certain essential value oriented principles,5,16
2920,2920,the assessment list for trustworthy artificial intelligence altai made those requirements operational in a piloting process with over  organisations,5,18
2921,2921,in addition the ai alliance was formed as a platform for approximately  stakeholders to debate the technological and societal implications of ai culminating in a yearly ai assembly,5,4
2922,2922,the white paper on ai further developed this inclusive approach inciting comments from more than  stakeholders including over  additional position papers,5,14
2923,2923,as a result the commission published an inception impact assessment which in turn attracted more than  comments,5,19
2924,2924,additional stakeholder workshops and events were also organised the results of which support the analysis in the impact assessment and the policy choices made in this proposal,5,17
2925,2925,an external study was also procured to feed into the impact assessment,5,17
2926,2926,in line with its better regulation policy the commission conducted an impact assessment for this proposal examined by the commissions regulatory scrutiny board,5,19
2927,2927,a meeting with the regulatory scrutiny board was held on  december  which was followed by a negative opinion,5,0
2928,2928,after substantial revision of the impact assessment to address the comments and a resubmission of the impact assessment the regulatory scrutiny board issued a positive opinion on  march,5,17
2929,2929,the opinions of the regulatory scrutiny board the recommendations and an explanation of how they have been taken into account are presented in annex  of the impact assessment,5,17
2930,2930,the commission examined different policy options to achieve the general objective of the proposal which is to ensure the proper functioning of the single market by creating the conditions for the development and use of trustworthy ai in the union,5,19
2931,2931,four policy options of different degrees of regulatory intervention were assessedoption  eu legislative instrument setting up a voluntary labelling schemeoption  a sectoral ad hoc approachoption  horizontal eu legislative instrument following a proportionate riskbased approachoption  horizontal eu legislative instrument following a proportionate riskbased approach  codes of conduct for non high risk ai systemsoption  horizontal eu legislative instrument establishing mandatory requirements for all ai systems irrespective of the risk they pose,5,8
2932,2932,according to the commissions established methodology each policy option was evaluated against economic and societal impacts with a particular focus on impacts on fundamental rights,5,17
2933,2933,the preferred option is option  a regulatory framework for high risk ai systems only with the possibility for all providers of non high risk ai systems to follow a code of conduct,5,21
2934,2934,the requirements will concern data documentation and traceability provision of information and transparency human oversight and robustness and accuracy and would be mandatory for high risk ai systems,5,5
2935,2935,companies that introduced codes of conduct for other ai systems would do so voluntarily,5,25
2936,2936,the preferred option was considered suitable to address in the most effective way the objectives of this proposal,5,21
2937,2937,by requiring a restricted yet effective set of actions from ai developers and users the preferred option limits the risks of violation of fundamental rights and safety of people and foster effective supervision and enforcement by targeting the requirements only to systems where there is a high risk that such violations could occur,5,21
2938,2938,as a result that option keeps compliance costs to a minimum thus avoiding an unnecessary slowing of uptake due to higher prices and compliance costs,5,22
2939,2939,in order to address possible disadvantages for smes this option includes several provisions to support their compliance and reduce their costs including creation of regulatory sandboxes and obligation to consider smes interests when setting fees related to conformity assessment,5,0
2940,2940,the preferred option will increase peoples trust in ai companies will gain in legal certainty and member states will see no reason to take unilateral action that could fragment the single market,5,0
2941,2941,as a result of higher demand due to higher trust more available offers due to legal certainty and the absence of obstacles to cross border movement of ai systems the single market for ai will likely flourish,5,0
2942,2942,the european union will continue to develop a fastgrowing ai ecosystem of innovative services and products embedding ai technology or stand alone ai systems resulting in increased digital autonomy,5,2
2943,2943,businesses or public authorities that develop or use ai applications that constitute a high risk for the safety or fundamental rights of citizens would have to comply with specific requirements and obligations,5,5
2944,2944,compliance with these requirements would imply costs amounting to approximately eur   to eur   for the supply of an average highrisk ai system of around eur   by,5,5
2945,2945,for ai users there would also be the annual cost for the time spent on ensuring human oversight where this is appropriate depending on the use case,5,4
2946,2946,those have been estimated at approximately eur   to eur   per year,5,2
2947,2947,verification costs could amount to another eur   to eur   for suppliers of high risk ai,5,10
2948,2948,businesses or public authorities that develop or use any ai applications not classified as high risk would only have minimal obligations of information,5,9
2949,2949,however they could choose to join others and together adopt a code of conduct to follow suitable requirements and to ensure that their ai systems are trustworthy,5,1
2950,2950,in such a case costs would be at most as high as for high risk ai systems but most probably lower,5,25
2951,2951,the impacts of the policy options on different categories of stakeholders economic operators business conformity assessment bodies standardisation bodies and other public bodies individualscitizens researchers are explained in detail in annex  of the impact assessment supporting this proposal,5,17
2952,2952,this proposal lays down obligation that will apply to providers and users of high risk ai systems,5,5
2953,2953,for providers who develop and place such systems on the union market it will create legal certainty and ensure that no obstacle to the cross border provision of ai related services and products emerge,5,1
2954,2954,for companies using ai it will promote trust among their customers,5,0
2955,2955,for national public administrations it will promote public trust in the use of ai and strengthen enforcement mechanisms by introducing a european coordination mechanism providing for appropriate capacities and facilitating audits of the ai systems with new requirements for documentation traceability and transparency,5,1
2956,2956,moreover the framework will envisage specific measures supporting innovation including regulatory sandboxes and specific measures supporting small scale users and providers of high risk ai systems to comply with the new rules,5,8
2957,2957,the proposal also specifically aims at strengthening europes competitiveness and industrial basis in ai,5,2
2958,2958,full consistency is ensured with existing sectoral union legislation applicable to ai systems eg,5,1
2959,2959,on products and services that will bring further clarity and simplify the enforcement of the new rules,5,8
2960,2960,the use of ai with its specific characteristics eg,5,4
2961,2961,opacity complexity dependency on data autonomous behaviour can adversely affect a number of fundamental rights enshrined in the eu charter of fundamental rights the charter,5,11
2962,2962,this proposal seeks to ensure a high level of protection for those fundamental rights and aims to address various sources of risks through a clearly defined risk based approach,5,12
2963,2963,with a set of requirements for trustworthy ai and proportionate obligations on all value chain participants the proposal will enhance and promote the protection of the rights protected by the charter the right to human dignity article  respect for private life and protection of personal data articles  and  nondiscrimination article  and equality between women and men article,5,27
2964,2964,it aims to prevent a chilling effect on the rights to freedom of expression article  and freedom of assembly article  to ensure protection of the right to an effective remedy and to a fair trial the rights of defence and the presumption of innocence articles  and  as well as the general principle of good administration,5,12
2965,2965,furthermore as applicable in certain domains the proposal will positively affect the rights of a number of special groups such as the workers rights to fair and just working conditions article  a high level of consumer protection article  the rights of the child article  and the integration of persons with disabilities article,5,27
2966,2966,the right to a high level of environmental protection and the improvement of the quality of the environment article  is also relevant including in relation to the health and safety of people,5,16
2967,2967,the obligations for ex ante testing risk management and human oversight will also facilitate the respect of other fundamental rights by minimising the risk of erroneous or biased ai assisted decisions in critical areas such as education and training employment important services law enforcement and the judiciary,5,16
2968,2968,in case infringements of fundamental rights still happen effective redress for affected persons will be made possible by ensuring transparency and traceability of the ai systems coupled with strong ex post controls,5,10
2969,2969,this proposal imposes some restrictions on the freedom to conduct business article  and the freedom of art and science article  to ensure compliance with overriding reasons of public interest such as health safety consumer protection and the protection of other fundamental rights responsible innovation when high risk ai technology is developed and used,5,2
2970,2970,those restrictions are proportionate and limited to the minimum necessary to prevent and mitigate serious safety risks and likely infringements of fundamental rights,5,1
2971,2971,the increased transparency obligations will also not disproportionately affect the right to protection of intellectual property article  since they will be limited only to the minimum necessary information for individuals to exercise their right to an effective remedy and to the necessary transparency towards supervision and enforcement authorities in line with their mandates,5,0
2972,2972,any disclosure of information will be carried out in compliance with relevant legislation in the field including directive  on the protection of undisclosed know how and business information trade secrets against their unlawful acquisition use and disclosure,5,1
2973,2973,when public authorities and notified bodies need to be given access to confidential information or source code to examine compliance with substantial obligations they are placed under binding confidentiality obligations,5,1
2974,2974,member states will have to designate supervisory authorities in charge of implementing the legislative requirements,5,1
2975,2975,their supervisory function could build on existing arrangements for example regarding conformity assessment bodies or market surveillance but would require sufficient technological expertise and human and financial resources,5,8
2976,2976,depending on the preexisting structure in each member state this could amount to  to  full time equivalents per member state,5,1
2977,2977,a detailed overview of the costs involved is provided in the financial statement linked to this proposal,5,27
2978,2978,providing for a robust monitoring and evaluation mechanism is crucial to ensure that the proposal will be effective in achieving its specific objectives,5,17
2979,2979,the commission will be in charge of monitoring the effects of the proposal,5,19
2980,2980,it will establish a system for registering stand alone high risk ai applications in a public eu wide database,5,5
2981,2981,this registration will also enable competent authorities users and other interested people to verify if the high risk ai system complies with the requirements laid down in the proposal and to exercise enhanced oversight over those ai systems posing high risks to fundamental rights,5,5
2982,2982,to feed this database ai providers will be obliged to provide meaningful information about their systems and the conformity assessment carried out on those systems,5,5
2983,2983,moreover ai providers will be obliged to inform national competent authorities about serious incidents or malfunctioning that constitute a breach of fundamental rights obligations as soon as they become aware of them as well as any recalls or withdrawals of ai systems from the market,5,10
2984,2984,national competent authorities will then investigate the incidentsor malfunctioning collect all the necessary information and regularly transmit it to the commission with adequate metadata,5,10
2985,2985,the commission will complement this information on the incidents by a comprehensive analysis of the overall market for ai,5,19
2986,2986,the commission will publish a report evaluating and reviewing the proposed ai framework five years following the date on which it becomes applicable,5,2
2987,2987,title i defines the subject matter of the regulation and the scope of application of the new rules that cover the placing on the market putting into service and use of ai systems,5,8
2988,2988,it also sets out the definitions used throughout the instrument,5,26
2989,2989,the definition of ai system in the legal framework aims to be as technology neutral and future proof as possible taking into account the fast technological and market developments related to ai,5,5
2990,2990,in order to provide the needed legal certainty title i is complemented by annex i which contains a detailed list of approaches and techniques for the development of ai to be adapted by the commission in line with new technological developments,5,5
2991,2991,key participants across the ai value chain are also clearly defined such as providers and users of ai systems that cover both public and private operators to ensure a level playing field,5,4
2992,2992,title ii establishes a list of prohibited ai,5,5
2993,2993,the regulation follows a risk based approach differentiating between uses of ai that create  an unacceptable risk ii a high risk and iii low or minimal risk,5,3
2994,2994,the list of prohibited practices in title ii comprises all those ai systems whose use is considered unacceptable as contravening union values for instance by violating fundamental rights,5,1
2995,2995,the prohibitions covers practices that have a significant potential to manipulate persons through subliminal techniques beyond their consciousness or exploit vulnerabilities of specific vulnerable groups such as children or persons with disabilities in order to materially distort their behaviour in a manner that is likely to cause them or another person psychological or physical harm,5,12
2996,2996,other manipulative or exploitative practices affecting adults that might be facilitated by ai systems could be covered by the existing data protection consumer protection and digital service legislation that guarantee that natural persons are properly informed and have free choice not to be subject to profiling or other practices that might affect their behaviour,5,12
2997,2997,the proposal also prohibits ai based social scoring for general purposes done by public authorities,5,17
2998,2998,finally the use of real time remote biometric identification systems in publicly accessible spaces for the purpose of law enforcement is also prohibited unless certain limited exceptions apply,5,12
2999,2999,title iii contains specific rules for ai systems that create a high risk to the health and safety or fundamental rights of natural persons,5,5
3000,3000,in line with a risk based approach those high risk ai systems are permitted on the european market subject to compliance with certain mandatory requirements and an ex ante conformity assessment,5,5
3001,3001,the classification of an ai system as high risk is based on the intended purpose of the ai system in line with existing product safety legislation,5,5
3002,3002,therefore the classification as high risk does not only depend on the function performed by the ai system but also on the specific purpose and modalities for which that system is used,5,5
3003,3003,chapter  of title iii sets the classification rules and identifies two main categories of highrisk ai systemsai systems intended to be used as safety component of products that are subject to third party ex ante conformity assessmentother stand alone ai systems with mainly fundamental rights implications that are explicitly listed in annex iii,5,5
3004,3004,this list of high risk ai systems in annex iii contains a limited number of ai systems whose risks have already materialised or are likely to materialise in the near future,5,5
3005,3005,to ensure that the regulation can be adjusted to emerging uses and applications of ai the commission may expand the list of high risk ai systems used within certain pre defined areas by applying a set of criteria and risk assessment methodology,5,5
3006,3006,chapter  sets out the legal requirements for high risk ai systems in relation to data and data governance documentation and recording keeping transparency and provision of information to users human oversight robustness accuracy and security,5,8
3007,3007,the proposed minimum requirements are already state of the art for many diligent operators and the result of two years of preparatory work derived from the ethics guidelines of the hleg piloted by more than  organisations,5,27
3008,3008,they are also largely consistent with other international recommendations and principles which ensures that the proposed ai framework is compatible with those adopted by the eus international trade partners,5,2
3009,3009,the precise technical solutions to achieve compliance with those requirements may be provided by standards or by other technical specifications or otherwise be developed in accordance with general engineering or scientific knowledge at the discretion of the provider of the ai system,5,5
3010,3010,this flexibility is particularly important because it allows providers of ai systems to choose the way to meet their requirements taking into account the state of the art and technological and scientific progress in this field,5,4
3011,3011,chapter  places a clear set of horizontal obligations on providers of high risk ai systems,5,5
3012,3012,proportionate obligations are also placed on users and other participants across the ai value chain eg importers distributors authorized representatives,5,0
3013,3013,chapter  sets the framework for notified bodies to be involved as independent third parties in conformity assessment procedures while chapter  explains in detail the conformity assessment procedures to be followed for each type of high risk ai system,5,18
3014,3014,the conformity assessment approach aims to minimise the burden for economic operators as well as for notified bodies whose capacity needs to be progressively ramped up over time,5,18
3015,3015,ai systems intended to be used as safety components of products that are regulated under the new legislative framework legislation eg,5,5
3016,3016,machinery toys medical devices etc,5,24
3017,3017,will be subject to the same ex ante and ex post compliance and enforcement mechanisms of the products of which they are a component,5,1
3018,3018,the key difference is that the ex ante and ex post mechanisms will ensure compliance not only with the requirements established by sectorial legislation but also with the requirements established by this regulation,5,1
3019,3019,as regards stand alone high risk ai systems that are referred to in annex iii a new compliance and enforcement system will be established,5,5
3020,3020,this follows the model of the new legislative framework legislation implemented through internal control checks by the providers with the exception of remote biometric identification systems that would be subject to third party conformity assessment,5,12
3021,3021,a comprehensive ex ante conformity assessment through internal checks combined with a strong ex post enforcement could be an effective and reasonable solution for those systems given the early phase of the regulatory intervention and the fact the ai sector is very innovative and expertise for auditing is only now being accumulated,5,0
3022,3022,an assessment through internal checks for stand alone high risk ai systems would require a full effective and properly documented ex ante compliance with all requirements of the regulation and compliance with robust quality and risk management systems and post market monitoring,5,5
3023,3023,after the provider has performed the relevant conformity assessment it should register those stand alone high risk ai systems in an eu database that will be managed by the commission to increase public transparency and oversight and strengthen ex post supervision by competent authorities,5,5
3024,3024,by contrast for reasons of consistency with the existing product safety legislation the conformity assessments of ai systems that are safety components of products will follow a system with third party conformity assessment procedures already established under the relevant sectoral product safety legislation,5,18
3025,3025,new ex ante re assessments of the conformity will be needed in case of substantial modifications to the ai systems and notably changes which go beyond what is pre determined by the provider in its technical documentation and checked at the moment of the ex ante conformity assessment,5,18
3026,3026,title iv concerns certain ai systems to take account of the specific risks of manipulation they pose,5,5
3027,3027,transparency obligations will apply for systems that  interact with humans ii are used to detect emotions or determine association with social categories based on biometric data or iii generate or manipulate content deep fakes,5,12
3028,3028,when persons interact with an ai system or their emotions or characteristics are recognised through automated means people must be informed of that circumstance,5,4
3029,3029,if an ai system is used to generate or manipulate image audio or video content that appreciably resembles authentic content there should be an obligation to disclose that the content is generated through automated means subject to exceptions for legitimate purposes law enforcement freedom of expression,5,12
3030,3030,this allows persons to make informed choices or step back from a given situation,5,21
3031,3031,title v contributes to the objective to create a legal framework that is innovation friendly future proof and resilient to disruption,5,10
3032,3032,to that end it encourages national competent authorities to set up regulatory sandboxes and sets a basic framework in terms of governance supervision and liability,5,8
3033,3033,ai regulatory sandboxes establish a controlled environment to test innovative technologies for a limited time on the basis of a testing plan agreed with the competent authorities,5,8
3034,3034,title v also contains measures to reduce the regulatory burden on smes and start ups,5,0
3035,3035,title vi sets up the governance systems at union and national level,5,26
3036,3036,at union level the proposal establishes a european artificial intelligence board the board composed of representatives from the member states and the commission,5,2
3037,3037,the board will facilitate a smooth effective and harmonised implementation of this regulation by contributing to the effective cooperation of the national supervisory authorities and the commission and providing advice and expertise to the commission,5,19
3038,3038,it will also collect and share best practices among the member states,5,1
3039,3039,at national level member states will have to designate one or more national competent authorities and among them the national supervisory authority for the purpose of supervising the application and implementation of the regulation,5,1
3040,3040,the european data protection supervisor will act as the competent authority for the supervision of the union institutions agencies and bodies when they fall within the scope of this regulation,5,2
3041,3041,title vii aims to facilitate the monitoring work of the commission and national authorities through the establishment of an eu wide database for stand alone high risk ai systems with mainly fundamental rights implications,5,2
3042,3042,the database will be operated by the commission and provided with data by the providers of the ai systems who will be required to register their systems before placing them on the market or otherwise putting them into service,5,20
3043,3043,title viii sets out the monitoring and reporting obligations for providers of ai systems with regard to post market monitoring and reporting and investigating on ai related incidents and malfunctioning,5,5
3044,3044,market surveillance authorities would also control the market and investigate compliance with the obligations and requirements for all high risk ai systems already placed on the market,5,8
3045,3045,market surveillance authorities would have all powers under regulation eu  on market surveillance,5,12
3046,3046,ex post enforcement should ensure that once the ai system has been put on the market public authorities have the powers and resources to intervene in case ai systems generate unexpected risks which warrant rapid action,5,5
3047,3047,they will also monitor compliance of operators with their relevant obligations under the regulation,5,1
3048,3048,the proposal does not foresee the automatic creation of any additional bodies or authorities at member state level,5,26
3049,3049,member states may therefore appoint and draw upon the expertise of existing sectorial authorities who would be entrusted also with the powers to monitor and enforce the provisions of the regulation,5,1
3050,3050,all this is without prejudice to the existing system and allocation of powers of ex post enforcement of obligations regarding fundamental rights in the member states,5,26
3051,3051,when necessary for their mandate existing supervision and enforcement authorities will also have the power to request and access any documentation maintained following this regulation and where needed request market surveillance authorities to organise testing of the high risk ai system through technical means,5,8
3052,3052,title ix creates a framework for the creation of codes of conduct which aim to encourage providers of non high risk ai systems to apply voluntarily the mandatory requirements for high risk ai systems as laid out in title iii,5,5
3053,3053,providers of non high risk ai systems may create and implement the codes of conduct themselves,5,5
3054,3054,those codes may also include voluntary commitments related for example to environmental sustainability accessibility for persons with disability stakeholders participation in the design and development of ai systems and diversity of development teams,5,27
3055,3055,title x emphasizes the obligation of all parties to respect the confidentiality of information and data and sets out rules for the exchange of information obtained during the implementation of the regulation,5,11
3056,3056,title x also includes measures to ensure the effective implementation of the regulation through effective proportionate and dissuasive penalties for infringements of the provisions,5,1
3057,3057,title xi sets out rules for the exercise of delegation and implementing powers,5,26
3058,3058,the proposal empowers the commission to adopt where appropriate implementing acts to ensure uniform application of the regulation or delegated acts to update or complement the lists in annexes i to vii,5,19
3059,3059,title xii contains an obligation for the commission to assess regularly the need for an update of annex iii and to prepare regular reports on the evaluation and review of the regulation,5,19
3060,3060,it also lays down final provisions including a differentiated transitional period for the initial date of the applicability of the regulation to facilitate the smooth implementation for all parties concerned,5,22
3061,3061,the purpose of this regulation is to improve the functioning of the internal market by laying down a uniform legal framework in particular for the development marketing and use of artificial intelligence in conformity with union values,5,0
3062,3062,this regulation pursues a number of overriding reasons of public interest such as a high level of protection of health safety and fundamental rights and it ensures the free movement of ai based goods and services cross border thus preventing member states from imposing restrictions on the development marketing and use of ai systems unless explicitly authorised by this regulation,5,1
3063,3063,artificial intelligence systems ai systems can be easily deployed in multiple sectors of the economy and society including cross border and circulate throughout the union,5,4
3064,3064,certain member states have already explored the adoption of national rules to ensure that artificial intelligence is safe and is developed and used in compliance with fundamental rights obligations,5,8
3065,3065,differing national rules may lead to fragmentation of the internal market and decrease legal certainty for operators that develop or use ai systems,5,8
3066,3066,a consistent and high level of protection throughout the union should therefore be ensured while divergences hampering the free circulation of ai systems and related products and services within the internal market should be prevented by laying down uniform obligations for operators and guaranteeing the uniform protection of overriding reasons of public interest and of rights of persons throughout the internal market based on article  of the treaty on the functioning of the european union tfeu,5,2
3067,3067,to the extent that this regulation contains specific rules on the protection of individuals with regard to the processing of personal data concerning restrictions of the use of ai systems for real time remote biometric identification in publicly accessible spaces for the purpose of law enforcement it is appropriate to base this regulation in as far as those specific rules are concerned on article  of the tfeu,5,12
3068,3068,in light of those specific rules and the recourse to article  tfeu it is appropriate to consult the european data protection board,5,2
3069,3069,artificial intelligence is a fast evolving family of technologies that can contribute to a wide array of economic and societal benefits across the entire spectrum of industries and social activities,5,4
3070,3070,by improving prediction optimising operations and resource allocation and personalising digital solutions available for individuals and organisations the use of artificial intelligence can provide key competitive advantages to companies and support socially and environmentally beneficial outcomes for example in healthcare farming education and training infrastructure management energy transport and logistics public services security justice resource and energy efficiency and climate change mitigation and adaptation,5,13
3071,3071,at the same time depending on the circumstances regarding its specific application and use artificial intelligence may generate risks and cause harm to public interests and rights that are protected by union law,5,0
3072,3072,such harm might be material or immaterial,5,25
3073,3073,a union legal framework laying down harmonised rules on artificial intelligence is therefore needed to foster the development use and uptake of artificial intelligence in the internal market that at the same time meets a high level of protection of public interests such as health and safety and the protection of fundamental rights as recognised and protected by union law,5,2
3074,3074,to achieve that objective rules regulating the placing on the market and putting into service of certain ai systems should be laid down thus ensuring the smooth functioning of the internal market and allowing those systems to benefit from the principle of free movement of goods and services,5,0
3075,3075,by laying down those rules this regulation supports the objective of the union of being a global leader in the development of secure trustworthy and ethical artificial intelligence as stated by the european council and it ensures the protection of ethical principles as specifically requested by the european parliament,5,2
3076,3076,the notion of ai system should be clearly defined to ensure legal certainty while providing the flexibility to accommodate future technological developments,5,5
3077,3077,the definition should be based on the key functional characteristics of the software in particular the ability for a given set of human defined objectives to generate outputs such as content predictions recommendations or decisions which influence the environment with which the system interacts be it in a physical or digital dimension,5,26
3078,3078,ai systems can be designed to operate with varying levels of autonomy and be used on a stand alone basis or as a component of a product irrespective of whether the system is physically integrated into the product embedded or serve the functionality of the product without being integrated therein non embedded,5,4
3079,3079,the definition of ai system should be complemented by a list of specific techniques and approaches used for its development which should be kept up todate in the light of market and technological developments through the adoption of delegated acts by the commission to amend that list,5,5
3080,3080,the notion of biometric data used in this regulation is in line with and should be interpreted consistently with the notion of biometric data as defined in article  of regulation eu  of the european parliament and of the council article  of regulation eu  of the european parliament and of the council and article  of directive eu  of the european parliament and of the council,5,2
3081,3081,the notion of remote biometric identification system as used in this regulation should be defined functionally as an ai system intended for the identification of natural persons at a distance through the comparison of a persons biometric data with the biometric data contained in a reference database and without prior knowledge whether the targeted person will be present and can be identified irrespectively of the particular technology processes or types of biometric data used,5,12
3082,3082,considering their different characteristics and manners in which they are used as well as the different risks involved a distinction should be made between real time and post remote biometric identification systems,5,12
3083,3083,in the case of real time systems the capturing of the biometric data the comparison and the identification occur all instantaneously near instantaneously or in any event without a significant delay,5,12
3084,3084,in this regard there should be no scope for circumventing the rules of this regulation on the real time use of the ai systems in question by providing for minor delays,5,1
3085,3085,real time systems involve the use of live or near live material such as video footage generated by a camera or other device with similar functionality,5,15
3086,3086,in the case of post systems in contrast the biometric data have already been captured and the comparison and identification occur only after a significant delay,5,12
3087,3087,this involves material such as pictures or video footage generated by closed circuit television cameras or private devices which has been generated before the use of the system in respect of the natural persons concerned,5,12
3088,3088,for the purposes of this regulation the notion of publicly accessible space should be understood as referring to any physical place that is accessible to the public irrespective of whether the place in question is privately or publicly owned,5,12
3089,3089,therefore the notion does not cover places that are private in nature and normally not freely accessible for third parties including law enforcement authorities unless those parties have been specifically invited or authorised such as homes private clubs offices warehouses and factories,5,12
3090,3090,online spaces are not covered either as they are not physical spaces,5,7
3091,3091,however the mere fact that certain conditions for accessing a particular space may apply such as admission tickets or age restrictions does not mean that the space is not publicly accessible within the meaning of this regulation,5,1
3092,3092,consequently in addition to public spaces such as streets relevant parts of government buildings and most transport infrastructure spaces such as cinemas theatres shops and shopping centres are normally also publicly accessible,5,12
3093,3093,whether a given space is accessible to the public should however be determined on a case bycase basis having regard to the specificities of the individual situation at hand,5,12
3094,3094,in order to ensure a level playing field and an effective protection of rights and freedoms of individuals across the union the rules established by this regulation should apply to providers of ai systems in a non discriminatory manner irrespective of whether they are established within the union or in a third country and to users of ai systems established within the union,5,6
3095,3095,in light of their digital nature certain ai systems should fall within the scope of this regulation even when they are neither placed on the market nor put into service nor used in the union,5,25
3096,3096,this is the case for example of an operator established in the union that contracts certain services to an operator established outside the union in relation to an activity to be performed by an ai system that would qualify as high risk and whose effects impact natural persons located in the union,5,1
3097,3097,in those circumstances the ai system used by the operator outside the union could process data lawfully collected in and transferred from the union and provide to the contracting operator in the union the output of that ai system resulting from that processing without that ai system being placed on the market put into service or used in the union,5,1
3098,3098,to prevent the circumvention of this regulation and to ensure an effective protection of natural persons located in the union this regulation should also apply to providers and users of ai systems that are established in a third country to the extent the output produced by those systems is used in the union,5,1
3099,3099,nonetheless to take into account existing arrangements and special needs for cooperation with foreign partners with whom information and evidence is exchanged this regulation should not apply to public authorities of a third country and international organisations when acting in the framework of international agreements concluded at national or european level for law enforcement and judicial cooperation with the union or with its member states,5,1
3100,3100,such agreements have been concluded bilaterally between member states and third countries or between the european union europol and other eu agencies and third countries and international organisations,5,2
3101,3101,this regulation should also apply to union institutions offices bodies and agencies when acting as a provider or user of an ai system,5,1
3102,3102,ai systems exclusively developed or used for military purposes should be excluded from the scope of this regulation where that use falls under the exclusive remit of the common foreign and security policy regulated under title v of the treaty on the european union teu,5,2
3103,3103,this regulation should be without prejudice to the provisions regarding the liability of intermediary service providers set out in directive ec of the european parliament and of the council as amended by the digital services act,5,2
3104,3104,in order to ensure a consistent and high level of protection of public interests as regards health safety and fundamental rights common normative standards for all high risk ai systems should be established,5,5
3105,3105,those standards should be consistent with the charter of fundamental rights of the european union the charter and should be non discriminatory and in line with the unions international trade commitments,5,2
3106,3106,in order to introduce a proportionate and effective set of binding rules for ai systems a clearly defined risk based approach should be followed,5,5
3107,3107,that approach should tailor the type and content of such rules to the intensity and scope of the risks that ai systems can generate,5,21
3108,3108,it is therefore necessary to prohibit certain artificial intelligence practices to lay down requirements for high risk ai systems and obligations for the relevant operators and to lay down transparency obligations for certain ai systems,5,5
3109,3109,aside from the many beneficial uses of artificial intelligence that technology can also be misused and provide novel and powerful tools for manipulative exploitative and social control practices,5,4
3110,3110,such practices are particularly harmful and should be prohibited because they contradict union values of respect for human dignity freedom equality democracy and the rule of law and union fundamental rights including the right to non discrimination data protection and privacy and the rights of the child,5,6
3111,3111,the placing on the market putting into service or use of certain ai systems intended to distort human behaviour whereby physical or psychological harms are likely to occur should be forbidden,5,5
3112,3112,such ai systems deploy subliminal components individuals cannot perceive or exploit vulnerabilities of children and people due to their age physical or mental incapacities,5,12
3113,3113,they do so with the intention to materially distort the behaviour of a person and in a manner that causes or is likely to cause harm to that or another person,5,12
3114,3114,the intention may not be presumed if the distortion of human behaviour results from factors external to the ai system which are outside of the control of the provider or the user,5,9
3115,3115,research for legitimate purposes in relation to such ai systems should not be stifled by the prohibition if such research does not amount to use of the ai system in human machine relations that exposes natural persons to harm and such research is carried out in accordance with recognised ethical standards for scientific research,5,16
3116,3116,ai systems providing social scoring of natural persons for general purpose by public authorities or on their behalf may lead to discriminatory outcomes and the exclusion of certain groups,5,6
3117,3117,they may violate the right to dignity and non discrimination and the values of equality and justice,5,6
3118,3118,such ai systems evaluate or classify the trustworthiness of natural persons based on their social behaviour in multiple contexts or known or predicted personal or personality characteristics,5,18
3119,3119,the social score obtained from such ai systems may lead to the detrimental or unfavourable treatment of natural persons or whole groups thereof in social contexts which are unrelated to the context in which the data was originally generated or collected or to a detrimental treatment that is disproportionate or unjustified to the gravity of their social behaviour,5,6
3120,3120,such ai systems should be therefore prohibited,5,5
3121,3121,the use of ai systems for real time remote biometric identification of natural persons in publicly accessible spaces for the purpose of law enforcement is considered particularly intrusive in the rights and freedoms of the concerned persons to the extent that it may affect the private life of a large part of the population evoke a feeling of constant surveillance and indirectly dissuade the exercise of the freedom of assembly and other fundamental rights,5,12
3122,3122,in addition the immediacy of the impact and the limited opportunities for further checks or corrections in relation to the use of such systems operating in real time carry heightened risks for the rights and freedoms of the persons that are concerned by law enforcement activities,5,12
3123,3123,the use of those systems for the purpose of law enforcement should therefore be prohibited except in three exhaustively listed and narrowly defined situations where the use is strictly necessary to achieve a substantial public interest the importance of which outweighs the risks,5,12
3124,3124,those situations involve the search for potential victims of crime including missing children certain threats to the life or physical safety of natural persons or of a terrorist attack and the detection localisation identification or prosecution of perpetrators or suspects of the criminal offences referred to in council framework decision jha if those criminal offences are punishable in the member state concerned by a custodial sentence or a detention order for a maximum period of at least three years and as they are defined in the law of that member state,5,12
3125,3125,such threshold for the custodial sentence or detention order in accordance with national law contributes to ensure that the offence should be serious enough to potentially justify the use of real time remote biometric identification systems,5,12
3126,3126,moreover of the  criminal offences listed in the council framework decision jha some are in practice likely to be more relevant than others in that the recourse to real time remote biometric identification will foreseeably be necessary and proportionate to highly varying degrees for the practical pursuit of the detection localisation identification or prosecution of a perpetrator or suspect of the different criminal offences listed and having regard to the likely differences in the seriousness probability and scale of the harm or possible negative consequences,5,12
3127,3127,in order to ensure that those systems are used in a responsible and proportionate manner it is also important to establish that in each of those three exhaustively listed and narrowly defined situations certain elements should be taken into account in particular as regards the nature of the situation giving rise to the request and the consequences of the use for the rights and freedoms of all persons concerned and the safeguards and conditions provided for with the use,5,15
3128,3128,in addition the use of real time remote biometric identification systems in publicly accessible spaces for the purpose of law enforcement should be subject to appropriate limits in time and space having regard in particular to the evidence or indications regarding the threats the victims or perpetrator,5,12
3129,3129,the reference database of persons should be appropriate for each use case in each of the three situations mentioned above,5,24
3130,3130,each use of a real time remote biometric identification system in publicly accessible spaces for the purpose of law enforcement should be subject to an express and specific authorisation by a judicial authority or by an independent administrative authority of a member state,5,12
3131,3131,such authorisation should in principle be obtained prior to the use except in duly justified situations of urgency that is situations where the need to use the systems in question is such as to make it effectively and objectively impossible to obtain an authorisation before commencing the use,5,15
3132,3132,in such situations of urgency the use should be restricted to the absolute minimum necessary and be subject to appropriate safeguards and conditions as determined in national law and specified in the context of each individual urgent use case by the law enforcement authority itself,5,22
3133,3133,in addition the law enforcement authority should in such situations seek to obtain an authorisation as soon as possible whilst providing the reasons for not having been able to request it earlier,5,22
3134,3134,furthermore it is appropriate to provide within the exhaustive framework set by this regulation that such use in the territory of a member state in accordance with this regulation should only be possible where and in as far as the member state in question has decided to expressly provide for the possibility to authorise such use in its detailed rules of national law,5,1
3135,3135,consequently member states remain free under this regulation not to provide for such a possibility at all or to only provide for such a possibility in respect of some of the objectives capable of justifying authorised use identified in this regulation,5,1
3136,3136,the use of ai systems for real time remote biometric identification of natural persons in publicly accessible spaces for the purpose of law enforcement necessarily involves the processing of biometric data,5,12
3137,3137,the rules of this regulation that prohibit subject to certain exceptions such use which are based on article  tfeu should apply as lex specialis in respect of the rules on the processing of biometric data contained in article  of directive eu  thus regulating such use and the processing of biometric data involved in an exhaustive manner,5,12
3138,3138,therefore such use and processing should only be possible in as far as it is compatible with the framework set by this regulation without there being scope outside that framework for the competent authorities where they act for purpose of law enforcement to use such systems and process such data in connection thereto on the grounds listed in article  of directive eu,5,8
3139,3139,in this context this regulation is not intended to provide the legal basis for the processing of personal data under article  of directive,5,1
3140,3140,however the use of real time remote biometric identification systems in publicly accessible spaces for purposes other than law enforcement including by competent authorities should not be covered by the specific framework regarding such use for the purpose of law enforcement set by this regulation,5,12
3141,3141,such use for purposes other than law enforcement should therefore not be subject to the requirement of an authorisation under this regulation and the applicable detailed rules of national law that may give effect to it,5,1
3142,3142,any processing of biometric data and other personal data involved in the use of ai systems for biometric identification other than in connection to the use of real time remote biometric identification systems in publicly accessible spaces for the purpose of law enforcement as regulated by this regulation including where those systems are used by competent authorities in publicly accessible spaces for other purposes than law enforcement should continue to comply with all requirements resulting from article  of regulation eu  article  of regulation eu  and article  of directive eu  as applicable,5,12
3143,3143,in accordance with article a of protocol no  on the position of the united kingdom and ireland in respect of the area of freedom security and justice as annexed to the teu and to the tfeu ireland is not bound by the rules laid down in article  point   and  of this regulation adopted on the basis of article  of the tfeu which relate to the processing of personal data by the member states when carrying out activities falling within the scope of chapter  or chapter  of title v of part three of the tfeu where ireland is not bound by the rules governing the forms of judicial cooperation in criminal matters or police cooperation which require compliance with the provisions laid down on the basis of article  of the tfeu,5,2
3144,3144,in accordance with articles  and a of protocol no  on the position of denmark annexed to the teu and tfeu denmark is not bound by rules laid down in article  point   and  of this regulation adopted on the basis of article  of the tfeu or subject to their application which relate to the processing of personal data by the member states when carrying out activities falling within the scope of chapter  or chapter  of title v of part three of the tfeu,5,2
3145,3145,high risk ai systems should only be placed on the union market or put into service if they comply with certain mandatory requirements,5,5
3146,3146,those requirements should ensure that high risk ai systems available in the union or whose output is otherwise used in the union do not pose unacceptable risks to important union public interests as recognised and protected by union law,5,5
3147,3147,ai systems identified as high risk should be limited to those that have a significant harmful impact on the health safety and fundamental rights of persons in the union and such limitation minimises any potential restriction to international trade if any,5,5
3148,3148,ai systems could produce adverse outcomes to health and safety of persons in particular when such systems operate as components of products,5,4
3149,3149,consistently with the objectives of union harmonisation legislation to facilitate the free movement of products in the internal market and to ensure that only safe and otherwise compliant products find their way into the market it is important that the safety risks that may be generated by a product as a whole due to its digital components including ai systems are duly prevented and mitigated,5,5
3150,3150,for instance increasingly autonomous robots whether in the context of manufacturing or personal assistance and care should be able to safely operate and performs their functions in complex environments,5,4
3151,3151,similarly in the health sector where the stakes for life and health are particularly high increasingly sophisticated diagnostics systems and systems supporting human decisions should be reliable and accurate,5,18
3152,3152,the extent of the adverse impact caused by the ai system on the fundamental rights protected by the charter is of particular relevance when classifying an ai system as high risk,5,5
3153,3153,those rights include the right to human dignity respect for private and family life protection of personal data freedom of expression and information freedom of assembly and of association and non discrimination consumer protection workers rights rights of persons with disabilities right to an effective remedy and to a fair trial right of defence and the presumption of innocence right to good administration,5,12
3154,3154,in addition to those rights it is important to highlight that children have specific rights as enshrined in article  of the eu charter and in the united nations convention on the rights of the child further elaborated in the uncrc general comment no,5,2
3155,3155,as regards the digital environment both of which require consideration of the childrens vulnerabilities and provision of such protection and care as necessary for their well being,5,23
3156,3156,the fundamental right to a high level of environmental protection enshrined in the charter and implemented in union policies should also be considered when assessing the severity of the harm that an ai system can cause including in relation to the health and safety of persons,5,2
3157,3157,as regards high risk ai systems that are safety components of products or systems or which are themselves products or systems falling within the scope of regulation ec no  of the european parliament and of the council regulation eu no  of the european parliament and of the council regulation eu no  of the european parliament and of the council directive eu of council and regulation eu  of the european parliament and of the council where a third party conformity assessment is provided for medium risk and high risk products,5,5
3158,3158,as regards stand alone ai systems meaning high risk ai systems other than those that are safety components of products or which are themselves products it is appropriate to classify them as high risk if in the light of their intended purpose they pose a high risk of harm to the health and safety or the fundamental rights of persons taking into account both the severity of the possible harm and its probability of occurrence and they are used in a number of specifically pre defined areas specified in the regulation,5,5
3159,3159,the identification of those systems is based on the same methodology and criteria envisaged also for any future amendments of the list of high risk ai systems,5,5
3160,3160,technical inaccuracies of ai systems intended for the remote biometric identification of natural persons can lead to biased results and entail discriminatory effects,5,12
3161,3161,this is particularly relevant when it comes to age ethnicity sex or disabilities,5,6
3162,3162,therefore real time and post remote biometric identification systems should be classified as high risk,5,12
3163,3163,in view of the risks that they pose both types of remote biometric identification systems should be subject to specific requirements on logging capabilities and human oversight,5,12
3164,3164,as regards the management and operation of critical infrastructure it is appropriate to classify as high risk the ai systems intended to be used as safety components in the management and operation of road traffic and the supply of water gas heating and electricity since their failure or malfunctioning may put at risk the life and health of persons at large scale and lead to appreciable disruptions in the ordinary conduct of social and economic activities,5,5
3165,3165,ai systems used in education or vocational training notably for determining access or assigning persons to educational and vocational training institutions or to evaluate persons on tests as part of or as a precondition for their education should be considered high risk since they may determine the educational and professional course of a persons life and therefore affect their ability to secure their livelihood,5,5
3166,3166,when improperly designed and used such systems may violate the right to education and training as well as the right not to be discriminated against and perpetuate historical patterns of discrimination,5,6
3167,3167,ai systems used in employment workers management and access to self employment notably for the recruitment and selection of persons for making decisions on promotion and termination and for task allocation monitoring or evaluation of persons in work related contractual relationships should also be classified as high risk since those systems may appreciably impact future career prospects and livelihoods of these persons,5,0
3168,3168,relevant work related contractual relationships should involve employees and persons providing services through platforms as referred to in the commission work programme,5,19
3169,3169,such persons should in principle not be considered users within the meaning of this regulation,5,1
3170,3170,throughout the recruitment process and in the evaluation promotion or retention of persons in work related contractual relationships such systems may perpetuate historical patterns of discrimination for example against women certain age groups persons with disabilities or persons of certain racial or ethnic origins or sexual orientation,5,6
3171,3171,ai systems used to monitor the performance and behaviour of these persons may also impact their rights to data protection and privacy,5,9
3172,3172,another area in which the use of ai systems deserves special consideration is the access to and enjoyment of certain essential private and public services and benefits necessary for people to fully participate in society or to improve ones standard of living,5,4
3173,3173,in particular ai systems used to evaluate the credit score or creditworthiness of natural persons should be classified as high risk ai systems since they determine those persons access to financial resources or essential services such as housing electricity and telecommunication services,5,5
3174,3174,ai systems used for this purpose may lead to discrimination of persons or groups and perpetuate historical patterns of discrimination for example based on racial or ethnic origins disabilities age sexual orientation or create new forms of discriminatory impacts,5,6
3175,3175,considering the very limited scale of the impact and the available alternatives on the market it is appropriate to exempt ai systems for the purpose of creditworthiness assessment and credit scoring when put into service by small scale providers for their own use,5,0
3176,3176,natural persons applying for or receiving public assistance benefits and services from public authorities are typically dependent on those benefits and services and in a vulnerable position in relation to the responsible authorities,5,17
3177,3177,if ai systems are used for determining whether such benefits and services should be denied reduced revoked or reclaimed by authorities they may have a significant impact on persons livelihood and may infringe their fundamental rights such as the right to social protection nondiscrimination human dignity or an effective remedy,5,17
3178,3178,those systems should therefore be classified as high risk,5,5
3179,3179,nonetheless this regulation should not hamper the development and use of innovative approaches in the public administration which would stand to benefit from a wider use of compliant and safe ai systems provided that those systems do not entail a high risk to legal and natural persons,5,8
3180,3180,finally ai systems used to dispatch or establish priority in the dispatching of emergency first response services should also be classified as high risk since they make decisions in very critical situations for the life and health of persons and their property,5,5
3181,3181,actions by law enforcement authorities involving certain uses of ai systems are characterised by a significant degree of power imbalance and may lead to surveillance arrest or deprivation of a natural persons liberty as well as other adverse impacts on fundamental rights guaranteed in the charter,5,12
3182,3182,in particular if the ai system is not trained with high quality data does not meet adequate requirements in terms of its accuracy or robustness or is not properly designed and tested before being put on the market or otherwise put into service it may single out people in a discriminatory or otherwise incorrect or unjust manner,5,6
3183,3183,furthermore the exercise of important procedural fundamental rights such as the right to an effective remedy and to a fair trial as well as the right of defence and the presumption of innocence could be hampered in particular where such ai systems are not sufficiently transparent explainable and documented,5,10
3184,3184,it is therefore appropriate to classify as high risk a number of ai systems intended to be used in the law enforcement context where accuracy reliability and transparency is particularly important to avoid adverse impacts retain public trust and ensure accountability and effective redress,5,12
3185,3185,in view of the nature of the activities in question and the risks relating thereto those high risk ai systems should include in particular ai systems intended to be used by law enforcement authorities for individual risk assessments polygraphs and similar tools or to detect the emotional state of natural person to detect deep fakes for the evaluation of the reliability of evidence in criminal proceedings for predicting the occurrence or reoccurrence of an actual or potential criminal offence based on profiling of natural persons or assessing personality traits and characteristics or past criminal behaviour of natural persons or groups for profiling in the course of detection investigation or prosecution of criminal offences as well as for crime analytics regarding natural persons,5,12
3186,3186,ai systems specifically intended to be used for administrative proceedings by tax and customs authorities should not be considered high risk ai systems used by law enforcement authorities for the purposes of prevention detection investigation and prosecution of criminal offences,5,12
3187,3187,ai systems used in migration asylum and border control management affect people who are often in particularly vulnerable position and who are dependent on the outcome of the actions of the competent public authorities,5,4
3188,3188,the accuracy nondiscriminatory nature and transparency of the ai systems used in those contexts are therefore particularly important to guarantee the respect of the fundamental rights of the affected persons notably their rights to free movement non discrimination protection of private life and personal data international protection and good administration,5,12
3189,3189,it is therefore appropriate to classify as high risk ai systems intended to be used by the competent public authorities charged with tasks in the fields of migration asylum and border control management as polygraphs and similar tools or to detect the emotional state of a natural person for assessing certain risks posed by natural persons entering the territory of a member state or applying for visa or asylum for verifying the authenticity of the relevant documents of natural persons for assisting competent public authorities for the examination of applications for asylum visa and residence permits and associated complaints with regard to the objective to establish the eligibility of the natural persons applying for a status,5,12
3190,3190,ai systems in the area of migration asylum and border control management covered by this regulation should comply with the relevant procedural requirements set by the directive eu of the european parliament and of the council the regulation ec no  of the european parliament and of the council and other relevant legislation,5,2
3191,3191,certain ai systems intended for the administration of justice and democratic processes should be classified as high risk considering their potentially significant impact on democracy rule of law individual freedoms as well as the right to an effective remedy and to a fair trial,5,5
3192,3192,in particular to address the risks of potential biases errors and opacity it is appropriate to qualify as high risk ai systems intended to assist judicial authorities in researching and interpreting facts and the law and in applying the law to a concrete set of facts,5,5
3193,3193,such qualification should not extend however to ai systems intended for purely ancillary administrative activities that do not affect the actual administration of justice in individual cases such as anonymisation or pseudonymization of judicial decisions documents or data communication between personnel administratve tasks or allocations of resources,5,26
3194,3194,the fact that an ai system is classified as high risk under this regulation should not be interpreted as indicating that the use of the system is necessarily lawful under other acts of union law or under national law compatible with union law such as on the protection of personal data on the use of polygraphs and similar tools or other systems to detect the emotional state of natural persons,5,5
3195,3195,any such use should continue to occur solely in accordance with the applicable requirements resulting from the charter and from the applicable acts of secondary union law and national law,5,1
3196,3196,this regulation should not be understood as providing for the legal ground for processing of personal data including special categories of personal data where relevant,5,9
3197,3197,to mitigate the risks from high risk ai systems placed or otherwise put into service on the union market for users and affected persons certain mandatory requirements should apply taking into account the intended purpose of the use of the system and according to the risk management system to be established by the provider,5,5
3198,3198,requirements should apply to high risk ai systems as regards the quality of data sets used technical documentation and record keeping transparency and the provision of information to users human oversight and robustness accuracy and cybersecurity,5,5
3199,3199,those requirements are necessary to effectively mitigate the risks for health safety and fundamental rights as applicable in the light of the intended purpose of the system and no other less trade restrictive measures are reasonably available thus avoiding unjustified restrictions to trade,5,27
3200,3200,high data quality is essential for the performance of many ai systems especially when techniques involving the training of models are used with a view to ensure that the high risk ai system performs as intended and safely and it does not become the source of discrimination prohibited by union law,5,5
3201,3201,high quality training validation and testing data sets require the implementation of appropriate data governance and management practices,5,13
3202,3202,training validation and testing data sets should be sufficiently relevant representative and free of errors and complete in view of the intended purpose of the system,5,13
3203,3203,they should also have the appropriate statistical properties including as regards the persons or groups of persons on which the high risk ai system is intended to be used,5,5
3204,3204,in particular training validation and testing data sets should take into account to the extent required in the light of their intended purpose the features characteristics or elements that are particular to the specific geographical behavioural or functional setting or context within which the ai system is intended to be used,5,18
3205,3205,in order to protect the right of others from the discrimination that might result from the bias in ai systems the providers shouldbe able to process also special categories of personal data as a matter of substantial public interest in order to ensure the bias monitoring detection and correction in relation to high risk ai systems,5,6
3206,3206,for the development of high risk ai systems certain actors such as providers notified bodies and other relevant entities such as digital innovation hubs testing experimentation facilities and researchers should be able to access and use high quality datasets within their respective fields of activities which are related to this regulation,5,8
3207,3207,european common data spaces established by the commission and the facilitation of data sharing between businesses and with government in the public interest will be instrumental to provide trustful accountable and non discriminatory access to high quality data for the training validation and testing of ai systems,5,2
3208,3208,for example in health the european health data space will facilitate non discriminatory access to health data and the training of artificial intelligence algorithms on those datasets in a privacy preserving secure timely transparent and trustworthy manner and with an appropriate institutional governance,5,11
3209,3209,relevant competent authorities including sectoral ones providing or supporting the access to data may also support the provision of high quality data for the training validation and testing of ai systems,5,18
3210,3210,having information on how high risk ai systems have been developed and how they perform throughout their lifecycle is essential to verify compliance with the requirements under this regulation,5,5
3211,3211,this requires keeping records and the availability of a technical documentation containing information which is necessary to assess the compliance of the ai system with the relevant requirements,5,5
3212,3212,such information should include the general characteristics capabilities and limitations of the system algorithms data training testing and validation processes used as well as documentation on the relevant risk management system,5,24
3213,3213,the technical documentation should be kept up to date,5,24
3214,3214,to address the opacity that may make certain ai systems incomprehensible to or too complex for natural persons a certain degree of transparency should be required for high risk ai systems,5,14
3215,3215,users should be able to interpret the system output and use it appropriately,5,14
3216,3216,high risk ai systems should therefore be accompanied by relevant documentation and instructions of use and include concise and clear information including in relation to possible risks to fundamental rights and discrimination where appropriate,5,5
3217,3217,high risk ai systems should be designed and developed in such a way that natural persons can oversee their functioning,5,5
3218,3218,for this purpose appropriate human oversight measures should be identified by the provider of the system before its placing on the market or putting into service,5,15
3219,3219,in particular where appropriate such measures should guarantee that the system is subject to in built operational constraints that cannot be overridden by the system itself and is responsive to the human operator and that the natural persons to whom human oversight has been assigned have the necessary competence training and authority to carry out that role,5,15
3220,3220,high risk ai systems should perform consistently throughout their lifecycle and meet an appropriate level of accuracy robustness and cybersecurity in accordance with the generally acknowledged state of the art,5,5
3221,3221,the level of accuracy and accuracy metrics should be communicated to the users,5,18
3222,3222,the technical robustness is a key requirement for high risk ai systems,5,5
3223,3223,they should be resilient against risks connected to the limitations of the system eg,5,3
3224,3224,errors faults inconsistencies unexpected situations as well as against malicious actions that may compromise the security of the ai system and result in harmful or otherwise undesirable behaviour,5,11
3225,3225,failure to protect against these risks could lead to safety impacts or negatively affect the fundamental rights for example due to erroneous decisions or wrong or biased outputs generated by the ai system,5,3
3226,3226,cybersecurity plays a crucial role in ensuring that ai systems are resilient against attempts to alter their use behaviour performance or compromise their security properties by malicious third parties exploiting the systems vulnerabilities,5,11
3227,3227,cyberattacks against ai systems can leverage ai specific assets such as training data sets eg,5,13
3228,3228,data poisoning or trained models eg,5,13
3229,3229,adversarial attacks or exploit vulnerabilities in the ai systems digital assets or the underlying ict infrastructure,5,25
3230,3230,to ensure a level of cybersecurity appropriate to the risks suitable measures should therefore be taken by the providers of high risk ai systems also taking into account as appropriate the underlying ict infrastructure,5,5
3231,3231,as part of union harmonisation legislation rules applicable to the placing on the market putting into service and use of high risk ai systems should be laid down consistently with regulation ec no  of the european parliament and of the council setting out the requirements for accreditation and the market surveillance of products decision no ec of the european parliament and of the council on a common framework for the marketing of products and regulation eu  of the european parliament and of the council on market surveillance and compliance of products new legislative framework for the marketing of products,5,2
3232,3232,it is appropriate that a specific natural or legal person defined as the provider takes the responsibility for the placing on the market or putting into service of a high risk ai system regardless of whether that natural or legal person is the person who designed or developed the system,5,16
3233,3233,the provider should establish a sound quality management system ensure the accomplishment of the required conformity assessment procedure draw up the relevant documentation and establish a robust post market monitoring system,5,0
3234,3234,public authorities which put into service high risk ai systems for their own use may adopt and implement the rules for the quality management system as part of the quality management system adopted at a national or regional level as appropriate taking into account the specificities of the sector and the competences and organisation of the public authority in question,5,5
3235,3235,where a high risk ai system that is a safety component of a product which is covered by a relevant new legislative framework sectorial legislation is not placed on the market or put into service independently from the product the manufacturer of the final product as defined under the relevant new legislative framework legislation should comply with the obligations of the provider established in this regulation and notably ensure that the ai system embedded in the final product complies with the requirements of this regulation,5,5
3236,3236,to enable enforcement of this regulation and create a level playing field for operators and taking into account the different forms of making available of digital products it is important to ensure that under all circumstances a person established in the union can provide authorities with all the necessary information on the compliance of an ai system,5,5
3237,3237,therefore prior to making their ai systems available in the union where an importer cannot be identified providers established outside the union shall by written mandate appoint an authorised representative established in the union,5,1
3238,3238,in line with new legislative framework principles specific obligations for relevant economic operators such as importers and distributors should be set to ensure legal certainty and facilitate regulatory compliance by those relevant operators,5,0
3239,3239,given the nature of ai systems and the risks to safety and fundamental rights possibly associated with their use including as regard the need to ensure proper monitoring of the performance of an ai system in a real life setting it is appropriate to set specific responsibilities for users,5,16
3240,3240,users should in particular use high risk ai systems in accordance with the instructions of use and certain other obligations should be provided for with regard to monitoring of the functioning of the ai systems and with regard to record keeping as appropriate,5,5
3241,3241,it is appropriate to envisage that the user of the ai system should be the natural or legal person public authority agency or other body under whose authority the ai system is operated except where the use is made in the course of a personal nonprofessional activity,5,16
3242,3242,in the light of the complexity of the artificial intelligence value chain relevant third parties notably the ones involved in the sale and the supply of software software tools and components pre trained models and data or providers of network services should cooperate as appropriate with providers and users to enable their compliance with the obligations under this regulation and with competent authorities established under this regulation,5,5
3243,3243,standardisation should play a key role to provide technical solutions to providers to ensure compliance with this regulation,5,27
3244,3244,compliance with harmonised standards as defined in regulation eu no  of the european parliament and of the council should be a means for providers to demonstrate conformity with the requirements of this regulation,5,2
3245,3245,however the commission could adopt common technical specifications in areas where no harmonised standards exist or where they are insufficient,5,25
3246,3246,in order to ensure a high level of trustworthiness of high risk ai systems those systems should be subject to a conformity assessment prior to their placing on the market or putting into service,5,5
3247,3247,it is appropriate that in order to minimise the burden on operators and avoid any possible duplication for high risk ai systems related to products which are covered by existing union harmonisation legislation following the new legislative framework approach the compliance of those ai systems with the requirements of this regulation should be assessed as part of the conformity assessment already foreseen under that legislation,5,5
3248,3248,the applicability of the requirements of this regulation should thus not affect the specific logic methodology or general structure of conformity assessment under the relevant specific new legislative framework legislation,5,1
3249,3249,this approach is fully reflected in the interplay between this regulation and the machinery regulation,5,1
3250,3250,while safety risks of ai systems ensuring safety functions in machinery are addressed by the requirements of this regulation certain specific requirements in the machinery regulation will ensure the safe integration of the ai system into the overall machinery so as not to compromise the safety of the machinery as a whole,5,5
3251,3251,the machinery regulation applies the same definition of ai system as this regulation,5,8
3252,3252,given the more extensive experience of professional pre market certifiers in the field of product safety and the different nature of risks involved it is appropriate to limit at least in an initial phase of application of this regulation the scope of application of third party conformity assessment for high risk ai systems other than those related to products,5,5
3253,3253,therefore the conformity assessment of such systems should be carried out as a general rule by the provider under its own responsibility with the only exception of ai systems intended to be used for the remote biometric identification of persons for which the involvement of a notified body in the conformity assessment should be foreseen to the extent they are not prohibited,5,12
3254,3254,in order to carry out third party conformity assessment for ai systems intended to be used for the remote biometric identification of persons notified bodies should be designated under this regulation by the national competent authorities provided they are compliant with a set of requirements notably on independence competence and absence of conflicts of interests,5,12
3255,3255,in line with the commonly established notion of substantial modification for products regulated by union harmonisation legislation it is appropriate that an ai system undergoes a new conformity assessment whenever a change occurs which may affect the compliance of the system with this regulation or when the intended purpose of the system changes,5,5
3256,3256,in addition as regards ai systems which continue to learn after being placed on the market or put into service ie,5,4
3257,3257,they automatically adapt how functions are carried out it is necessary to provide rules establishing that changes to the algorithm and its performance that have been pre determined by the provider and assessed at the moment of the conformity assessment should not constitute a substantial modification,5,18
3258,3258,high risk ai systems should bear the ce marking to indicate their conformity with this regulation so that they can move freely within the internal market,5,5
3259,3259,member states should not create unjustified obstacles to the placing on the market or putting into service of high risk ai systems that comply with the requirements laid down in this regulation and bear the ce marking,5,5
3260,3260,under certain conditions rapid availability of innovative technologies may be crucial for health and safety of persons and for society as a whole,5,12
3261,3261,it is thus appropriate that under exceptional reasons of public security or protection of life and health of natural persons and the protection of industrial and commercial property member states could authorise the placing on the market or putting into service of ai systems which have not undergone a conformity assessment,5,5
3262,3262,n order to facilitate the work of the commission and the member states in the artificial intelligence field as well as to increase the transparency towards the public providers of high risk ai systems other than those related to products falling within the scope of relevant existing union harmonisation legislation should be required to register their high risk ai system in a eu database to be established and managed by the commission,5,5
3263,3263,the commission should be the controller of that database in accordance with regulation eu  of the european parliament and of the council,5,2
3264,3264,in order to ensure the full functionality of the database when deployed the procedure for setting the database should include the elaboration of functional specifications by the commission and an independent audit report,5,19
3265,3265,certain ai systems intended to interact with natural persons or to generate content may pose specific risks of impersonation or deception irrespective of whether they qualify as high risk or not,5,12
3266,3266,in certain circumstances the use of these systems should therefore be subject to specific transparency obligations without prejudice to the requirements and obligations for high risk ai systems,5,5
3267,3267,in particular natural persons should be notified that they are interacting with an ai system unless this is obvious from the circumstances and the context of use,5,4
3268,3268,moreover natural persons should be notified when they are exposed to an emotion recognition system or a biometric categorisation system,5,12
3269,3269,such information and notifications should be provided in accessible formats for persons with disabilities,5,27
3270,3270,further users who use an ai system to generate or manipulate image audio or video content that appreciably resembles existing persons places or events and would falsely appear to a person to be authentic should disclose that the content has been artificially created or manipulated by labelling the artificial intelligence output accordingly and disclosing its artificial origin,5,14
3271,3271,artificial intelligence is a rapidly developing family of technologies that requires novel forms of regulatory oversight and a safe space for experimentation while ensuring responsible innovation and integration of appropriate safeguards and risk mitigation measures,5,8
3272,3272,to ensure a legal framework that is innovation friendly futureproof and resilient to disruption national competent authorities from one or more member states should be encouraged to establish artificial intelligence regulatory sandboxes to facilitate the development and testing of innovative ai systems under strict regulatory oversight before these systems are placed on the market or otherwise put into service,5,8
3273,3273,the objectives of the regulatory sandboxes should be to foster ai innovation by establishing a controlled experimentation and testing environment in the development and pre marketing phase with a view to ensuring compliance of the innovative ai systems with this regulation and other relevant union and member states legislation to enhance legal certainty for innovators and the competent authorities oversight and understanding of the opportunities emerging risks and the impacts of ai use and to accelerate access to markets including by removing barriers for small and medium enterprises smes and start ups,5,8
3274,3274,to ensure uniform implementation across the union and economies of scale it is appropriate to establish common rules for the regulatory sandboxes implementation and a framework for cooperation between the relevant authorities involved in the supervision of the sandboxes,5,8
3275,3275,this regulation should provide the legal basis for the use of personal data collected for other purposes for developing certain ai systems in the public interest within the ai regulatory sandbox in line with article  of regulation eu  and article  of regulation eu  and without prejudice to article  of directive eu,5,2
3276,3276,participants in the sandbox should ensure appropriate safeguards and cooperate with the competent authorities including by following their guidance and acting expeditiously and in good faith to mitigate any high risks to safety and fundamental rights that may arise during the development and experimentation in the sandbox,5,16
3277,3277,the conduct of the participants in the sandbox should be taken into account when competent authorities decide whether to impose an administrative fine under article  of regulation  and article  of directive,5,16
3278,3278,in order to promote and protect innovation it is important that the interests of smallscale providers and users of ai systems are taken into particular account,5,0
3279,3279,to this objective member states should develop initiatives which are targeted at those operators including on awareness raising and information communication,5,1
3280,3280,moreover the specific interests and needs of small scale providers shall be taken into account when notified bodies set conformity assessment fees,5,27
3281,3281,translation costs related to mandatory documentation and communication with authorities may constitute a significant cost for providers and other operators notably those of a smaller scale,5,1
3282,3282,member states should possibly ensure that one of the languages determined and accepted by them for relevant providers documentation and for communication with operators is one which is broadly understood by the largest possible number of crossborder users,5,1
3283,3283,in order to minimise the risks to implementation resulting from lack of knowledge and expertise in the market as well as to facilitate compliance of providers and notified bodies with their obligations under this regulation the ai on demand platform the european digital innovation hubs and the testing and experimentation facilities established by the commission and the member states at national or eu level should possibly contribute to the implementation of this regulation,5,8
3284,3284,within their respective mission and fields of competence they may provide in particular technical and scientific support to providers and notified bodies,5,27
3285,3285,it is appropriate that the commission facilitates to the extent possible access to testing and experimentation facilities to bodies groups or laboratories established or accredited pursuant to any relevant union harmonisation legislation and which fulfil tasks in the context of conformity assessment of products or devices covered by that union harmonisation legislation,5,19
3286,3286,this is notably the case for expert panels expert laboratories and reference laboratories in the field of medical devices pursuant to regulation eu  and regulation eu,5,8
3287,3287,in order to facilitate a smooth effective and harmonised implementation of this regulation a european artificial intelligence board should be established,5,2
3288,3288,the board should be responsible for a number of advisory tasks including issuing opinions recommendations advice or guidance on matters related to the implementation of this regulation including on technical specifications or existing standards regarding the requirements established in this regulation and providing advice to and assisting the commission on specific questions related to artificial intelligence,5,19
3289,3289,member states hold a key role in the application and enforcement of this regulation,5,1
3290,3290,in this respect each member state should designate one or more national competent authorities for the purpose of supervising the application and implementation of this regulation,5,1
3291,3291,in order to increase organisation efficiency on the side of member states and to set an official point of contact vis  vis the public and other counterparts at member state and union levels in each member state one national authority should be designated as national supervisory authority,5,1
3292,3292,in order to ensure that providers of high risk ai systems can take into account the experience on the use of high risk ai systems for improving their systems and the design and development process or can take any possible corrective action in a timely manner all providers should have a post market monitoring system in place,5,5
3293,3293,this system is also key to ensure that the possible risks emerging from ai systems which continue to learn after being placed on the market or put into service can be more efficiently and timely addressed,5,5
3294,3294,in this context providers should also be required to have a system in place to report to the relevant authorities any serious incidents or any breaches to national and union law protecting fundamental rights resulting from the use of their ai systems,5,1
3295,3295,in order to ensure an appropriate and effective enforcement of the requirements and obligations set out by this regulation which is union harmonisation legislation the system of market surveillance and compliance of products established by regulation eu  should apply in its entirety,5,2
3296,3296,where necessary for their mandate national public authorities or bodies which supervise the application of union law protecting fundamental rights including equality bodies should also have access to any documentation created under this regulation,5,1
3297,3297,union legislation on financial services includes internal governance and risk management rules and requirements which are applicable to regulated financial institutions in the course of provision of those services including when they make use of ai systems,5,0
3298,3298,in order to ensure coherent application and enforcement of the obligations under this regulation and relevant rules and requirements of the union financial services legislation the authorities responsible for the supervision and enforcement of the financial services legislation including where applicable the european central bank should be designated as competent authorities for the purpose of supervising the implementation of this regulation including for market surveillance activities as regards ai systems provided or used by regulated and supervised financial institutions,5,0
3299,3299,to further enhance the consistency between this regulation and the rules applicable to credit institutions regulated under directive eu of the european parliament and of the council it is also appropriate to integrate the conformity assessment procedure and some of the providers procedural obligations in relation to risk management post marketing monitoring and documentation into the existing obligations and procedures under directive eu,5,2
3300,3300,in order to avoid overlaps limited derogations should also be envisaged in relation to the quality management system of providers and the monitoring obligation placed on users of high risk ai systems to the extent that these apply to credit institutions regulated by directive eu,5,0
3301,3301,the development of ai systems other than high risk ai systems in accordance with the requirements of this regulation may lead to a larger uptake of trustworthy artificial intelligence in the union,5,5
3302,3302,providers of non high risk ai systems should be encouraged to create codes of conduct intended to foster the voluntary application of the mandatory requirements applicable to high risk ai systems,5,5
3303,3303,providers should also be encouraged to apply on a voluntary basis additional requirements related for example to environmental sustainability accessibility to persons with disability stakeholders participation in the design and development of ai systems and diversity of the development teams,5,27
3304,3304,the commission may develop initiatives including of a sectorial nature to facilitate the lowering of technical barriers hindering cross border exchange of data for ai development including on data access infrastructure semantic and technical interoperability of different types of data,5,11
3305,3305,it is important that ai systems related to products that are not high risk in accordance with this regulation and thus are not required to comply with the requirements set out herein are nevertheless safe when placed on the market or put into service,5,5
3306,3306,to contribute to this objective the directive ec of the european parliament and of the council would apply as a safety net,5,2
3307,3307,in order to ensure trustful and constructive cooperation of competent authorities on union and national level all parties involved in the application of this regulation should respect the confidentiality of information and data obtained in carrying out their tasks,5,1
3308,3308,member states should take all necessary measures to ensure that the provisions of this regulation are implemented including by laying down effective proportionate and dissuasive penalties for their infringement,5,1
3309,3309,for certain specific infringements member states should take into account the margins and criteria set out in this regulation,5,1
3310,3310,the european data protection supervisor should have the power to impose fines on union institutions agencies and bodies falling within the scope of this regulation,5,2
3311,3311,in order to ensure that the regulatory framework can be adapted where necessary the power to adopt acts in accordance with article  tfeu should be delegated to the commission to amend the techniques and approaches referred to in annex i to define ai systems the union harmonisation legislation listed in annex ii the high risk ai systems listed in annex iii the provisions regarding technical documentation listed in annex iv the content of the eu declaration of conformity in annex v the provisions regarding the conformity assessment procedures in annex vi and vii and the provisions establishing the high risk ai systems to which the conformity assessment procedure based on assessment of the quality management system and assessment of the technical documentation should apply,5,5
3312,3312,it is of particular importance that the commission carry out appropriate consultations during its preparatory work including at expert level and that those consultations be conducted in accordance with the principles laid down in the interinstitutional agreement of  april  on better law making,5,19
3313,3313,in particular to ensure equal participation in the preparation of delegated acts the european parliament and the council receive all documents at the same time as member states experts and their experts systematically have access to meetings of commission expert groups dealing with the preparation of delegated acts,5,19
3314,3314,in order to ensure uniform conditions for the implementation of this regulation implementing powers should be conferred on the commission,5,19
3315,3315,those powers should be exercised in accordance with regulation eu no  of the european parliament and of the council,5,2
3316,3316,since the objective of this regulation cannot be sufficiently achieved by the member states and can rather by reason of the scale or effects of the action be better achieved at union level the union may adopt measures in accordance with the principle of subsidiarity as set out in article  teu,5,1
3317,3317,in accordance with the principle of proportionality as set out in that article this regulation does not go beyond what is necessary in order to achieve that objective,5,1
3318,3318,this regulation should apply from  op  please insert the date established in art,5,22
3319,3319,however the infrastructure related to the governance and the conformity assessment system should be operational before that date therefore the provisions on notified bodies and governance structure should apply from  op  please insert the date  three months following the entry into force of this regulation,5,22
3320,3320,in addition member states should lay down and notify to the commission the rules on penalties including administrative fines and ensure that they are properly and effectively implemented by the date of application of this regulation,5,1
3321,3321,therefore the provisions on penalties should apply from op  please insert the date  twelve months following the entry into force of this regulation,5,22
3322,3322,the european data protection supervisor and the european data protection board were consulted in accordance with article  of regulation eu  and delivered an opinion on,5,2
3323,3323,this regulation lays downharmonised rules for the placing on the market the putting into service and the use of artificial intelligence systems ai systems in the unionprohibitions of certain artificial intelligence practicesspecific requirements for high risk ai systems and obligations for operators of such systemsharmonised transparency rules for ai systems intended to interact with natural persons emotion recognition systems and biometric categorisation systems and ai systems used to generate or manipulate image audio or video contentrules on market monitoring and surveillance,5,5
3324,3324,this regulation applies toproviders placing on the market or putting into service ai systems in the union irrespective of whether those providers are established within the union or in a third countryusers of ai systems located within the unionproviders and users of ai systems that are located in a third country where the output produced by the system is used in the unionthis regulation shall not apply to ai systems developed or used exclusively for military purposes,5,1
3325,3325,this regulation shall not apply to public authorities in a third country nor to international organisations falling within the scope of this regulation pursuant to paragraph  where those authorities or organisations use ai systems in the framework of international agreements for law enforcement and judicial cooperation with the union or with one or more member states,5,8
3326,3326,this regulation shall not affect the application of the provisions on the liability of intermediary service providers set out in chapter ii section iv of directive ec of the european parliament and of the council as to be replaced by the corresponding provisions of the digital services act,5,23
3327,3327,for the purpose of this regulation the following definitions apply artificial intelligence system ai system means software that is developed with one or more of the techniques and approaches listed in annex i and can for a given set of human defined objectives generate outputs such as content predictions recommendations or decisions influencing the environments they interact with provider means a natural or legal person public authority agency or other body that develops an ai system or that has an ai system developed with a view to placing it on the market or putting it into service under its own name or trademark whether for payment or free of chargesmall scale provider means a provider that is a micro or small enterprise within the meaning of commission recommendation ec user means any natural or legal person public authority agency or other body using an ai system under its authority except where the ai system is used in the course of a personal non professional activity authorised representative means any natural or legal person established in the union who has received a written mandate from a provider of an ai system to respectively perform and carry out on its behalf the obligations and procedures established by this regulation importer means any natural or legal person established in the union that places on the market or puts into service an ai system that bears the name or trademark of a natural or legal person established outside the union distributor means any natural or legal person in the supply chain other than the provider or the importer that makes an ai system available on the union market without affecting its properties operator means the provider the user the authorised representative the importer and the distributor placing on the market means the first making available of an ai system on the union market making available on the market means any supply of an ai system for distribution or use on the union market in the course of a commercial activity whether in return for payment or free of charge putting into service means the supply of an ai system for first use directly to the user or for own use on the union market for its intended purpose intended purpose means the use for which an ai system is intended by the provider including the specific context and conditions of use as specified in the information supplied by the provider in the instructions for use promotional or sales materials and statements as well as in the technical documentation reasonably foreseeable misuse means the use of an ai system in a way that is not in accordance with its intended purpose but which may result from reasonably foreseeable human behaviour or interaction with other systems safety component of a product or system means a component of a product or of a system which fulfils a safety function for that product or system or the failure or malfunctioning of which endangers the health and safety of persons or property instructions for use means the information provided by the provider to inform the user of in particular an ai systems intended purpose and proper use inclusive of the specific geographical behavioural or functional setting within which the high risk ai system is intended to be used recall of an ai system means any measure aimed at achieving the return to the provider of an ai system made available to userswithdrawal of an ai system means any measure aimed at preventing the distribution display and offer of an ai system performance of an ai system means the ability of an ai system to achieve its intended purpose notifying authority means the national authority responsible for setting up and carrying out the necessary procedures for the assessment designation and notification of conformity assessment bodies and for their monitoring conformity assessment means the process of verifying whether the requirements set out in title iii chapter  of this regulation relating to an ai system have been fulfilled conformity assessment body means a body that performs third party conformity assessment activities including testing certification and inspection notified body means a conformity assessment body designated in accordance with this regulation and other relevant union harmonisation legislation substantial modification means a change to the ai system following its placing on the market or putting into service which affects the compliance of the ai system with the requirements set out in title iii chapter  of this regulation or results in a modification to the intended purpose for which the ai system has been assessed ce marking of conformity ce marking means a marking by which a provider indicates that an ai system is in conformity with the requirements set out in title iii chapter  of this regulation and other applicable union legislation harmonising the conditions for the marketing of products union harmonisation legislation providing for its affixing post market monitoring means all activities carried out by providers of ai systems to proactively collect and review experience gained from the use of ai systems they place on the market or put into service for the purpose of identifying any need to immediately apply any necessary corrective or preventive actions market surveillance authority means the national authority carrying out the activities and taking the measures pursuant to regulation eu  harmonised standard means a european standard as defined in article  of regulation eu no  common specifications means a document other than a standard containing technical solutions providing a means to comply with certain requirements and obligations established under this regulation training data means data used for training an ai system through fitting its learnable parameters including the weights of a neural network validation data means data used for providing an evaluation of the trained ai system and for tuning its non learnable parameters and its learning process among other things in order to prevent overfitting whereas the validation dataset can be a separate dataset or part of the training dataset either as a fixed or variable split testing data means data used for providing an independent evaluation of the trained and validated ai system in order to confirm the expected performance of that system before its placing on the market or putting into serviceinput data means data provided to or directly acquired by an ai system on the basis of which the system produces an output biometric data means personal data resulting from specific technical processing relating to the physical physiological or behavioural characteristics of a natural person which allow or confirm the unique identification of that natural person such as facial images or dactyloscopic data emotion recognition system means an ai system for the purpose of identifying or inferring emotions or intentions of natural persons on the basis of their biometric data biometric categorisation system means an ai system for the purpose of assigning natural persons to specific categories such as sex age hair colour eye colour tattoos ethnic origin or sexual or political orientation on the basis of their biometric data remote biometric identification system means an ai system for the purpose of identifying natural persons at a distance through the comparison of a persons biometric data with the biometric data contained in a reference database and without prior knowledge of the user of the ai system whether the person will be present and can be identified  real time remote biometric identification system means a remote biometric identification system whereby the capturing of biometric data the comparison and the identification all occur without a significant delay,5,5
3328,3328,this comprises not only instant identification but also limited short delays in order to avoid circumvention,5,12
3329,3329,post remote biometric identification system means a remote biometric identification system other than a real time remote biometric identification system publicly accessible space means any physical place accessible to the public regardless of whether certain conditions for access may apply law enforcement authority meansany public authority competent for the prevention investigation detection or prosecution of criminal offences or the execution of criminal penalties including the safeguarding against and the prevention of threats to public security orany other body or entity entrusted by member state law to exercise public authority and public powers for the purposes of the prevention investigation detection or prosecution of criminal offences or the execution of criminal penalties including the safeguarding against and the prevention of threats to public security law enforcement means activities carried out by law enforcement authorities for the prevention investigation detection or prosecution of criminal offences or the execution of criminal penalties including the safeguarding against and the prevention of threats to public securitynational supervisory authority means the authority to which a member state assigns the responsibility for the implementation and application of this regulation for coordinating the activities entrusted to that member state for acting as the single contact point for the commission and for representing the member state at the european artificial intelligence boardnational competent authority means the national supervisory authority the notifying authority and the market surveillance authority serious incident means any incident that directly or indirectly leads might have led or might lead to any of the followingthe death of a person or serious damage to a persons health to property or the environmenta serious and irreversible disruption of the management and operation of critical infrastructure,5,12
3330,3330,the following artificial intelligence practices shall be prohibitedthe placing on the market putting into service or use of an ai system that deploys subliminal techniques beyond a persons consciousness in order to materially distort a persons behaviour in a manner that causes or is likely to cause that person or another person physical or psychological harmthe placing on the market putting into service or use of an ai system that exploits any of the vulnerabilities of a specific group of persons due to their age physical or mental disability in order to materially distort the behaviour of a person pertaining to that group in a manner that causes or is likely to cause that person or another person physical or psychological harmthe placing on the market putting into service or use of ai systems by public authorities or on their behalf for the evaluation or classification of the trustworthiness of natural persons over a certain period of time based on their social behaviour or known or predicted personal or personality characteristics with the social score leading to either or both of the following detrimental or unfavourable treatment of certain natural persons or whole groups thereof in social contexts which are unrelated to the contexts in which the data was originally generated or collectedii detrimental or unfavourable treatment of certain natural persons or whole groups thereof that is unjustified or disproportionate to their social behaviour or its gravitythe use of real time remote biometric identification systems in publicly accessible spaces for the purpose of law enforcement unless and in as far as such use is strictly necessary for one of the following objectivesthe targeted search for specific potential victims of crime including missing childrenii the prevention of a specific substantial and imminent threat to the life or physical safety of natural persons or of a terrorist attackiii the detection localisation identification or prosecution of a perpetrator or suspect of a criminal offence referred to in article  of council framework decision jha and punishable in the member state concerned by a custodial sentence or a detention order for a maximum period of at least three years as determined by the law of that member state,5,5
3331,3331,the use of real time remote biometric identification systems in publicly accessible spaces for the purpose of law enforcement for any of the objectives referred to in paragraph  point d shall take into account the following elementsthe nature of the situation giving rise to the possible use in particular the seriousness probability and scale of the harm caused in the absence of the use of the systemthe consequences of the use of the system for the rights and freedoms of all persons concerned in particular the seriousness probability and scale of those consequences,5,12
3332,3332,in addition the use of real time remote biometric identification systems in publicly accessible spaces for the purpose of law enforcement for any of the objectives referred to in paragraph  point d shall comply with necessary and proportionate safeguards and conditions in relation to the use in particular as regards the temporal geographic and personal limitations,5,12
3333,3333,as regards paragraphs  point  and  each individual use for the purpose of law enforcement of a real time remote biometric identification system in publicly accessible spaces shall be subject to a prior authorisation granted by a judicial authority or by an independent administrative authority of the member state in which the use is to take place issued upon a reasoned request and in accordance with the detailed rules of national law referred to in paragraph,5,12
3334,3334,however in a duly justified situation of urgency the use of the system may be commenced without an authorisation and the authorisation may be requested only during or after the use,5,5
3335,3335,the competent judicial or administrative authority shall only grant the authorisation where it is satisfied based on objective evidence or clear indications presented to it that the use of the real time remote biometric identification system at issue is necessary for and proportionate to achieving one of the objectives specified in paragraph  point  as identified in the request,5,12
3336,3336,in deciding on the request the competent judicial or administrative authority shall take into account the elements referred to in paragraph,5,19
3337,3337,a member state may decide to provide for the possibility to fully or partially authorise the use of real time remote biometric identification systems in publicly accessible spaces for the purpose of law enforcement within the limits and under the conditions listed in paragraphs  point   and,5,12
3338,3338,that member state shall lay down in its national law the necessary detailed rules for the request issuance and exercise of as well as supervision relating to the authorisations referred to in paragraph,5,1
3339,3339,those rules shall also specify in respect of which of the objectives listed in paragraph  point  including which of the criminal offences referred to in point iii thereof the competent authorities may be authorised to use those systems for the purpose of law enforcement,5,5
3340,3340,irrespective of whether an ai system is placed on the market or put into service independently from the products referred to in points  and  that ai system shall be considered high risk where both of the following conditions are fulfilledthe ai system is intended to be used as a safety component of a product or is itself a product covered by the union harmonisation legislation listed in annex iithe product whose safety component is the ai system or the ai system itself as a product is required to undergo a third party conformity assessment with a view to the placing on the market or putting into service of that product pursuant to the union harmonisation legislation listed in annex ii,5,5
3341,3341,in addition to the high risk ai systems referred to in paragraph  ai systems referred to in annex iii shall also be considered high risk,5,5
3342,3342,the commission is empowered to adopt delegated acts in accordance with article  to update the list in annex iii by adding high risk ai systems where both of the following conditions are fulfilledthe ai systems are intended to be used in any of the areas listed in points  to  of annex iiithe ai systems pose a risk of harm to the health and safety or a risk of adverse impact on fundamental rights that is in respect of its severity and probability of occurrence equivalent to or greater than the risk of harm or of adverse impact posed by the high risk ai systems already referred to in annex iii,5,5
3343,3343,when assessing for the purposes of paragraph  whether an ai system poses a risk of harm to the health and safety or a risk of adverse impact on fundamental rights that is equivalent to or greater than the risk of harm posed by the high risk ai systems already referred to in annex iii the commission shall take into account the following criteria the intended purpose of the ai systemthe extent to which an ai system has been used or is likely to be usedthe extent to which the use of an ai system has already caused harm to the health and safety or adverse impact on the fundamental rights or has given rise to significant concerns in relation to the materialisation of such harm or adverse impact as demonstrated by reports or documented allegations submitted to national competent authoritiesthe potential extent of such harm or such adverse impact in particular in terms of its intensity and its ability to affect a plurality of personsthe extent to which potentially harmed or adversely impacted persons are dependent on the outcome produced with an ai system in particular because for practical or legal reasons it is not reasonably possible to opt out from that outcomethe extent to which potentially harmed or adversely impacted persons are in a vulnerable position in relation to the user of an ai system in particular due to an imbalance of power knowledge economic or social circumstances or agehe extent to which the outcome produced with an ai system is easily reversible whereby outcomes having an impact on the health or safety of persons shall not be considered as easily reversiblethe extent to which existing union legislation provides foreffective measures of redress in relation to the risks posed by an ai system with the exclusion of claims for damageseffective measures to prevent or substantially minimise those risks,5,5
3344,3344,high risk ai systems shall comply with the requirements established in this chapter,5,5
3345,3345,the intended purpose of the high risk ai system and the risk management system referred to in article  shall be taken into account when ensuring compliance with those requirements,5,5
3346,3346,a risk management system shall be established implemented documented and maintained in relation to high risk ai systems,5,5
3347,3347,the risk management system shall consist of a continuous iterative process run throughout the entire lifecycle of a high risk ai system requiring regular systematic updating,5,5
3348,3348,it shall comprise the following stepsidentification and analysis of the known and foreseeable risks associated with each high risk ai systemestimation and evaluation of the risks that may emerge when the high risk ai system is used in accordance with its intended purpose and under conditions of reasonably foreseeable misuseevaluation of other possibly arising risks based on the analysis of data gathered from the post market monitoring system referred to in article adoption of suitable risk management measures in accordance with the provisions of the following paragraphs,5,5
3349,3349,the risk management measures referred to in paragraph  point  shall give due consideration to the effects and possible interactions resulting from the combined application of the requirements set out in this chapter,5,3
3350,3350,they shall take into account the generally acknowledged state of the art including as reflected in relevant harmonised standards or common specifications,5,18
3351,3351,the risk management measures referred to in paragraph  point  shall be such that any residual risk associated with each hazard as well as the overall residual risk of the high risk ai systems is judged acceptable provided that the high risk ai system is used in accordance with its intended purpose or under conditions of reasonably foreseeable misuse,5,5
3352,3352,those residual risks shall be communicated to the user,5,3
3353,3353,in identifying the most appropriate risk management measures the following shall be ensuredelimination or reduction of risks as far as possible through adequate design and developmentwhere appropriate implementation of adequate mitigation and control measures in relation to risks that cannot be eliminatedprovision of adequate information pursuant to article  in particular as regards the risks referred to in paragraph  point  of this article and where appropriate training to users,5,3
3354,3354,in eliminating or reducing risks related to the use of the high risk ai system due consideration shall be given to the technical knowledge experience education training to be expected by the user and the environment in which the system is intended to be used,5,5
3355,3355,high risk ai systems shall be tested for the purposes of identifying the most appropriate risk management measures,5,5
3356,3356,testing shall ensure that high risk ai systems perform consistently for their intended purpose and they are in compliance with the requirements set out in this chapter,5,5
3357,3357,testing procedures shall be suitable to achieve the intended purpose of the ai system and do not need to go beyond what is necessary to achieve that purpose,5,18
3358,3358,the testing of the high risk ai systems shall be performed as appropriate at any point in time throughout the development process and in any event prior to the placing on the market or the putting into service,5,5
3359,3359,testing shall be made against preliminarily defined metrics and probabilistic thresholds that are appropriate to the intended purpose of the high risk ai system,5,5
3360,3360,when implementing the risk management system described in paragraphs  to  specific consideration shall be given to whether the high risk ai system is likely to be accessed by or have an impact on children,5,5
3361,3361,for credit institutions regulated by directive eu the aspects described in paragraphs  to  shall be part of the risk management procedures established by those institutions pursuant to article  of that directive,5,2
3362,3362,high risk ai systems which make use of techniques involving the training of models with data shall be developed on the basis of training validation and testing data sets that meet the quality criteria referred to in paragraphs  to,5,5
3363,3363,training validation and testing data sets shall be subject to appropriate data governance and management practices,5,13
3364,3364,those practices shall concern in particular the relevant design choicesdata collectionrelevant data preparation processing operations such as annotation labelling cleaning enrichment and aggregationthe formulation of relevant assumptions notably with respect to the information that the data are supposed to measure and representa prior assessment of the availability quantity and suitability of the data sets that are neededexamination in view of possible biasesthe identification of any possible data gaps or shortcomings and how those gaps and shortcomings can be addressed,5,24
3365,3365,training validation and testing data sets shall be relevant representative free of errors and complete,5,13
3366,3366,they shall have the appropriate statistical properties including where applicable as regards the persons or groups of persons on which the high risk ai system is intended to be used,5,5
3367,3367,these characteristics of the data sets may be met at the level of individual data sets or a combination thereof,5,24
3368,3368,training validation and testing data sets shall take into account to the extent required by the intended purpose the characteristics or elements that are particular to the specific geographical behavioural or functional setting within which the highrisk ai system is intended to be used,5,5
3369,3369,to the extent that it is strictly necessary for the purposes of ensuring bias monitoring detection and correction in relation to the high risk ai systems the providers of such systems may process special categories of personal data referred to in article  of regulation eu  article  of directive eu  and article  of regulation eu  subject to appropriate safeguards for the fundamental rights and freedoms of natural persons including technical limitations on the re use and use of state of the art security and privacy preserving measures such as pseudonymisation or encryption where anonymisation may significantly affect the purpose pursued,5,2
3370,3370,appropriate data governance and management practices shall apply for the development of high risk ai systems other than those which make use of techniques involving the training of models in order to ensure that those high risk ai systems comply with paragraph,5,5
3371,3371,the technical documentation of a high risk ai system shall be drawn up before that system is placed on the market or put into service and shall be kept up to date,5,5
3372,3372,the technical documentation shall be drawn up in such a way to demonstrate that the high risk ai system complies with the requirements set out in this chapter and provide national competent authorities and notified bodies with all the necessary information to assess the compliance of the ai system with those requirements,5,5
3373,3373,it shall contain at a minimum the elements set out in annex iv,5,22
3374,3374,where a high risk ai system related to a product to which the legal acts listed in annex ii section a apply is placed on the market or put into service one single technical documentation shall be drawn up containing all the information set out in annex iv as well as the information required under those legal acts,5,5
3375,3375,the commission is empowered to adopt delegated acts in accordance with article  to amend annex iv where necessary to ensure that in the light of technical progress the technical documentation provides all the necessary information to assess the compliance of the system with the requirements set out in this chapter,5,19
3376,3376,high risk ai systems shall be designed and developed with capabilities enabling the automatic recording of events logs while the high risk ai systems is operating,5,5
3377,3377,those logging capabilities shall conform to recognised standards or common specifications,5,18
3378,3378,the logging capabilities shall ensure a level of traceability of the ai systems functioning throughout its lifecycle that is appropriate to the intended purpose of the system,5,4
3379,3379,in particular logging capabilities shall enable the monitoring of the operation of the high risk ai system with respect to the occurrence of situations that may result in the ai system presenting a risk within the meaning of article  or lead to a substantial modification and facilitate the post market monitoring referred to in article,5,5
3380,3380,for high risk ai systems referred to in paragraph  point  of annex iii the logging capabilities shall provide at a minimumrecording of the period of each use of the system start date and time and end date and time of each usethe reference database against which input data has been checked by the systemthe input data for which the search has led to a matchthe identification of the natural persons involved in the verification of the results as referred to in article,5,5
3381,3381,high risk ai systems shall be designed and developed in such a way to ensure that their operation is sufficiently transparent to enable users to interpret the systems output and use it appropriately,5,5
3382,3382,an appropriate type and degree of transparency shall be ensured with a view to achieving compliance with the relevant obligations of the user and of the provider set out in chapter  of this title,5,14
3383,3383,high risk ai systems shall be accompanied by instructions for use in an appropriate digital format or otherwise that include concise complete correct and clear information that is relevant accessible and comprehensible to users,5,5
3384,3384,the information referred to in paragraph  shall specifythe identity and the contact details of the provider and where applicable of its authorised representativethe characteristics capabilities and limitations of performance of the high risk ai system includingits intended purposethe level of accuracy robustness and cybersecurity referred to in article  against which the high risk ai system has been tested and validated and which can be expected and any known and foreseeable circumstances that may have an impact on that expected level of accuracy robustness and cybersecurityany known or foreseeable circumstance related to the use of the highrisk ai system in accordance with its intended purpose or under conditions of reasonably foreseeable misuse which may lead to risks to the health and safety or fundamental rightsits performance as regards the persons or groups of persons on which the system is intended to be usedwhen appropriate specifications for the input data or any other relevant information in terms of the training validation and testing data sets used taking into account the intended purpose of the ai system,5,5
3385,3385,the changes to the high risk ai system and its performance which have been pre determined by the provider at the moment of the initial conformity assessment if anythe human oversight measures referred to in article  including the technical measures put in place to facilitate the interpretation of the outputs of ai systems by the usersthe expected lifetime of the high risk ai system and any necessary maintenance and care measures to ensure the proper functioning of that ai system including as regards software updates,5,5
3386,3386,high risk ai systems shall be designed and developed in such a way including with appropriate human machine interface tools that they can be effectively overseen by natural persons during the period in which the ai system is in use,5,5
3387,3387,human oversight shall aim at preventing or minimising the risks to health safety or fundamental rights that may emerge when a high risk ai system is used in accordance with its intended purpose or under conditions of reasonably foreseeable misuse in particular when such risks persist notwithstanding the application of other requirements set out in this chapter,5,5
3388,3388,human oversight shall be ensured through either one or all of the following measuresidentified and built when technically feasible into the high risk ai system by the provider before it is placed on the market or put into serviceidentified by the provider before placing the high risk ai system on the market or putting it into service and that are appropriate to be implemented by the user,5,5
3389,3389,the measures referred to in paragraph  shall enable the individuals to whom human oversight is assigned to do the following as appropriate to the circumstancesfully understand the capacities and limitations of the high risk ai system and be able to duly monitor its operation so that signs of anomalies dysfunctions and unexpected performance can be detected and addressed as soon as possibleremain aware of the possible tendency of automatically relying or over relying on the output produced by a high risk ai system automation bias in particular for high risk ai systems used to provide information or recommendations for decisions to be taken by natural personsbe able to correctly interpret the high risk ai systems output taking into account in particular the characteristics of the system and the interpretation tools and methods availablebe able to decide in any particular situation not to use the high risk ai system or otherwise disregard override or reverse the output of the high risk ai systembe able to intervene on the operation of the high risk ai system or interrupt the system through a stop button or a similar procedure,5,5
3390,3390,for high risk ai systems referred to in point  of annex iii the measures referred to in paragraph  shall be such as to ensure that in addition no action or decision is taken by the user on the basis of the identification resulting from the system unless this has been verified and confirmed by at least two natural persons,5,5
3391,3391,high risk ai systems shall be designed and developed in such a way that they achieve in the light of their intended purpose an appropriate level of accuracy robustness and cybersecurity and perform consistently in those respects throughout their lifecycle,5,5
3392,3392,the levels of accuracy and the relevant accuracy metrics of high risk ai systems shall be declared in the accompanying instructions of use,5,5
3393,3393,high risk ai systems shall be resilient as regards errors faults or inconsistencies that may occur within the system or the environment in which the system operates in particular due to their interaction with natural persons or other systems,5,5
3394,3394,the robustness of high risk ai systems may be achieved through technical redundancy solutions which may include backup or fail safe plans,5,5
3395,3395,high risk ai systems that continue to learn after being placed on the market or put into service shall be developed in such a way to ensure that possibly biased outputs due to outputs used as an input for future operations feedback loops are duly addressed with appropriate mitigation measures,5,5
3396,3396,high risk ai systems shall be resilient as regards attempts by unauthorised third parties to alter their use or performance by exploiting the system vulnerabilities,5,5
3397,3397,the technical solutions aimed at ensuring the cybersecurity of high risk ai systems shall be appropriate to the relevant circumstances and the risks,5,5
3398,3398,the technical solutions to address ai specific vulnerabilities shall include where appropriate measures to prevent and control for attacks trying to manipulate the training dataset data poisoning inputs designed to cause the model to make a mistake adversarial examples or model flaws,5,11
3399,3399,providers of high risk ai systems shallensure that their high risk ai systems are compliant with the requirements set out in chapter  of this titlehave a quality management system in place which complies with article draw up the technical documentation of the high risk ai systemwhen under their control keep the logs automatically generated by their high risk ai systemsensure that the high risk ai system undergoes the relevant conformity assessment procedure prior to its placing on the market or putting into servicecomply with the registration obligations referred to in article take the necessary corrective actions if the high risk ai system is not in conformity with the requirements set out in chapter  of this titleinform the national competent authorities of the member states in which they made the ai system available or put it into service and where applicable the notified body of the non compliance and of any corrective actions takento affix the ce marking to their high risk ai systems to indicate the conformity with this regulation in accordance with article upon request of a national competent authority demonstrate the conformity of the high risk ai system with the requirements set out in chapter  of this title,5,5
3400,3400,providers of high risk ai systems shall put a quality management system in place that ensures compliance with this regulation,5,5
3401,3401,that system shall be documented in a systematic and orderly manner in the form of written policies procedures and instructions and shall include at least the following aspectsa strategy for regulatory compliance including compliance with conformity assessment procedures and procedures for the management of modifications to the high risk ai systemtechniques procedures and systematic actions to be used for the design design control and design verification of the high risk ai systemtechniques procedures and systematic actions to be used for the development quality control and quality assurance of the high risk ai systemexamination test and validation procedures to be carried out before during and after the development of the high risk ai system and the frequency with which they have to be carried outtechnical specifications including standards to be applied and where the relevant harmonised standards are not applied in full the means to be used to ensure that the high risk ai system complies with the requirements set out in chapter  of this titlesystems and procedures for data management including data collection data analysis data labelling data storage data filtration data mining data aggregation data retention and any other operation regarding the data that is performed before and for the purposes of the placing on the market or putting into service of high risk ai systemsthe risk management system referred to in article the setting up implementation and maintenance of a post market monitoring system in accordance with article procedures related to the reporting of serious incidents and of malfunctioning in accordance with article the handling of communication with national competent authorities competent authorities including sectoral ones providing or supporting the access to data notified bodies other operators customers or other interested partiessystems and procedures for record keeping of all relevant documentation and informationresource management including security of supply related measuresan accountability framework setting out the responsibilities of the management and other staff with regard to all aspects listed in this paragraph,5,5
3402,3402,the implementation of aspects referred to in paragraph  shall be proportionate to the size of the providers organisation,5,27
3403,3403,for providers that are credit institutions regulated by directive  eu the obligation to put a quality management system in place shall be deemed to be fulfilled by complying with the rules on internal governance arrangements processes and mechanisms pursuant to article  of that directive,5,2
3404,3404,in that context any harmonised standards referred to in article  of this regulation shall be taken into account,5,1
3405,3405,providers of high risk ai systems shall draw up the technical documentation referred to in article  in accordance with annex iv,5,5
3406,3406,providers that are credit institutions regulated by directive eu shall maintain the technical documentation as part of the documentation concerning internal governance arrangements processes and mechanisms pursuant to article  of that directive,5,2
3407,3407,providers of high risk ai systems shall ensure that their systems undergo the relevant conformity assessment procedure in accordance with article  prior to their placing on the market or putting into service,5,5
3408,3408,where the compliance of the ai systems with the requirements set out in chapter  of this title has been demonstrated following that conformity assessment the providers shall draw up an eu declaration of conformity in accordance with article  and affix the ce marking of conformity in accordance with article,5,5
3409,3409,for high risk ai systems referred to in point  of annex iii that are placed on the market or put into service by providers that are credit institutions regulated by directive eu the conformity assessment shall be carried out as part of the procedure referred to in articles  to of that directive,5,5
3410,3410,providers of high risk ai systems shall keep the logs automatically generated by their high risk ai systems to the extent such logs are under their control by virtue of a contractual arrangement with the user or otherwise by law,5,5
3411,3411,the logs shall be kept for a period that is appropriate in the light of the intended purpose of high risk ai system and applicable legal obligations under union or national law,5,5
3412,3412,providers that are credit institutions regulated by directive eu shall maintain the logs automatically generated by their high risk ai systems as part of the documentation under articles  of that directive,5,0
3413,3413,providers of high risk ai systems which consider or have reason to consider that a high risk ai system which they have placed on the market or put into service is not in conformity with this regulation shall immediately take the necessary corrective actions to bring that system into conformity to withdraw it or to recall it as appropriate,5,5
3414,3414,they shall inform the distributors of the high risk ai system in question and where applicable the authorised representative and importers accordingly,5,5
3415,3415,where the high risk ai system presents a risk within the meaning of article  and thatrisk is known to the provider of the system that provider shall immediately inform the national competent authorities of the member states in which it made the system available and where applicable the notified body that issued a certificate for the high risk ai system in particular of the non compliance and of any corrective actions takenproviders of high risk ai systems shall upon request by a national competent authority provide that authority with all the information and documentation necessary to demonstrate the conformity of the high risk ai system with the requirements set out in chapter  of this title in an official union language determined by the member state concerned,5,5
3416,3416,upon a reasoned request from a national competent authority providers shall also give that authority access to the logs automatically generated by the high risk ai system to the extent such logs are under their control by virtue of a contractual arrangement with the user or otherwise by law,5,5
3417,3417,where a high risk ai system related to products to which the legal acts listed in annex ii section a apply is placed on the market or put into service together with the product manufactured in accordance with those legal acts and under the name of the product manufacturer the manufacturer of the product shall take the responsibility of the complianceof the ai system with this regulation and as far as the ai system is concerned have the same obligations imposed by the present regulation on the provider,5,5
3418,3418,prior to making their systems available on the union market where an importer cannot be identified providers established outside the union shall by written mandate appoint an authorised representative which is established in the union,5,1
3419,3419,the authorised representative shall perform the tasks specified in the mandate received from the provider,5,1
3420,3420,the mandate shall empower the authorised representative to carry out the following taskskeep a copy of the eu declaration of conformity and the technical documentation at the disposal of the national competent authorities and national authorities referred to in article provide a national competent authority upon a reasoned request with all the information and documentation necessary to demonstrate the conformity of a high risk ai system with the requirements set out in chapter  of this title including access to the logs automatically generated by the high risk ai system to the extent such logs are under the control of the provider by virtue of a contractual arrangement with the user or otherwise by lawcooperate with competent national authorities upon a reasoned request on any action the latter takes in relation to the high risk ai system,5,5
3421,3421,before placing a high risk ai system on the market importers of such system shall ensure thatthe appropriate conformity assessment procedure has been carried out by the provider of that ai systemthe provider has drawn up the technical documentation in accordance with annex ivthe system bears the required conformity marking and is accompanied by the required documentation and instructions of use,5,5
3422,3422,where an importer considers or has reason to consider that a high risk ai system is not in conformity with this regulation it shall not place that system on the market until that ai system has been brought into conformity,5,5
3423,3423,where the high risk ai system presents a risk within the meaning of article  the importer shall inform the provider of the ai system and the market surveillance authorities to that effect,5,5
3424,3424,importers shall indicate their name registered trade name or registered trade mark and the address at which they can be contacted on the high risk ai system or where that is not possible on its packaging or its accompanying documentation as applicable,5,1
3425,3425,importers shall ensure that while a high risk ai system is under their responsibility where applicable storage or transport conditions do not jeopardise its compliance with the requirements set out in chapter  of this title,5,5
3426,3426,importers shall provide national competent authorities upon a reasoned request with all necessary information and documentation to demonstrate the conformity of a high risk ai system with the requirements set out in chapter  of this title in a language which can be easily understood by that national competent authority including access to the logs automatically generated by the high risk ai system to the extent such logs are under the control of the provider by virtue of a contractual arrangement with the user or otherwise by law,5,5
3427,3427,they shall also cooperate with those authorities on any action national competent authority takes in relation to that system,5,1
3428,3428,before making a high risk ai system available on the market distributors shall verify that the high risk ai system bears the required ce conformity marking that it is accompanied by the required documentation and instruction of use and that the provider and the importer of the system as applicable have complied with the obligations set out in this regulation,5,5
3429,3429,where a distributor considers or has reason to consider that a high risk ai system is not in conformity with the requirements set out in chapter  of this title it shall not make the high risk ai system available on the market until that system has been brought into conformity with those requirements,5,5
3430,3430,furthermore where the system presents a risk within the meaning of article  the distributor shall inform the provider or the importer of the system as applicable to that effect,5,5
3431,3431,distributors shall ensure that while a high risk ai system is under their responsibility where applicable storage or transport conditions do not jeopardise the compliance of the system with the requirements set out in chapter  of this title,5,5
3432,3432,a distributor that considers or has reason to consider that a high risk ai system which it has made available on the market is not in conformity with the requirements set out in chapter  of this title shall take the corrective actions necessary to bring that system into conformity with those requirements to withdraw it or recall it or shall ensure that the provider the importer or any relevant operator as appropriate takes those corrective actions,5,5
3433,3433,where the high risk ai system presents a risk within the meaning of article  the distributor shall immediately inform the national competent authorities of the member states in which it has made the product available to that effect giving details in particular of the non compliance and of any corrective actions taken,5,5
3434,3434,upon a reasoned request from a national competent authority distributors of highrisk ai systems shall provide that authority with all the information and documentation necessary to demonstrate the conformity of a high risk system with the requirements set out in chapter  of this title,5,5
3435,3435,distributors shall also cooperate with that national competent authority on any action taken by that authority,5,1
3436,3436,any distributor importer user or other third party shall be considered a provider for the purposes of this regulation and shall be subject to the obligations of the provider under article  in any of the following circumstancesthey place on the market or put into service a high risk ai system under their name or trademarkthey modify the intended purpose of a high risk ai system already placed on the market or put into servicethey make a substantial modification to the high risk ai system,5,5
3437,3437,where the circumstances referred to in paragraph  point  or  occur the provider that initially placed the high risk ai system on the market or put it into service shall no longer be considered a provider for the purposes of this regulation,5,5
3438,3438,users of high risk ai systems shall use such systems in accordance with the instructions of use accompanying the systems pursuant to paragraphs  and,5,5
3439,3439,the obligations in paragraph  are without prejudice to other user obligations under union or national law and to the users discretion in organising its own resources and activities for the purpose of implementing the human oversight measures indicated by the provider,5,1
3440,3440,without prejudice to paragraph  to the extent the user exercises control over the input data that user shall ensure that input data is relevant in view of the intended purpose of the high risk ai system,5,5
3441,3441,users shall monitor the operation of the high risk ai system on the basis of the instructions of use,5,5
3442,3442,when they have reasons to consider that the use in accordance with the instructions of use may result in the ai system presenting a risk within the meaning of article  they shall inform the provider or distributor and suspend the use of the system,5,5
3443,3443,they shall also inform the provider or distributor when they have identified any serious incident or any malfunctioning within the meaning of article  and interrupt the use of the ai system,5,5
3444,3444,in case the user is not able to reach the provider article  shall apply mutatis mutandis,5,23
3445,3445,for users that are credit institutions regulated by directive eu the monitoring obligation set out in the first subparagraph shall be deemed to be fulfilled by complying with the rules on internal governance arrangements processes and mechanisms pursuant to article  of that directive,5,2
3446,3446,users of high risk ai systems shall keep the logs automatically generated by that high risk ai system to the extent such logs are under their control,5,5
3447,3447,the logs shall be kept for a period that is appropriate in the light of the intended purpose of the highrisk ai system and applicable legal obligations under union or national law,5,22
3448,3448,users that are credit institutions regulated by directive eu shall maintain the logs as part of the documentation concerning internal governance arrangements processes and mechanisms pursuant to article  of that directive,5,2
3449,3449,users of high risk ai systems shall use the information provided under article  to comply with their obligation to carry out a data protection impact assessment under article  of regulation eu  or article  of directive eu  where applicable,5,2
3450,3450,each member state shall designate or establish a notifying authority responsible for setting up and carrying out the necessary procedures for the assessment designation and notification of conformity assessment bodies and for their monitoring,5,1
3451,3451,member states may designate a national accreditation body referred to in regulation ec no  as a notifying authority,5,1
3452,3452,notifying authorities shall be established organised and operated in such a way that no conflict of interest arises with conformity assessment bodies and the objectivity and impartiality of their activities are safeguarded,5,1
3453,3453,notifying authorities shall be organised in such a way that decisions relating to the notification of conformity assessment bodies are taken by competent persons different from those who carried out the assessment of those bodies,5,1
3454,3454,notifying authorities shall not offer or provide any activities that conformity assessment bodies perform or any consultancy services on a commercial or competitive basis,5,1
3455,3455,notifying authorities shall safeguard the confidentiality of the information they obtain,5,1
3456,3456,notifying authorities shall have a sufficient number of competent personnel at their disposal for the proper performance of their tasks,5,1
3457,3457,notifying authorities shall make sure that conformity assessments are carried out in a proportionate manner avoiding unnecessary burdens for providers and that notified bodies perform their activities taking due account of the size of an undertaking the sector in which it operates its structure and the degree of complexity of the ai system in question,5,18
3458,3458,conformity assessment bodies shall submit an application for notification to the notifying authority of the member state in which they are established,5,1
3459,3459,the application for notification shall be accompanied by a description of the conformity assessment activities the conformity assessment module or modules and the artificial intelligence technologies for which the conformity assessment body claims to be competent as well as by an accreditation certificate where one exists issued by a national accreditation body attesting that the conformity assessment body fulfils the requirements laid down in article,5,18
3460,3460,any valid document related to existing designations of the applicant notified body under any other union harmonisation legislation shall be added,5,1
3461,3461,where the conformity assessment body concerned cannot provide an accreditation certificate it shall provide the notifying authority with the documentary evidence necessary for the verification recognition and regular monitoring of its compliance with the requirements laid down in article,5,18
3462,3462,for notified bodies which are designated under any other union harmonisation legislation all documents and certificates linked to those designations may be used to support their designation procedure under this regulation as appropriate,5,1
3463,3463,notifying authorities may notify only conformity assessment bodies which have satisfied the requirements laid down in article,5,18
3464,3464,notifying authorities shall notify the commission and the other member states using the electronic notification tool developed and managed by the commission,5,1
3465,3465,the notification shall include full details of the conformity assessment activities the conformity assessment module or modules and the artificial intelligence technologies concerned,5,18
3466,3466,the conformity assessment body concerned may perform the activities of a notified body only where no objections are raised by the commission or the other member states within one month of a notification,5,19
3467,3467,notifying authorities shall notify the commission and the other member states of any subsequent relevant changes to the notification,5,1
3468,3468,notified bodies shall verify the conformity of high risk ai system in accordance with the conformity assessment procedures referred to in article,5,5
3469,3469,notified bodies shall satisfy the organisational quality management resources and process requirements that are necessary to fulfil their tasks,5,1
3470,3470,the organisational structure allocation of responsibilities reporting lines and operation of notified bodies shall be such as to ensure that there is confidence in the performance by and in the results of the conformity assessment activities that the notified bodies conduct,5,1
3471,3471,notified bodies shall be independent of the provider of a high risk ai system in relation to which it performs conformity assessment activities,5,5
3472,3472,notified bodies shall also be independent of any other operator having an economic interest in the highrisk ai system that is assessed as well as of any competitors of the provider,5,5
3473,3473,notified bodies shall be organised and operated so as to safeguard the independence objectivity and impartiality of their activities,5,1
3474,3474,notified bodies shall document and implement a structure and procedures to safeguard impartiality and to promote and apply the principles of impartiality throughout their organisation personnel and assessment activities,5,1
3475,3475,notified bodies shall have documented procedures in place ensuring that their personnel committees subsidiaries subcontractors and any associated body or personnel of external bodies respect the confidentiality of the information which comes into their possession during the performance of conformity assessment activities except when disclosure is required by law,5,1
3476,3476,the staff of notified bodies shall be bound to observe professional secrecy with regard to all information obtained in carrying out their tasks under this regulation except in relation to the notifying authorities of the member state in which their activities are carried out,5,1
3477,3477,notified bodies shall have procedures for the performance of activities which take due account of the size of an undertaking the sector in which it operates its structure the degree of complexity of the ai system in question,5,4
3478,3478,notified bodies shall take out appropriate liability insurance for their conformity assessment activities unless liability is assumed by the member state concerned in accordance with national law or that member state is directly responsible for the conformity assessment,5,10
3479,3479,notified bodies shall be capable of carrying out all the tasks falling to them under this regulation with the highest degree of professional integrity and the requisite competence in the specific field whether those tasks are carried out by notified bodies themselves or on their behalf and under their responsibility,5,1
3480,3480,notified bodies shall have sufficient internal competences to be able to effectively evaluate the tasks conducted by external parties on their behalf,5,1
3481,3481,to that end at all times and for each conformity assessment procedure and each type of high risk ai system in relation to which they have been designated the notified body shall have permanent availability of sufficient administrative technical and scientific personnel who possess experience and knowledge relating to the relevant artificial intelligence technologies data and data computing and to the requirements set out in chapter  of this title,5,5
3482,3482,notified bodies shall participate in coordination activities as referred to in article,5,1
3483,3483,they shall also take part directly or be represented in european standardisation organisations or ensure that they are aware and up to date in respect of relevant standards,5,2
3484,3484,notified bodies shall make available and submit upon request all relevant documentation including the providers documentation to the notifying authority referred to in article  to allow it to conduct its assessment designation notification monitoring and surveillance activities and to facilitate the assessment outlined in this chapter,5,1
3485,3485,where a notified body subcontracts specific tasks connected with the conformity assessment or has recourse to a subsidiary it shall ensure that the subcontractor or the subsidiary meets the requirements laid down in article  and shall inform the notifying authority accordingly,5,1
3486,3486,notified bodies shall take full responsibility for the tasks performed by subcontractors or subsidiaries wherever these are established,5,1
3487,3487,activities may be subcontracted or carried out by a subsidiary only with the agreement of the provider,5,1
3488,3488,notified bodies shall keep at the disposal of the notifying authority the relevant documents concerning the assessment of the qualifications of the subcontractor or the subsidiary and the work carried out by them under this regulation,5,1
3489,3489,the commission shall assign an identification number to notified bodies,5,1
3490,3490,it shall assign a single number even where a body is notified under several union acts,5,1
3491,3491,the commission shall make publicly available the list of the bodies notified under this regulation including the identification numbers that have been assigned to them and the activities for which they have been notified,5,1
3492,3492,the commission shall ensure that the list is kept up to date,5,19
3493,3493,where a notifying authority has suspicions or has been informed that a notified body no longer meets the requirements laid down in article  or that it is failing to fulfil its obligations that authority shall without delay investigate the matter with the utmost diligence,5,1
3494,3494,in that context it shall inform the notified body concerned about the objections raised and give it the possibility to make its views known,5,14
3495,3495,if the notifying authority comes to the conclusion that the notified body investigation no longer meets the requirements laid down in article  or that it is failing to fulfil its obligations it shall restrict suspend or withdraw the notification as appropriate depending on the seriousness of the failure,5,22
3496,3496,it shall also immediately inform the commission and the other member states accordingly,5,1
3497,3497,in the event of restriction suspension or withdrawal of notification or where the notified body has ceased its activity the notifying authority shall take appropriate steps to ensure that the files of that notified body are either taken over by another notified body or kept available for the responsible notifying authorities at their request,5,1
3498,3498,the commission shall where necessary investigate all cases where there are reasons to doubt whether a notified body complies with the requirements laid down in article,5,19
3499,3499,the notifying authority shall provide the commission on request with all relevant information relating to the notification of the notified body concerned,5,1
3500,3500,the commission shall ensure that all confidential information obtained in the course of its investigations pursuant to this article is treated confidentially,5,19
3501,3501,where the commission ascertains that a notified body does not meet or no longer meets the requirements laid down in article  it shall adopt a reasoned decision requesting the notifying member state to take the necessary corrective measures including withdrawal of notification if necessary,5,1
3502,3502,that implementing act shall be adopted in accordance with the examination procedure referred to in article,5,19
3503,3503,the commission shall ensure that with regard to the areas covered by this regulation appropriate coordination and cooperation between notified bodies active in the conformity assessment procedures of ai systems pursuant to this regulation are put in place and properly operated in the form of a sectoral group of notified bodies,5,19
3504,3504,member states shall ensure that the bodies notified by them participate in the work of that group directly or by means of designated representatives,5,1
3505,3505,conformity assessment bodies established under the law of a third country with which the union has concluded an agreement may be authorised to carry out the activities of notified bodies under this regulation,5,1
3506,3506,high risk ai systems which are in conformity with harmonised standards or parts thereof the references of which have been published in the official journal of the european union shall be presumed to be in conformity with the requirements set out in chapter  of this title to the extent those standards cover those requirements,5,5
3507,3507,where harmonised standards referred to in article  do not exist or where the commission considers that the relevant harmonised standards are insufficient or that there is a need to address specific safety or fundamental right concerns the commission may by means of implementing acts adopt common specifications in respect of the requirements set out in chapter  of this title,5,1
3508,3508,those implementing acts shall be adopted in accordance with the examination procedure referred to in article,5,19
3509,3509,the commission when preparing the common specifications referred to in paragraph  shall gather the views of relevant bodies or expert groups established under relevant sectorial union law,5,19
3510,3510,high risk ai systems which are in conformity with the common specifications referred to in paragraph  shall be presumed to be in conformity with the requirements set out in chapter  of this title to the extent those common specifications cover those requirements,5,5
3511,3511,where providers do not comply with the common specifications referred to in paragraph  they shall duly justify that they have adopted technical solutions that are at least equivalent thereto,5,27
3512,3512,taking into account their intended purpose high risk ai systems that have been trained and tested on data concerning the specific geographical behavioural and functional setting within which they are intended to be used shall be presumed to be in compliance with the requirement set out in article,5,5
3513,3513,high risk ai systems that have been certified or for which a statement of conformity has been issued under a cybersecurity scheme pursuant to regulation eu  of the european parliament and of the council and the references of which have been published in the official journal of the european union shall be presumed to be in compliance with the cybersecurity requirements set out in article  of this regulation in so far as the cybersecurity certificate or statement of conformity or parts thereof cover those requirements,5,5
3514,3514,for high risk ai systems listed in point  of annex iii where in demonstrating the compliance of a high risk ai system with the requirements set out in chapter  of this title the provider has applied harmonised standards referred to in article  or where applicable common specifications referred to in article  the provider shall follow one of the following procedures the conformity assessment procedure based on internal control referred to in annex vi the conformity assessment procedure based on assessment of the quality management system and assessment of the technical documentation with the involvement of a notified body referred to in annex vii,5,5
3515,3515,where in demonstrating the compliance of a high risk ai system with the requirements set out in chapter  of this title the provider has not applied or has applied only in part harmonised standards referred to in article  or where such harmonised standards do not exist and common specifications referred to in article  are not available the provider shall follow the conformity assessment procedure set out in annex vii,5,5
3516,3516,for the purpose of the conformity assessment procedure referred to in annex vii the provider may choose any of the notified bodies,5,18
3517,3517,however when the system is intended to be put into service by law enforcement immigration or asylum authorities as well as eu institutions bodies or agencies the market surveillance authority referred to in article  or  as applicable shall act as a notified body,5,1
3518,3518,for high risk ai systems referred to in points  to  of annex iii providers shall follow the conformity assessment procedure based on internal control as referred to in annex vi which does not provide for the involvement of a notified body,5,5
3519,3519,for high risk ai systems referred to in point  of annex iii placed on the market or put into service by credit institutions regulated by directive eu the conformity assessment shall be carried out as part of the procedure referred to in articles  to of that directive,5,5
3520,3520,for high risk ai systems to which legal acts listed in annex ii section a apply the provider shall follow the relevant conformity assessment as required under those legal acts,5,5
3521,3521,the requirements set out in chapter  of this title shall apply to those high risk ai systems and shall be part of that assessment,5,5
3522,3522,points    and the fifth paragraph of point  of annex vii shall also apply,5,22
3523,3523,for the purpose of that assessment notified bodies which have been notified under those legal acts shall be entitled to control the conformity of the high risk ai systems with the requirements set out in chapter  of this title provided that the compliance of those notified bodies with requirements laid down in article   and  has been assessed in the context of the notification procedure under those legal acts,5,5
3524,3524,where the legal acts listed in annex ii section a enable the manufacturer of the product to opt out from a third party conformity assessment provided that that manufacturer has applied all harmonised standards covering all the relevant requirements that manufacturer may make use of that option only if he has also applied harmonised standards or where applicable common specifications referred to in article  covering the requirements set out in chapter  of this title,5,1
3525,3525,high risk ai systems shall undergo a new conformity assessment procedure whenever they are substantially modified regardless of whether the modified system is intended to be further distributed or continues to be used by the current user,5,5
3526,3526,for high risk ai systems that continue to learn after being placed on the market or put into service changes to the high risk ai system and its performance that have been pre determined by the provider at the moment of the initial conformity assessment and are part of the information contained in the technical documentation referred to in point  of annex iv shall not constitute a substantial modification,5,5
3527,3527,the commission is empowered to adopt delegated acts in accordance with article  for the purpose of updating annexes vi and annex vii in order to introduce elements of the conformity assessment procedures that become necessary in light of technical progress,5,19
3528,3528,the commission is empowered to adopt delegated acts to amend paragraphs  and  in order to subject high risk ai systems referred to in points  to  of annex iii to the conformity assessment procedure referred to in annex vii or parts thereof,5,19
3529,3529,the commission shall adopt such delegated acts taking into account the effectiveness of the conformity assessment procedure based on internal control referred to in annex vi in preventing or minimizing the risks to health and safety and protection of fundamental rights posed by such systems as well as the availability of adequate capacities and resources among notified bodies,5,19
3530,3530,certificates issued by notified bodies in accordance with annex vii shall be drawnup in an official union language determined by the member state in which the notified body is established or in an official union language otherwise acceptable to the notified body,5,1
3531,3531,certificates shall be valid for the period they indicate which shall not exceed five years,5,22
3532,3532,on application by the provider the validity of a certificate may be extended for further periods each not exceeding five years based on a re assessment in accordance with the applicable conformity assessment procedures,5,18
3533,3533,where a notified body finds that an ai system no longer meets the requirements set out in chapter  of this title it shall taking account of the principle of proportionality suspend or withdraw the certificate issued or impose any restrictions on it unless compliance with those requirements is ensured by appropriate corrective action taken by the provider of the system within an appropriate deadline set by the notified body,5,5
3534,3534,the notified body shall give reasons for its decision,5,21
3535,3535,member states shall ensure that an appeal procedure against decisions of the notified bodies is available to parties having a legitimate interest in that decision,5,21
3536,3536,notified bodies shall inform the notifying authority of the followingany union technical documentation assessment certificates any supplements to those certificates quality management system approvals issued in accordance with the requirements of annex viiany refusal restriction suspension or withdrawal of a union technical documentation assessment certificate or a quality management system approval issued in accordance with the requirements of annex viiany circumstances affecting the scope of or conditions for notificationany request for information which they have received from market surveillance authorities regarding conformity assessment activitieson request conformity assessment activities performed within the scope of their notification and any other activity performed including cross border activities and subcontracting,5,1
3537,3537,each notified body shall inform the other notified bodies ofquality management system approvals which it has refused suspended or withdrawn and upon request of quality system approvals which it has issuedeu technical documentation assessment certificates or any supplements thereto which it has refused withdrawn suspended or otherwise restricted and upon request of the certificates andor supplements thereto which it has issued,5,1
3538,3538,each notified body shall provide the other notified bodies carrying out similar conformity assessment activities covering the same artificial intelligence technologies with relevant information on issues relating to negative and on request positive conformity assessment results,5,18
3539,3539,by way of derogation from article  any market surveillance authority may authorise the placing on the market or putting into service of specific high risk ai systems within the territory of the member state concerned for exceptional reasons of public security or the protection of life and health of persons environmental protection and the protection of key industrial and infrastructural assets,5,5
3540,3540,that authorisation shall be for a limited period of time while the necessary conformity assessment procedures are being carried out and shall terminate once those procedures have been completed,5,19
3541,3541,the completion of those procedures shall be undertaken without undue delay,5,22
3542,3542,the authorisation referred to in paragraph  shall be issued only if the market surveillance authority concludes that the high risk ai system complies with the requirements of chapter  of this title,5,5
3543,3543,the market surveillance authority shall inform the commission and the other member states of any authorisation issued pursuant to paragraph,5,1
3544,3544,where within  calendar days of receipt of the information referred to in paragraph  no objection has been raised by either a member state or the commission in respect of an authorisation issued by a market surveillance authority of a member state in accordance with paragraph  that authorisation shall be deemed justified,5,19
3545,3545,where within  calendar days of receipt of the notification referred to in paragraph  objections are raised by a member state against an authorisation issued by a market surveillance authority of another member state or where the commission considers the authorisation to be contrary to union law or the conclusion of the member states regarding the compliance of the system as referred to in paragraph  to be unfounded the commission shall without delay enter into consultation with the relevant member state the operators concerned shall be consulted and have the possibility to present their views,5,1
3546,3546,in view thereof the commission shall decide whether the authorisation is justified or not,5,19
3547,3547,the commission shall address its decision to the member state concerned and the relevant operator or operators,5,19
3548,3548,if the authorisation is considered unjustified this shall be withdrawn by the market surveillance authority of the member state concerned,5,12
3549,3549,by way of derogation from paragraphs  to  for high risk ai systems intended to be used as safety components of devices or which are themselves devices covered by regulation eu  and regulation eu  article  of regulation eu  and article  of regulation eu  shall apply also with regard to the derogation from the conformity assessment of the compliance with the requirements set out in chapter  of this title,5,5
3550,3550,the provider shall draw up a written eu declaration of conformity for each ai system and keep it at the disposal of the national competent authorities for  years after the ai system has been placed on the market or put into service,5,5
3551,3551,the eu declaration of conformity shall identify the ai system for which it has been drawn up,5,2
3552,3552,a copy of the eu declaration of conformity shall be given to the relevant national competent authorities upon request,5,2
3553,3553,the eu declaration of conformity shall state that the high risk ai system in question meets the requirements set out in chapter  of this title,5,5
3554,3554,the eu declaration of conformity shall contain the information set out in annex v and shall be translated into an official union language or languages required by the member states in which the high risk ai system is made available,5,2
3555,3555,where high risk ai systems are subject to other union harmonisation legislation which also requires an eu declaration of conformity a single eu declaration of conformity shall be drawn up in respect of all union legislations applicable to the high risk ai system,5,5
3556,3556,the declaration shall contain all the information required for identification of the union harmonisation legislation to which the declaration relates,5,1
3557,3557,by drawing up the eu declaration of conformity the provider shall assume responsibility for compliance with the requirements set out in chapter  of this title,5,2
3558,3558,the provider shall keep the eu declaration of conformity up to date as appropriate,5,2
3559,3559,the commission shall be empowered to adopt delegated acts in accordance with article  for the purpose of updating the content of the eu declaration of conformity set out in annex v in order to introduce elements that become necessary in light of technical progress,5,19
3560,3560,the ce marking shall be affixed visibly legibly and indelibly for high risk ai systems,5,5
3561,3561,where that is not possible or not warranted on account of the nature of the high risk ai system it shall be affixed to the packaging or to the accompanying documentation as appropriate,5,5
3562,3562,the ce marking referred to in paragraph  of this article shall be subject to the general principles set out in article  of regulation ec no,5,1
3563,3563,where applicable the ce marking shall be followed by the identification number of the notified body responsible for the conformity assessment procedures set out in article,5,18
3564,3564,the identification number shall also be indicated in any promotional material which mentions that the high risk ai system fulfils the requirements for ce marking,5,5
3565,3565,the provider shall for a period ending  years after the ai system has been placed on the market or put into service keep at the disposal of the national competent authoritiesthe technical documentation referred to in article the documentation concerning the quality management system referred to article the documentation concerning the changes approved by notified bodies where applicablethe decisions and other documents issued by the notified bodies where applicablethe eu declaration of conformity referred to in article,5,5
3566,3566,before placing on the market or putting into service a high risk ai system referred to in article  the provider or where applicable the authorised representative shall register that system in the eu database referred to in article,5,5
3567,3567,transparency obligations for certain ai systems,5,14
3568,3568,providers shall ensure that ai systems intended to interact with natural persons are designed and developed in such a way that natural persons are informed that they are interacting with an ai system unless this is obvious from the circumstances and the context of use,5,5
3569,3569,this obligation shall not apply to ai systems authorised by law to detect prevent investigate and prosecute criminal offences unless those systems are available for the public to report a criminal offence,5,5
3570,3570,users of an emotion recognition system or a biometric categorisation system shall inform of the operation of the system the natural persons exposed thereto,5,12
3571,3571,this obligation shall not apply to ai systems used for biometric categorisation which are permitted by law to detect prevent and investigate criminal offences,5,12
3572,3572,users of an ai system that generates or manipulates image audio or video content that appreciably resembles existing persons objects places or other entities or events and would falsely appear to a person to be authentic or truthful deep fake shall disclose that the content has been artificially generated or manipulatedhowever the first subparagraph authorised by law to detect prevent investigate and prosecute criminal offences or it is necessary for the exercise of the right to freedom of expression and the right to freedom of the arts and sciences guaranteed in the charter of fundamental rights of the eu and subject to appropriate safeguards or the rights and freedoms of third parties,5,12
3573,3573,paragraphs   and  shall not affect the requirements and obligations set out in title iii of this regulation,5,1
3574,3574,ai regulatory sandboxes established by one or more member states competent authorities or the european data protection supervisor shall provide a controlled environment that facilitates the development testing and validation of innovative ai systems for a limited time before their placement on the market or putting into service pursuant to a specific plan,5,8
3575,3575,this shall take place under the direct supervision and guidance by the competent authorities with a view to ensuring compliance with the requirements of this regulation and where relevant other union and member states legislation supervised within the sandbox,5,1
3576,3576,member states shall ensure that to the extent the innovative ai systems involve the processing of personal data or otherwise fall under the supervisory remit of other national authorities or competent authorities providing or supporting access to data the national data protection authorities and those other national authorities are associated to the operation of the ai regulatory sandbox,5,8
3577,3577,the ai regulatory sandboxes shall not affect the supervisory and corrective powers of the competent authorities,5,8
3578,3578,any significant risks to health and safety and fundamental rights identified during the development and testing of such systems shall result in immediate mitigation and failing that in the suspension of the development and testing process until such mitigation takes place,5,15
3579,3579,participants in the ai regulatory sandbox shall remain liable under applicable union and member states liability legislation for any harm inflicted on third parties as a result from the experimentation taking place in the sandbox,5,10
3580,3580,member states competent authorities that have established ai regulatory sandboxes shall coordinate their activities and cooperate within the framework of the european artificial intelligence board,5,8
3581,3581,they shall submit annual reports to the board and the commission on the results from the implementation of those scheme including good practices lessons learnt and recommendations on their setup and where relevant on the application of this regulation and other union legislation supervised within the sandbox,5,19
3582,3582,the modalities and the conditions of the operation of the ai regulatory sandboxes including the eligibility criteria and the procedure for the application selection participation and exiting from the sandbox and the rights and obligations of the participants shall be set out in implementing acts,5,8
3583,3583,those implementing acts shall be adopted in accordance with the examination procedure referred to in article,5,19
3584,3584,in the ai regulatory sandbox personal data lawfully collected for other purposes shall be processed for the purposes of developing and testing certain innovative ai systems in the sandbox under the following conditionsthe innovative ai systems shall be developed for safeguarding substantial public interest in one or more of the following areasthe prevention investigation detection or prosecution of criminal offences or the execution of criminal penalties including the safeguarding against and the prevention of threats to public security under the control and responsibility of the competent authorities,5,12
3585,3585,the processing shall be based on member state or union lawpublic safety and public health including disease prevention control and treatmenta high level of protection and improvement of the quality of the environmentthe data processed are necessary for complying with one or more of the requirements referred to in title iii chapter  where those requirements cannot be effectively fulfilled by processing anonymised synthetic or other non personal datathere are effective monitoring mechanisms to identify if any high risks to the fundamental rights of the data subjects may arise during the sandbox experimentation as well as response mechanism to promptly mitigate those risks and where necessary stop the processingany personal data to be processed in the context of the sandbox are in a functionally separate isolated and protected data processing environment under the control of the participants and only authorised persons have access to that dataany personal data processed are not be transmitted transferred or otherwise accessed by other partiesany processing of personal data in the context of the sandbox do not lead to measures or decisions affecting the data subjectsany personal data processed in the context of the sandbox are deleted once the participation in the sandbox has terminated or the personal data has reached the end of its retention periodthe logs of the processing of personal data in the context of the sandbox are kept for the duration of the participation in the sandbox and  year after its termination solely for the purpose of and only as long as necessary for fulfilling accountability and documentation obligations under this article or other application union or member states legislationcomplete and detailed description of the process and rationale behind the training testing and validation of the ai system is kept together with the testing results as part of the technical documentation in annex iva short summary of the ai project developed in the sandbox its objectives and expected results published on the website of the competent authorities,5,11
3586,3586,paragraph  is without prejudice to union or member states legislation excluding processing for other purposes than those explicitly mentioned in that legislation,5,1
3587,3587,member states shall undertake the following actionsprovide small scale providers and start ups with priority access to the ai regulatory sandboxes to the extent that they fulfil the eligibility conditionsorganise specific awareness raising activities about the application of this regulation tailored to the needs of the small scale providers and userswhere appropriate establish a dedicated channel for communication with small scale providers and user and other innovators to provide guidance and respond to queries about the implementation of this regulation,5,8
3588,3588,a european artificial intelligence board the board is established,5,2
3589,3589,the board shall provide advice and assistance to the commission in order tocontribute to the effective cooperation of the national supervisory authorities and the commission with regard to matters covered by this regulationcoordinate and contribute to guidance and analysis by the commission and the national supervisory authorities and other competent authorities on emerging issues across the internal market with regard to matters covered by this regulationassist the national supervisory authorities and the commission in ensuring the consistent application of this regulation,5,19
3590,3590,the board shall be composed of the national supervisory authorities who shall be represented by the head or equivalent high level official of that authority and the european data protection supervisor,5,2
3591,3591,other national authorities may be invited to the meetings where the issues discussed are of relevance for them,5,1
3592,3592,the board shall adopt its rules of procedure by a simple majority of its members following the consent of the commission,5,19
3593,3593,the rules of procedure shall also contain the operational aspects related to the execution of the boards tasks as listed in article,5,19
3594,3594,the board may establish sub groups as appropriate for the purpose of examining specific questions,5,19
3595,3595,the board shall be chaired by the commission,5,19
3596,3596,the commission shall convene the meetings and prepare the agenda in accordance with the tasks of the board pursuant to this regulation and with its rules of procedure,5,19
3597,3597,the commission shall provide administrative and analytical support for the activities of the board pursuant to this regulation,5,19
3598,3598,the board may invite external experts and observers to attend its meetings and may hold exchanges with interested third parties to inform its activities to an appropriate extent,5,1
3599,3599,to that end the commission may facilitate exchanges between the board and other union bodies offices agencies and advisory groups,5,19
3600,3600,when providing advice and assistance to the commission in the context of article  the board shall in particularcollect and share expertise and best practices among member statescontribute to uniform administrative practices in the member states including for the functioning of regulatory sandboxes referred to in article issue opinions recommendations or written contributions on matters related to the implementation of this regulation in particularon technical specifications or existing standards regarding the requirements set out in title iii chapter on the use of harmonised standards or common specifications referred to in articles  and on the preparation of guidance documents including the guidelines concerning the setting of administrative fines referred to in article,5,19
3601,3601,national competent authorities shall be established or designated by each member state for the purpose of ensuring the application and implementation of this regulation,5,1
3602,3602,national competent authorities shall be organised so as to safeguard the objectivity and impartiality of their activities and tasks,5,1
3603,3603,each member state shall designate a national supervisory authority among the national competent authorities,5,1
3604,3604,the national supervisory authority shall act as notifying authority and market surveillance authority unless a member state has organisational and administrative reasons to designate more than one authority,5,1
3605,3605,member states shall inform the commission of their designation or designations and where applicable the reasons for designating more than one authority,5,1
3606,3606,member states shall ensure that national competent authorities are provided with adequate financial and human resources to fulfil their tasks under this regulation,5,1
3607,3607,in particular national competent authorities shall have a sufficient number of personnel permanently available whose competences and expertise shall include an in depth understanding of artificial intelligence technologies data and data computing fundamental rights health and safety risks and knowledge of existing standards and legal requirements,5,8
3608,3608,member states shall report to the commission on an annual basis on the status of the financial and human resources of the national competent authorities with an assessment of their adequacy,5,19
3609,3609,the commission shall transmit that information to the board for discussion and possible recommendations,5,19
3610,3610,the commission shall facilitate the exchange of experience between national competent authorities,5,19
3611,3611,national competent authorities may provide guidance and advice on the implementation of this regulation including to small scale providers,5,27
3612,3612,whenever national competent authorities intend to provide guidance and advice with regard to an ai system in areas covered by other union legislation the competent national authorities under that union legislation shall be consulted as appropriate,5,1
3613,3613,member states may also establish one central contact point for communication with operators,5,1
3614,3614,when union institutions agencies and bodies fall within the scope of this regulation the european data protection supervisor shall act as the competent authority for their supervision,5,2
3615,3615,the commission shall in collaboration with the member states set up and maintain a eu database containing information referred to in paragraph  concerning high risk ai systems referred to in article  which are registered in accordance with article,5,2
3616,3616,the data listed in annex viii shall be entered into the eu database by the providers,5,2
3617,3617,the commission shall provide them with technical and administrative support,5,19
3618,3618,information contained in the eu database shall be accessible to the public,5,2
3619,3619,the eu database shall contain personal data only insofar as necessary for collecting and processing information in accordance with this regulation,5,9
3620,3620,that information shall include the names and contact details of natural persons who are responsible for registering the system and have the legal authority to represent the provider,5,1
3621,3621,the commission shall be the controller of the eu database,5,2
3622,3622,it shall also ensure to providers adequate technical and administrative support,5,27
3623,3623,providers shall establish and document a post market monitoring system in a manner that is proportionate to the nature of the artificial intelligence technologies and the risks of the high risk ai system,5,5
3624,3624,the post market monitoring system shall actively and systematically collect document and analyse relevant data provided by users or collected through other sources on the performance of high risk ai systems throughout their lifetime and allow the provider to evaluate the continuous compliance of ai systems with the requirements set out in title iii chapter,5,5
3625,3625,the post market monitoring system shall be based on a post market monitoring plan,5,0
3626,3626,the post market monitoring plan shall be part of the technical documentation referred to in annex iv,5,0
3627,3627,the commission shall adopt an implementing act laying down detailed provisions establishing a template for the post market monitoring plan and the list of elements to be included in the plan,5,19
3628,3628,for high risk ai systems covered by the legal acts referred to in annex ii where a post market monitoring system and plan is already established under that legislation the elements described in paragraphs   and  shall be integrated into that system and plan as appropriate,5,5
3629,3629,the first subparagraph shall also apply to high risk ai systems referred to in point  of annex iii placed on the market or put into service by credit institutions regulated by directive eu,5,5
3630,3630,sharing of information on incidents and malfunctioning,5,10
3631,3631,providers of high risk ai systems placed on the union market shall report any serious incident or any malfunctioning of those systems which constitutes a breach of obligations under union law intended to protect fundamental rights to the market surveillance authorities of the member states where that incident or breach occurred,5,5
3632,3632,such notification shall be made immediately after the provider has established a causal link between the ai system and the incident or malfunctioning or the reasonable likelihood of such a link and in any event not later than  days after the providers becomes aware of the serious incident or of the malfunctioning,5,10
3633,3633,upon receiving a notification related to a breach of obligations under union law intended to protect fundamental rights the market surveillance authority shall inform the national public authorities or bodies referred to in article,5,1
3634,3634,the commission shall develop dedicated guidance to facilitate compliance with the obligations set out in paragraph,5,19
3635,3635,that guidance shall be issued  months after the entry into force of this regulation at the latest,5,22
3636,3636,for high risk ai systems referred to in point  of annex iii which are placed on the market or put into service by providers that are credit institutions regulated by directive eu and for high risk ai systems which are safety components of devices or are themselves devices covered by regulation eu  and regulation eu  the notification of serious incidents or malfunctioning shall be limited to those that that constitute a breach of obligations under union law intended to protect fundamental rights,5,5
3637,3637,regulation eu  shall apply to ai systems covered by this regulation,5,2
3638,3638,however for the purpose of the effective enforcement of this regulationany reference to an economic operator under regulation eu  shall be understood as including all operators identified in title iii chapter  of this regulationany reference to a product under regulation eu  shall be understood as including all ai systems falling within the scope of this regulation,5,1
3639,3639,the national supervisory authority shall report to the commission on a regular basis the outcomes of relevant market surveillance activities,5,19
3640,3640,the national supervisory authority shall report without delay to the commission and relevant national competition authorities any information identified in the course of market surveillance activities that may be of potential interest for the application of union law on competition rules,5,19
3641,3641,for high risk ai systems related to products to which legal acts listed in annex ii section a apply the market surveillance authority for the purposes of this regulation shall be the authority responsible for market surveillance activities designated under those legal acts,5,5
3642,3642,for ai systems placed on the market put into service or used by financial institutions regulated by union legislation on financial services the market surveillance authority for the purposes of this regulation shall be the relevant authority responsible for the financial supervision of those institutions under that legislation,5,0
3643,3643,for ai systems listed in point  in so far as the systems are used for law enforcement purposes points  and  of annex iii member states shall designate as market surveillance authorities for the purposes of this regulation either the competent data protection supervisory authorities under directive eu  or regulation  or the national competent authorities supervising the activities of the law enforcement immigration or asylum authorities putting into service or using those systems,5,5
3644,3644,where union institutions agencies and bodies fall within the scope of this regulation the european data protection supervisor shall act as their market surveillance authority,5,2
3645,3645,member states shall facilitate the coordination between market surveillance authorities designated under this regulation and other relevant national authorities or bodies which supervise the application of union harmonisation legislation listed in annex ii or other union legislation that might be relevant for the high risk ai systems referred to in annex iii,5,1
3646,3646,access to data and documentation in the context of their activities the market surveillance authorities shall be granted full access to the training validation and testing datasets used by the provider including through application programming interfaces api or other appropriate technical means and tools enabling remote access,5,24
3647,3647,where necessary to assess the conformity of the high risk ai system with the requirements set out in title iii chapter  and upon a reasoned request the market surveillance authorities shall be granted access to the source code of the ai system,5,5
3648,3648,national public authorities or bodies which supervise or enforce the respect of obligations under union law protecting fundamental rights in relation to the use of high risk ai systems referred to in annex iii shall have the power to request and access any documentation created or maintained under this regulation when access to that documentation is necessary for the fulfilment of the competences under their mandate within the limits of their jurisdiction,5,5
3649,3649,the relevant public authority or body shall inform the market surveillance authority of the member state concerned of any such request,5,1
3650,3650,by  months after the entering into force of this regulation each member state shall identify the public authorities or bodies referred to in paragraph  and make a list publicly available on the website of the national supervisory authority,5,1
3651,3651,member states shall notify the list to the commission and all other member states and keep the list up to date,5,1
3652,3652,where the documentation referred to in paragraph  is insufficient to ascertain whether a breach of obligations under union law intended to protect fundamental rights has occurred the public authority or body referred to paragraph  may make a reasoned request to the market surveillance authority to organise testing of the highrisk ai system through technical means,5,5
3653,3653,the market surveillance authority shall organise the testing with the close involvement of the requesting public authority or body within reasonable time following the request,5,12
3654,3654,any information and documentation obtained by the national public authorities or bodies referred to in paragraph  pursuant to the provisions of this article shall be treated in compliance with the confidentiality obligations set out in article,5,1
3655,3655,ai systems presenting a risk shall be understood as a product presenting a risk defined in article  point  of regulation eu  insofar as risks to the health or safety or to the protection of fundamental rights of persons are concerned,5,5
3656,3656,where the market surveillance authority of a member state has sufficient reasons to consider that an ai system presents a risk as referred to in paragraph  they shall carry out an evaluation of the ai system concerned in respect of its compliance with all the requirements and obligations laid down in this regulation,5,5
3657,3657,when risks to the protection of fundamental rights are present the market surveillance authority shall also inform the relevant national public authorities or bodies referred to in article,5,12
3658,3658,the relevant operators shall cooperate as necessary with the market surveillance authorities and the other national public authorities or bodies referred to in article,5,1
3659,3659,where in the course of that evaluation the market surveillance authority finds that the ai system does not comply with the requirements and obligations laid down in this regulation it shall without delay require the relevant operator to take all appropriate corrective actions to bring the ai system into compliance to withdraw the ai system from the market or to recall it within a reasonable period commensurate with the nature of the risk as it may prescribe,5,5
3660,3660,the market surveillance authority shall inform the relevant notified body accordingly,5,1
3661,3661,article  of regulation eu  shall apply to the measures referred to in the second subparagraph,5,2
3662,3662,where the market surveillance authority considers that non compliance is not restricted to its national territory it shall inform the commission and the other member states of the results of the evaluation and of the actions which it has required the operator to take,5,1
3663,3663,the operator shall ensure that all appropriate corrective action is taken in respect of all the ai systems concerned that it has made available on the market throughout the union,5,5
3664,3664,where the operator of an ai system does not take adequate corrective action within the period referred to in paragraph  the market surveillance authority shall take all appropriate provisional measures to prohibit or restrict the ai systems being made available on its national market to withdraw the product from that market or to recall it,5,5
3665,3665,that authority shall inform the commission and the other member states without delay of those measures,5,1
3666,3666,the information referred to in paragraph  shall include all available details in particular the data necessary for the identification of the non compliant ai system the origin of the ai system the nature of the non compliance alleged and the risk involved the nature and duration of the national measures taken and the arguments put forward by the relevant operator,5,5
3667,3667,in particular the market surveillance authorities shall indicate whether the non compliance is due to one or more of the followinga failure of the ai system to meet requirements set out in title iii chapter shortcomings in the harmonised standards or common specifications referred to in articles  and  conferring a presumption of conformity,5,5
3668,3668,the market surveillance authorities of the member states other than the market surveillance authority of the member state initiating the procedure shall without delay inform the commission and the other member states of any measures adopted and of any additional information at their disposal relating to the non compliance of the ai system concerned and in the event of disagreement with the notified national measure of their objections,5,1
3669,3669,where within three months of receipt of the information referred to in paragraph  no objection has been raised by either a member state or the commission in respect of a provisional measure taken by a member state that measure shall be deemed justified,5,22
3670,3670,this is without prejudice to the procedural rights of the concerned operator in accordance with article  of regulation eu,5,1
3671,3671,the market surveillance authorities of all member states shall ensure that appropriate restrictive measures are taken in respect of the product concerned such as withdrawal of the product from their market without delay,5,0
3672,3672,where within three months of receipt of the notification referred to in article  objections are raised by a member state against a measure taken by another member state or where the commission considers the measure to be contrary to union law the commission shall without delay enter into consultation with the relevant member state and operator or operators and shall evaluate the national measure,5,1
3673,3673,on the basis of the results of that evaluation the commission shall decide whether the national measure is justified or not within  months from the notification referred to in article  and notify such decision to the member state concerned,5,18
3674,3674,if the national measure is considered justified all member states shall take the measures necessary to ensure that the non compliant ai system is withdrawn from their market and shall inform the commission accordingly,5,5
3675,3675,if the national measure is considered unjustified the member state concerned shall withdraw the measure,5,22
3676,3676,where the national measure is considered justified and the non compliance of the ai system is attributed to shortcomings in the harmonised standards or common specifications referred to in articles  and  of this regulation the commission shall apply the procedure provided for in article  of regulation eu no,5,2
3677,3677,where having performed an evaluation under article  the market surveillance authority of a member state finds that although an ai system is in compliance with this regulation it presents a risk to the health or safety of persons to the compliance with obligations under union or national law intended to protect fundamental rights or to other aspects of public interest protection it shall require the relevant operator to take all appropriate measures to ensure that the ai system concerned when placed on the market or put into service no longer presents that risk to withdraw the ai system from the market or to recall it within a reasonable period commensurate with the nature of the risk as it may prescribe,5,5
3678,3678,the provider or other relevant operators shall ensure that corrective action is taken in respect of all the ai systems concerned that they have made available on the market throughout the union within the timeline prescribed by the market surveillance authority of the member state referred to in paragraph,5,5
3679,3679,the member state shall immediately inform the commission and the other member states,5,1
3680,3680,that information shall include all available details in particular the data necessary for the identification of the ai system concerned the origin and the supply chain of the ai system the nature of the risk involved and the nature and duration of the national measures taken,5,5
3681,3681,the commission shall without delay enter into consultation with the member states and the relevant operator and shall evaluate the national measures taken,5,19
3682,3682,on the basis of the results of that evaluation the commission shall decide whether the measure is justified or not and where necessary propose appropriate measures,5,19
3683,3683,the commission shall address its decision to the member states,5,19
3684,3684,where the market surveillance authority of a member state makes one of the following findings it shall require the relevant provider to put an end to the noncompliance concernedthe conformity marking has been affixed in violation of article the conformity marking has not been affixedthe eu declaration of conformity has not been drawn upthe eu declaration of conformity has not been drawn up correctlythe identification number of the notified body which is involved in the conformity assessment procedure where applicable has not been affixedwhere the non compliance referred to in paragraph  persists the member state concerned shall take all appropriate measures to restrict or prohibit the high risk ai system being made available on the market or ensure that it is recalled or withdrawn from the market,5,1
3685,3685,the commission and the member states shall encourage and facilitate the drawing up of codes of conduct intended to foster the voluntary application to ai systems other than high risk ai systems of the requirements set out in title iii chapter  on the basis of technical specifications and solutions that are appropriate means of ensuring compliance with such requirements in light of the intended purpose of the systems,5,5
3686,3686,the commission and the board shall encourage and facilitate the drawing up of codes of conduct intended to foster the voluntary application to ai systems of requirements related for example to environmental sustainability accessibility for persons with a disability stakeholders participation in the design and development of the ai systems and diversity of development teams on the basis of clear objectives and key performance indicators to measure the achievement of those objectives,5,27
3687,3687,codes of conduct may be drawn up by individual providers of ai systems or by organisations representing them or by both including with the involvement of users and any interested stakeholders and their representative organisations,5,4
3688,3688,codes of conduct may cover one or more ai systems taking into account the similarity of the intended purpose of the relevant systems,5,4
3689,3689,the commission and the board shall take into account the specific interests and needs of the small scale providers and start ups when encouraging and facilitating the drawing up of codes of conduct,5,27
3690,3690,national competent authorities and notified bodies involved in the application of this regulation shall respect the confidentiality of information and data obtained in carrying out their tasks and activities in such a manner as to protect in particularintellectual property rights and confidential business information or trade secrets of a natural or legal person including source code except the cases referred to in article  of directive  on the protection of undisclosed know how and business information trade secrets against their unlawful acquisition use and disclosure apply,5,1
3691,3691,the effective implementation of this regulation in particular for the purpose of inspections investigations or audits public and national security interestsintegrity of criminal or administrative proceedings,5,12
3692,3692,without prejudice to paragraph  information exchanged on a confidential basis between the national competent authorities and between national competent authorities and the commission shall not be disclosed without the prior consultation of the originating national competent authority and the user when high risk ai systems referred to in points   and  of annex iii are used by law enforcement immigration or asylum authorities when such disclosure would jeopardise public and national security interests,5,1
3693,3693,when the law enforcement immigration or asylum authorities are providers of highrisk ai systems referred to in points   and  of annex iii the technical documentation referred to in annex iv shall remain within the premises of those authorities,5,5
3694,3694,those authorities shall ensure that the market surveillance authorities referred to in article  and  as applicable can upon request immediately access the documentation or obtain a copy thereof,5,1
3695,3695,only staff of the market surveillance authority holding the appropriate level of security clearance shall be allowed to access that documentation or any copy thereof,5,12
3696,3696,paragraphs  and  shall not affect the rights and obligations of the commission member states and notified bodies with regard to the exchange of information and the dissemination of warnings nor the obligations of the parties concerned to provide information under criminal law of the member states,5,1
3697,3697,the commission and member states may exchange where necessary confidential information with regulatory authorities of third countries with which they have concluded bilateral or multilateral confidentiality arrangements guaranteeing an adequate level of confidentiality,5,1
3698,3698,in compliance with the terms and conditions laid down in this regulation member states shall lay down the rules on penalties including administrative fines applicable to infringements of this regulation and shall take all measures necessary to ensure that they are properly and effectively implemented,5,1
3699,3699,the penalties provided for shall be effective proportionate and dissuasive,5,22
3700,3700,they shall take into particular account the interests of small scale providers and start up and their economic viability,5,0
3701,3701,the member states shall notify the commission of those rules and of those measures and shall notify it without delay of any subsequent amendment affecting them,5,1
3702,3702,the following infringements shall be subject to administrative fines of up to    eur or if the offender is company up to   of its total worldwide annual turnover for the preceding financial year whichever is highernon compliance with the prohibition of the artificial intelligence practices referred to in article non compliance of the ai system with the requirements laid down in article,5,5
3703,3703,the non compliance of the ai system with any requirements or obligations under this regulation other than those laid down in articles  and  shall be subject to administrative fines of up to    eur or if the offender is a company up to   of its total worldwide annual turnover for the preceding financial year whichever is higher,5,5
3704,3704,the supply of incorrect incomplete or misleading information to notified bodies and national competent authorities in reply to a request shall be subject to administrative fines of up to    eur or if the offender is a company up to   of its total worldwide annual turnover for the preceding financial year whichever is higher,5,10
3705,3705,when deciding on the amount of the administrative fine in each individual case all relevant circumstances of the specific situation shall be taken into account and due regard shall be given to the followingthe nature gravity and duration of the infringement and of its consequenceswhether administrative fines have been already applied by other market surveillance authorities to the same operator for the same infringement,5,1
3706,3706,the size and market share of the operator committing the infringementeach member state shall lay down rules on whether and to what extent administrative fines may be imposed on public authorities and bodies established in that member state,5,1
3707,3707,depending on the legal system of the member states the rules on administrative fines may be applied in such a manner that the fines are imposed by competent national courts of other bodies as applicable in those member states,5,1
3708,3708,the application of such rules in those member states shall have an equivalent effect,5,1
3709,3709,the european data protection supervisor may impose administrative fines on union institutions agencies and bodies falling within the scope of this regulation,5,2
3710,3710,when deciding whether to impose an administrative fine and deciding on the amount of the administrative fine in each individual case all relevant circumstances of the specific situation shall be taken into account and due regard shall be given to the followingthe nature gravity and duration of the infringement and of its consequencesthe cooperation with the european data protection supervisor in order to remedy the infringement and mitigate the possible adverse effects of the infringement including compliance with any of the measures previously ordered by the european data protection supervisor against the union institution or agency or body concerned with regard to the same subject matterany similar previous infringements by the union institution agency or bodythe following infringements shall be subject to administrative fines of up to   eurnon compliance with the prohibition of the artificial intelligence practices referred to in article non compliance of the ai system with the requirements laid down in article,5,2
3711,3711,the non compliance of the ai system with any requirements or obligations under this regulation other than those laid down in articles  and  shall be subject to administrative fines of up to   eur,5,5
3712,3712,before taking decisions pursuant to this article the european data protection supervisor shall give the union institution agency or body which is the subject of the proceedings conducted by the european data protection supervisor the opportunity of being heard on the matter regarding the possible infringement,5,2
3713,3713,the european data protection supervisor shall base his or her decisions only on elements and circumstances on which the parties concerned have been able to comment,5,2
3714,3714,complainants if any shall be associated closely with the proceedings,5,10
3715,3715,the rights of defense of the parties concerned shall be fully respected in the proceedings,5,12
3716,3716,they shall be entitled to have access to the european data protection supervisors file subject to the legitimate interest of individuals or undertakings in the protection of their personal data or business secrets,5,2
3717,3717,funds collected by imposition of fines in this article shall be the income of the general budget of the union,5,0
3718,3718,the power to adopt delegated acts is conferred on the commission subject to the conditions laid down in this article,5,19
3719,3719,the delegation of power referred to in article  article  article  article  and  and article  shall be conferred on the commission for an indeterminate period of time from entering into force of the regulation,5,19
3720,3720,the delegation of power referred to in article  article  article  article  and  and article  may be revoked at any time by the european parliament or by the council,5,2
3721,3721,a decision of revocation shall put an end to the delegation of power specified in that decision,5,22
3722,3722,it shall take effect the day following that of its publication in the official journal of the european union or at a later date specified therein,5,2
3723,3723,it shall not affect the validity of any delegated acts already in force,5,22
3724,3724,as soon as it adopts a delegated act the commission shall notify it simultaneously to the european parliament and to the council,5,2
3725,3725,any delegated act adopted pursuant to article  article  article  article  and  and article  shall enter into force only if no objection has been expressed by either the european parliament or the council within a period of three months of notification of that act to the european parliament and the council or if before the expiry of that period the european parliament and the council have both informed the commission that they will not object,5,2
3726,3726,that period shall be extended by three months at the initiative of the european parliament or of the council,5,2
3727,3727,the commission shall be assisted by a committee,5,19
3728,3728,that committee shall be a committee within the meaning of regulation eu no,5,2
3729,3729,where reference is made to this paragraph article  of regulation eu no  shall apply,5,2
3730,3730,in article  of regulation ec no  the following subparagraph is addedwhen adopting detailed measures related to technical specifications and procedures for approval and use of security equipment concerning artificial intelligence systems in the meaning of regulation eu yyyxx on artificial intelligence of the european parliament and of the council the requirements set out in chapter  title iii of that regulation shall be taken into accountwhen adopting delegated acts pursuant to the first subparagraph concerning artificial intelligence systems which are safety components in the meaning of regulation eu yyyxx on artificial intelligence of the european parliament and of the council the requirements set out in title iii chapter  of that regulation shall be taken into account,5,2
3731,3731,n article  of regulation eu no  the following subparagraph is addedwhen adopting delegated acts pursuant to the first subparagraph concerning artificial intelligence systems which are safety components in the meaning of regulation eu yyyxx on artificial intelligence of the european parliament and of the council the requirements set out in title iii chapter  of that regulation shall be taken into account,5,2
3732,3732,for artificial intelligence systems which are safety components in the meaning of regulation eu yyyxx on artificial intelligence of the european parliament and of the council when carrying out its activities pursuant to paragraph  and when adopting technical specifications and testing standards in accordance with paragraphs  and  the commission shall take into account the requirements set out in title iii chapter  of that regulation,5,5
3733,3733,when adopting delegated acts pursuant to paragraph  and implementing acts pursuant to paragraph  concerning artificial intelligence systems which are safety components in the meaning of regulation eu yyyxx on artificial intelligence of the european parliament and of the council the requirements set out in title iii chapter  of that regulation shall be taken into account,5,5
3734,3734,when adopting delegated acts pursuant to paragraph  concerning artificial intelligence systems which are safety components in the meaning of regulation eu yyyxx on artificial intelligence of the european parliament and of the council  the requirements set out in title iii chapter  of that regulation shall be taken into account,5,2
3735,3735,without prejudice to paragraph  when adopting implementing acts pursuant to paragraph  concerning artificial intelligence systems which are safety components in the meaning of regulation eu yyyxx on artificial intelligence of the european parliament and of the council the requirements set out in title iii chapter  of that regulation shall be taken into account,5,5
3736,3736,when adopting delegated acts pursuant to paragraphs  and  concerning artificial intelligence systems which are safety components in the meaning of regulation eu yyyxx on artificial intelligence the requirements set out in title iii chapter  of that regulation shall be taken into account,5,5
3737,3737,when adopting implementing acts pursuant to paragraph  concerning artificial intelligence systems which are safety components in the meaning of regulation eu yyyxx on artificial intelligence the requirements set out in title iii chapter  of that regulation shall be taken into account,5,5
3738,3738,when adopting delegated acts pursuant to paragraphs  and  concerning artificial intelligence systems which are safety components in the meaning of regulation eu yyyxx on artificial intelligence the requirements set out in title iii chapter  of that regulation shall be taken into accountwhen adopting those implementing acts concerning artificial intelligence systems which are safety components in the meaning of regulation eu yyyxx on artificial intelligence the requirements set out in title iii chapter  of that regulation shall be taken into account,5,5
3739,3739,when adopting delegated acts pursuant to paragraphs  and  concerning artificial intelligence systems which are safety components in the meaning of regulation eu yyyxx on artificial intelligence  the requirements set out in title iii chapter  of that regulation shall be taken into account,5,5
3740,3740,when adopting the implementing acts pursuant to paragraph  concerning artificial intelligence systems which are safety components in the meaning of regulation eu yyyxx on artificial intelligence of the european parliament and of the council the requirements set out in title iii chapter  of that regulation shall be taken into accountthis regulation shall not apply to the ai systems which are components of the largescale it systems established by the legal acts listed in annex ix that have been placed on the market or put into service before  months after the date of application of this regulation referred to in article  unless the replacement or amendment of those legal acts leads to a significant change in the design or intended purpose of the ai system or ai systems concernedhe requirements laid down in this regulation shall be taken into account where applicable in the evaluation of each large scale it systems established by the legal acts listed in annex ix to be undertaken as provided for in those respective acts,5,5
3741,3741,this regulation shall apply to the high risk ai systems other than the ones referred to in paragraph  that have been placed on the market or put into service before date of application of this regulation referred to in article  only if from that date those systems are subject to significant changes in their design or intended purpose,5,5
3742,3742,the commission shall assess the need for amendment of the list in annex iii once a year following the entry into force of this regulation,5,19
3743,3743,by three years after the date of application of this regulation referred to in article  and every four years thereafter the commission shall submit a report on the evaluation and review of this regulation to the european parliament and to the council,5,2
3744,3744,the reports shall be made public,5,24
3745,3745,the reports referred to in paragraph  shall devote specific attention to the following the status of the financial and human resources of the national competent authorities in order to effectively perform the tasks assigned to them under this regulation  the state of penalties and notably administrative fines as referred to in article  applied by member states to infringements of the provisions of this regulation,5,1
3746,3746,within three years after the date of application of this regulation referred to in article  and every four years thereafter the commission shall evaluate the impact and effectiveness of codes of conduct to foster the application of the requirements set out in title iii chapter  and possibly other additional requirements for ai systems other than high risk ai systems,5,19
3747,3747,for the purpose of paragraphs  to  the board the member states and national competent authorities shall provide the commission with information on its request,5,19
3748,3748,in carrying out the evaluations and reviews referred to in paragraphs  to  the commission shall take into account the positions and findings of the board of the european parliament of the council and of other relevant bodies or sources,5,19
3749,3749,the commission shall if necessary submit appropriate proposals to amend this regulation in particular taking into account developments in technology and in the light of the state of progress in the information society,5,19
3750,3750,this regulation shall enter into force on the twentieth day following that of its publication in the official journal of the european union,5,2
3751,3751,this regulation shall apply from  months following the entering into force of the regulation,5,22
3752,3752,by way of derogation from paragraph title iii chapter  and title vi shall apply from three months following the entry into force of this regulationarticle  shall apply from twelve months following the entry into force of this regulation,5,22
3753,3753,the general objective of the intervention is to ensure the proper functioning of the single market by creating the conditions for the development and use of trustworthy artificial intelligence in the union,5,0
3754,3754,specific objective no to set requirements specific to ai systems and obligations on all value chain participants in order to ensure that ai systems placed on the market and used are safe and respect existing law on fundamental rights and union valuesspecific objective no to ensure legal certainty to facilitate investment and innovation in ai by making it clear what essential requirements obligations as well as conformity and compliance procedures must be followed to place or use an ai system in the union marketspecific objective no to enhance governance and effective enforcement of existing law on fundamental rights and safety requirements applicable to ai systems by providing new powers resources and clear rules for relevant authorities on conformity assessment and expost monitoring procedures and the division of governance and supervision tasks between national and eu levelsspecific objective no to facilitate the development of a single market for lawful safe and trustworthy ai applications and prevent market fragmentation by taking eu action to set minimum requirement for ai systems to be placed and used in the union market in compliance with existing law on fundamental rights and safety,5,5
3755,3755,ai suppliers should benefit from a minimal but clear set of requirements creating legal certainty and ensuring access to the entire single market,5,0
3756,3756,ai users should benefit from legal certainty that the high risk ai systems they buy comply with european laws and values,5,2
3757,3757,consumers should benefit by reducing the risk of violations of their safety or fundamental rights,5,0
3758,3758,number of serious incidents or ai performances which constitute a serious incident or a breach of fundamental rights obligations semi annual by fields of applications and calculated a in absolute terms b as share of applications deployed and c as share of citizens concerned,5,24
3759,3759,the regulation should be fully applicable one year and a half after its adoption,5,22
3760,3760,however elements of the governance structure should be in place before then,5,26
3761,3761,in particular member states shall have appointed existing authorities andor established new authorities performing the tasks set out in the legislation earlier and the eu ai board should be set up and effective,5,2
3762,3762,by the time of applicability the european database of ai systems should be fully operative,5,2
3763,3763,in parallel to the adoption process it is therefore necessary to develop the database so that its development has come to an end when the regulation enters into force,5,0
3764,3764,an emerging patchy framework of potentially divergent national rules will hamper the seamless provision of ai systems across the eu and is ineffective in ensuring the safety and protection of fundamental rights and union values across the different member states,5,2
3765,3765,a common eu legislative action on ai could boost the internal market and has great potential to provide european industry with a competitive edge at the global scene and economies of scale that cannot be achieved by individual member states alone,5,2
3766,3766,the e commerce directive ec provides the core framework for the functioning of the single market and the supervision of digital services and sets a basic structure for a general cooperation mechanism among member states covering in principle all requirements applicable to digital services,5,2
3767,3767,the evaluation of the directive pointed to shortcomings in several aspects of this cooperation mechanism including important procedural aspects such as the lack of clear timeframes for response from member states coupled with a general lack of responsiveness to requests from their counterparts,5,1
3768,3768,this has led over the years to a lack of trust between member states in addressing concerns about providers offering digital services cross border,5,1
3769,3769,the evaluation of the directive showed the need to define a differentiated set of rules and requirements at european level,5,2
3770,3770,for this reason the implementation of the specific obligations laid down in this regulation would require a specific cooperation mechanism at eu level with a governance structure ensuring coordination of specific responsible bodies at eu level,5,2
3771,3771,the regulation laying down harmonised rules on artificial intelligence and amending certain union legislative acts defines a new common framework of requirements applicable to ai systems which goes well beyond the framework provided by existing legislation,5,8
3772,3772,for this reason a new national and european regulatory and coordination function needs to be established with this proposal,5,2
3773,3773,as regards possible synergies with other appropriate instruments the role of notifying authorities at national level can be performed by national authorities fulfilling similar functions sunder other eu regulations,5,1
3774,3774,moreover by increasing trust in ai and thus encouraging investment in development and adoption of ai it complements digital europe for which promoting the diffusion of ai is one of five priorities,5,0
3775,3775,the staff will be redeployed,5,22
3776,3776,the other costs will be supported from the dep,5,17
3777,3777,envelope given that the objective of this regulation  ensuring trustworthy ai  contributes directly to one key objective of digital europe  accelerating ai development and deployment in europe,5,2
3778,3778,explanatory memorandum,6,1
3779,3779,context of the proposalreasons for and objectives of the proposalthis explanatory memorandum accompanies the proposal for a directive on adapting non contractual civil liability rules to artificial intelligence ai,6,2
3780,3780,in a representative survey of liability ranked amongst the top three barriers to the use of ai by european companies,6,10
3781,3781,it was cited as the most relevant external obstacle  for companies that are planning tobut have not yet adopted ai,6,0
3782,3782,in her political guidelines commission president ursula von der leyen laid out acoordinated european approach on ai,6,2
3783,3783,in its white paper on ai of  february  thecommission undertook to promote the uptake of ai and to address the risks associated withsome of its uses by fostering excellence and trust,6,2
3784,3784,in the report on ai liability accompanying the white paper the commission identified the specific challenges posed byai to existing liability rules,6,10
3785,3785,in its conclusions on shaping europes digital future of  june the council welcomed the consultation on the policy proposals in the white paper onai and called on the commission to put forward concrete proposals,6,2
3786,3786,on  october  theeuropean parliament adopted a legislative own initiative resolution under article  tfeurequesting the commission to adopt a proposal for a civil liability regime for ai based onarticle  of the treaty on the functioning of the eu tfeu,6,2
3787,3787,current national liability rules in particular based on fault are not suited to handling liabilityclaims for damage caused by ai enabled products and services,6,10
3788,3788,under such rules victimsneed to prove a wrongful action or omission by a person who caused the damage,6,10
3789,3789,the specificcharacteristics of ai including complexity autonomy and opacity the so called black boxeffect may make it difficult or prohibitively expensive for victims to identify the liableperson and prove the requirements for a successful liability claim,6,10
3790,3790,in particular whenclaiming compensation victims could incur very high up front costs and face significantlylonger legal proceedings compared to cases not involving ai,6,10
3791,3791,victims may therefore bedeterred from claiming compensation altogether,6,10
3792,3792,these concerns have also been retained bythe european parliament ep in its resolution of  may  on artificial intelligence in adigital age,6,2
3793,3793,if a victim brings a claim national courts faced with the specific characteristics of ai mayadapt the way in which they apply existing rules on an ad hoc basis to come to a just result forthe victim,6,10
3794,3794,this will cause legal uncertainty,6,10
3795,3795,businesses will have difficulties to predict howthe existing liability rules will be applied and thus to assess and insure their liabilityexposure,6,10
3796,3796,the effect will be magnified for businesses trading across borders as theuncertainty will cover different jurisdictions,6,0
3797,3797,it will particularly affect small and medium sized enterprises smes which cannot rely on in house legal expertise or capital reserves,6,0
3798,3798,national ai strategies show that several member states are considering or even concretelyplanning legislative action on civil liability for ai,6,10
3799,3799,therefore it is expected that if the eudoes not act member states will adapt their national liability rules to the challenges of ai,6,2
3800,3800,this will result in further fragmentation and increased costs for businesses active throughoutthe eu,6,0
3801,3801,the open public consultation informing the impact assessment of this proposal confirmedthe problems explained above,6,17
3802,3802,in the opinion of the public the black box effect can make itdifficult for the victim to prove fault and causality and there may be uncertainty as to how thecourts will interpret and apply existing national liability rules in cases involving ai,6,10
3803,3803,furthermore it showed a public concern as to how legislative action on adapting liabilityrules initiated by individual member states and the ensuing fragmentation would affect thecosts for companies especially smes preventing the uptake of ai union wide,6,1
3804,3804,thus the objective of this proposal is to promote the rollout of trustworthy ai to harvest itsfull benefits for the internal market,6,0
3805,3805,it does so by ensuring victims of damage caused by aiobtain equivalent protection to victims of damage caused by products in general,6,10
3806,3806,it alsoreduces legal uncertainty of businesses developing or using ai regarding their possibleexposure to liability and prevents the emergence of fragmented ai specific adaptations ofnational civil liability rules,6,10
3807,3807,consistency with existing policy provisions in the policy areathis proposal is part of a package of measures to support the roll out of ai in europe byfostering excellence and trust,6,18
3808,3808,this package comprises three complementary work streams a legislative proposal laying down horizontal rules on artificial intelligencesystems ai act a revision of sectoral and horizontal product safety rules eu rules to address liability issues related to ai systems,6,2
3809,3809,in the ai act proposal the commission has proposed rules that seek to reduce risks for safetyand protect fundamental rights,6,19
3810,3810,safety and liability are two sides of the same coin they applyat different moments and reinforce each other,6,10
3811,3811,while rules to ensure safety and protectfundamental rights will reduce risks they do not eliminate those risks entirely,6,3
3812,3812,where such arisk materialises damage may still occur,6,10
3813,3813,in such instances the liability rules of this proposalwill apply,6,10
3814,3814,effective liability rules also provide an economic incentive to comply with safety rules andtherefore contribute to preventing the occurrence of damage,6,10
3815,3815,in addition this proposalcontributes to the enforcement of the requirements for high risk ai systems imposed by theai act because the failure to comply with those requirements constitutes an importantelement triggering the alleviations of the burden of proof,6,5
3816,3816,this proposal is also consistentwith the generaland sectoral product safety proposed rules applicable to ai enabledmachinery productsand radio equipment,6,5
3817,3817,the commission takes a holistic approach in its ai policy to liability by proposingadaptations to the producers liability for defective products under the product liabilitydirective as well as the targeted harmonisation under this proposal,6,10
3818,3818,these two policyinitiatives are closely linked and form a package as claims falling within their scope deal withdifferent types of liability,6,10
3819,3819,the product liability directive covers producers no fault liabilityfor defective products leading to compensation for certain types of damages mainly sufferedby individuals,6,10
3820,3820,this proposal covers national liability claims mainly based on the fault of anyperson with a view of compensating any type of damage and any type of victim,6,10
3821,3821,theycomplement one another to form an overall effective civil liability system,6,10
3822,3822,together these rules will promote trust in ai and other digital technologies by ensuring thatvictims are effectively compensated if damage occurs despite the preventive requirements ofthe ai act and other safety rules,6,11
3823,3823,consistency with other union policiesthe proposal is coherent with the unions overall digital strategy as it contributes topromoting technology that works for people one of the three main pillars of the policyorientation and objectives announced in the communication shaping europes digitalfuture,6,2
3824,3824,in this context this proposal aims to build trust in the ai and to increase its uptake,6,0
3825,3825,this willachieve synergies and is complementary with the cyber resilience act which also aims toincrease trust in products with digital elements by reducing cyber vulnerabilities and to betterprotect business and consumer users,6,0
3826,3826,this proposal does not affect the rules set by the digital services act dsa which providefor a comprehensive and fully harmonised framework for due diligence obligations foralgorithmic decision making by online platforms including its exemption of liability forproviders of intermediary services,6,23
3827,3827,in addition by promoting the roll out of ai this proposal is linked to the initiatives under theeu strategy for data,6,13
3828,3828,it also strengthens the unions role to help shape global norms andstandards and promote trustworthy ai that is consistent with union values and interests,6,1
3829,3829,the proposal also has indirect links with the european green deal,6,2
3830,3830,in particular digitaltechnologies including ai are a critical enabler for attaining the sustainability goals of thegreen deal in many different sectors including healthcare transport environment andfarming,6,4
3831,3831,main economic social and environmental impactsthe directive will contribute to the rollout of ai,6,17
3832,3832,the conditions for the roll out anddevelopment of ai technologies in the internal market can be significantly improved bypreventing fragmentation and increasing legal certainty through harmonised measures at eulevel compared to possible adaptations of liability rules at national level,6,10
3833,3833,the economicstudyunderpinning the impact assessment of this proposal concluded  as a conservativeestimate  that targeted harmonisation measures on civil liability for ai would have a positiveimpact of  to   on the production value of relevant cross border trade as compared to thebaseline scenario,6,10
3834,3834,this added value would be generated notably through reducedfragmentation and increased legal certainty regarding stakeholders liability exposure,6,10
3835,3835,thiswould lower stakeholders legal informationrepresentation internal risk management andcompliance costs facilitate financial planning as well as risk estimates for insurance purposesand enable companies  in particular smes  to explore new markets across borders,6,0
3836,3836,basedon the overall value of the eu ai market affected by the liability related problems addressedby this directive it is estimated that the latter will generate an additional market valuebetween eur mln and eur bln,6,2
3837,3837,in terms of social impacts the directive will increase societal trust in ai technologies andaccess to an effective justice system,6,17
3838,3838,it will contribute to an efficient civil liability regimeadapted to the specificities of ai where justified claims for compensation of damage aresuccessful,6,10
3839,3839,increasing societal trust would also benefit all companies in the ai value chainbecause strengthening citizens confidence will contribute to a faster uptake of ai,6,0
3840,3840,due to theincentivising effect of liability rules preventing liability gaps would also indirectly benefit allcitizens through an increased level of protection of health and safety article  tfeuand the obviation of sources of health risks article  tfeu,6,10
3841,3841,as regards environmental impacts the directive is also expected to contribute to achievingthe related sustainable development goals sdgs and targets,6,17
3842,3842,the uptake of ai applicationsis beneficial for the environment,6,4
3843,3843,for instance ai systems used in process optimisation makeprocesses less wasteful eg,6,4
3844,3844,by reducing the amount of fertilizers and pesticides neededdecreasing the water consumption at equal output etc,6,17
3845,3845,the directive would also impactpositively on sdgs because effective legislation on transparency accountability andfundamental rights will direct ais potential to benefit individuals and society towardsachieving the sdgs,6,17
3846,3846,legal basis subsidiarity and proportionalitylegal basisthe legal basis for the proposal is article  tfeu which provides for the adoption ofmeasures to ensure the establishment and functioning of the internal market,6,0
3847,3847,the problems this proposal aims to address in particular legal uncertainty and legalfragmentation hinder the development of the internal market and thus amount to significantobstacles to cross border trade in ai enabled products and services,6,0
3848,3848,the proposal addresses obstacles stemming from the fact that businesses that want to producedisseminate and operate ai enabled products and services across borders are uncertainwhether and how existing liability regimes apply to damage caused by ai,6,2
3849,3849,this uncertaintyconcerns particularly member states where businesses will export to or operate their productsand services,6,0
3850,3850,in a cross border context the law applicable to a non contractual liability arisingout of a tort or delict is by default the law of the country in which the damage occurs,6,10
3851,3851,forthese businesses it is essential to know the relevant liability risks and to be able to insurethemselves against them,6,10
3852,3852,in addition there are concrete signs that a number of member states are considering unilaterallegislative measures to address the specific challenges posed by ai with respect to liability,6,10
3853,3853,for example ai strategies adopted in czechia italy malta polandand portugalmention initiatives to clarify liability,6,10
3854,3854,given the large divergence between member statesexisting civil liability rules it is likely that any national ai specific measure on liabilitywould follow existing different national approaches and therefore increase fragmentation,6,10
3855,3855,therefore adaptations of liability rules taken on a purely national basis would increase thebarriers to the rollout of ai enabled products and services across the internal market andcontribute further to fragmentation,6,10
3856,3856,subsidiaritythe objectives of this proposal cannot be adequately achieved at national level becauseemerging divergent national rules would increase legal uncertainty and fragmentationcreating obstacles to the rollout of ai enabled products and services across the internalmarket,6,8
3857,3857,legal uncertainty would particularly affect companies active cross borders byimposing the need for additional legal informationrepresentation risk management costs andforegone revenue,6,0
3858,3858,at the same time differing national rules on compensation claims fordamage caused by ai would increase transaction costs for businesses especially for crossborder trade entailing significant internal market barriers,6,0
3859,3859,further legal uncertainty andfragmentation disproportionately affect start ups and smes which account for mostcompanies and the major share of investments in the relevant markets,6,0
3860,3860,in the absence of eu harmonised rules for compensating damage caused by ai systemsproviders operators and users of ai systems on the one hand and injured persons on the otherhand would be faced with  different liability regimes leading to different levels ofprotection and distorted competition among businesses from different member states,6,10
3861,3861,harmonised measures at eu level would significantly improve conditions for the rollout anddevelopment of ai technologies in the internal market by preventing fragmentation andincreasing legal certainty,6,2
3862,3862,this added value would be generated notably through reducedfragmentation and increased legal certainty regarding stakeholders liability exposure,6,10
3863,3863,moreover only eu action can consistently achieve the desired effect of promoting consumertrust in ai enabled products and services by preventing liability gaps linked to the specificcharacteristics of ai across the internal market,6,2
3864,3864,this would ensure a consistent minimumlevel of protection for all victims individuals and companies and consistent incentives toprevent damage and ensure accountability,6,10
3865,3865,proportionalitythe proposal is based on a staged approach,6,0
3866,3866,in the first stage the objectives are achieved witha minimally invasive approach the second stage involves re assessing the need for morestringent or extensive measures,6,17
3867,3867,the first stage is limited to the burden of proof measures to address the ai specific problemsidentified,6,11
3868,3868,it builds on the substantive conditions of liability currently existing in nationalrules such as causality or fault but focuses on targeted proof related measures ensuring thatvictims have the same level of protection as in cases not involving ai systems,6,10
3869,3869,moreoverfrom the various tools available in national law for easing the burden of proof this proposalhas chosen to use rebuttable presumptions as the least interventionist tool,6,10
3870,3870,such presumptionsare commonly found in national liability systems and they balance the interests of claimantsand defendants,6,10
3871,3871,at the same time they are designed to incentivise compliance with existingduties of care set at union or national level,6,27
3872,3872,the proposal does not lead to a reversal of theburden of proof to avoid exposing providers operators and users of ai systems to higherliability risks which may hamper innovation and reduce the uptake of ai enabled productsand services,6,2
3873,3873,the second stage included in the proposal ensures that when assessing the effect of the firststage in terms of victim protection and uptake of ai future technological regulatory andjurisprudential developments will be taken into account when re assessing the need toharmonise other elements of the claims for compensation or other tools related to liabilityclaims including for situations where strict liability would be more appropriate as requestedby the european parliament,6,10
3874,3874,such assessment would also likely consider whether such aharmonisation would need to be coupled with mandatory insurance to ensure effectiveness,6,17
3875,3875,choice of instrumenta directive is the most suitable instrument for this proposal as it provides the desiredharmonisation effect and legal certainty while also providing the flexibility to enable memberstates to embed the harmonised measures without friction into their national liability regimes,6,10
3876,3876,a mandatory instrument would prevent protection gaps stemming from partial or noimplementation,6,11
3877,3877,while a non binding instrument would be less intrusive it is unlikely toaddress the identified problems in an effective manner,6,25
3878,3878,the implementation rate of non binding instruments is difficult to predict and there is insufficient indication that thepersuasive effect of a recommendation would be strong enough to produce consistentadaptation of national laws,6,8
3879,3879,this effect is even more unlikely for measures concerning private law of which noncontractual liability rules form part,6,10
3880,3880,this area is characterised by long standing legaltraditions which makes member states reluctant to pursue coordinated reform unless drivenby the clear prospect of internal market benefits under a binding eu instrument or the need toadapt to new technologies in the digital economy,6,2
3881,3881,the existing significant divergences between member states liability frameworks are anotherreason why a recommendation is unlikely to be implemented in a consistent manner,6,10
3882,3882,results of ex post evaluations stakeholderconsultations and impact assessments stakeholder consultationsan extensive consultation strategy was implemented to ensure a wide participation ofstakeholders throughout the policy cycle of this proposal,6,17
3883,3883,the consultation strategy was basedon both public and several targeted consultations webinars bilateral discussions withcompanies and various organisations,6,27
3884,3884,after the initial questions on liability which were part of the public consultation on the whitepaper on ai and the commission report on safety and liability a dedicated online publicconsultation was open from  october  to  january  to gather views from a widevariety of stakeholders including consumers civil society organisations industryassociations businesses including smes and public authorities,6,7
3885,3885,after analysing all theresponses received the commission published a summary outcome and the individualresponses on its website,6,19
3886,3886,in total  responses were received from respondents from  member states as well asfrom third countries,6,24
3887,3887,overall the majority of stakeholders confirmed the problems withburden of proof legal uncertainty and fragmentation and supported action at eu level,6,10
3888,3888,eu citizens consumer organizations and academic institutions overwhelmingly confirmed theneed for eu action to ease victims problems with the burden of proof,6,2
3889,3889,businesses whilerecognising the negative effects of the uncertainty around the application of liability ruleswere more cautious and asked for targeted measures to avoid limiting innovation,6,0
3890,3890,a similar picture appeared regarding the policy options,6,22
3891,3891,eu citizens consumer organizationsand academic institutions strongly supported measures on the burden of proof andharmonising no fault liability referred to as strict liability coupled with mandatoryinsurance,6,10
3892,3892,businesses were more divided on the policy options with differences depending inpart on their size,6,0
3893,3893,strict liability was considered disproportionate by the majority of businessrespondents,6,10
3894,3894,harmonisation of the easing of the burden of proof gained more supportparticularly among smes,6,0
3895,3895,however businesses cautioned against a complete shift of theburden of proof,6,0
3896,3896,therefore the preferred policy option was developed and refined in light of feedback receivedfrom stakeholders throughout the impact assessment process to strike a balance between theneeds expressed and concerns raised by all relevant stakeholder groups,6,17
3897,3897,collection and use of expertisethe proposal builds on  years of analysis and close involvement of stakeholders includingacademics businesses consumer associations member states and citizens,6,17
3898,3898,the preparatorywork started in  with the setting up of the expert group on liability and newtechnologies new technologies formation,6,10
3899,3899,the expert group produced a report innovember that assessed the challenges some characteristics of ai pose to national civilliability rules,6,8
3900,3900,the input from the expert group report was complemented by three additional externalstudies a comparative law study based on a comparative legal analysis of european tortlaws focused on key ai related issues a behavioural economics study on the impacts of targeted adaptations of theliability regime on consumers decision making in particular their trust andwillingness to take up ai enabled products and services an economic study covering a number of issues the challenges faced byvictims of ai applications compared to victims of non ai devices when tryingto obtain compensation for their loss whether and to what extent businessesare uncertain about the application of current liability rules to their operationsinvolving ai and whether the impact of legal uncertainty can hamperinvestment in ai whether further fragmentation of national liability lawswould reduce the effectiveness of the internal market for ai applications andservices and whether and to what extent harmonising certain aspects ofnational civil liability via eu legislation would reduce these problems andfacilitate the overall uptake of ai technology by eu companies,6,10
3901,3901,impact assessmentin line with its better regulation policy the commission conducted an impact assessmentfor this proposal examined by the commissions regulatory scrutiny board,6,17
3902,3902,the meeting ofthe regulatory scrutiny board on  april  led to a positive opinion with comments,6,0
3903,3903,three policy options were assessedpolicy option  three measures to ease the burden of proof for victims trying to prove theirliability claim,6,10
3904,3904,policy option  the measures under option   harmonising strict liability rules for ai usecases with a particular risk profile coupled with a mandatory insurance,6,21
3905,3905,policy option  a staged approach consisting of a first stage the measures under option  a second stage a review mechanism to re assess in particular the need forharmonising strict liability for ai use cases with a particular risk profilepossibly coupled with a mandatory insurance,6,21
3906,3906,the policy options were compared by way of a multi criteria analysis taking into accounttheir effectiveness efficiency coherence and proportionality,6,17
3907,3907,the results of the multi criteriaand sensitivity analysis show that policy option  easing the burden of proof for ai relatedclaims  targeted review regarding strict liability possibly coupled with mandatory insuranceranks highest and is therefore the preferred policy choice for this proposal,6,10
3908,3908,the preferred policy option would ensure that victims of ai enabled products and servicesnatural persons businesses and any other public or private entities are no less protected thanvictims of traditional technologies,6,2
3909,3909,it would increase the level of trust in ai and promote itsuptake,6,0
3910,3910,furthermore it would reduce legal uncertainty and prevent fragmentation thus helpingcompanies and most of all smes that want to realise the full potential of the eu singlemarket by rolling out ai enabled products and services cross border,6,0
3911,3911,the preferred policyoption also creates better conditions for insurers to offer coverage of ai related activitieswhich is crucial for businesses especially smes to manage their risks,6,0
3912,3912,it is namely estimatedthat the preferred policy option would generate an increased ai market value in the eu between eur mln and eur bln in,6,2
3913,3913,fundamental rightsone of the most important functions of civil liability rules is to ensure that victims of damagecan claim compensation,6,10
3914,3914,by guaranteeing effective compensation these rules contribute tothe protection of the right to an effective remedy and a fair trial article  of the eu charterof fundamental rights referred to below as the charter while also giving potentially liablepersons an incentive to prevent damage in order to avoid liability,6,10
3915,3915,with this proposal the commission aims to ensure that victims of damage caused by ai havean equivalent level of protection under civil liability rules as victims of damage causedwithout the involvement of ai,6,10
3916,3916,the proposal will enable effective private enforcement offundamental rights and preserve the right to an effective remedy where ai specific risks havematerialised,6,2
3917,3917,in particular the proposal will help protect fundamental rights such as the rightto life article  of the charter the right to the physical and mental integrity article  andthe right to property article,6,12
3918,3918,in addition depending on each member states civil lawsystem and traditions victims will be able to claim compensation for damage to other legalinterests such as violations of personal dignity articles  and  of the charter respect forprivate and family life article  the right to equality article  and non discriminationarticle,6,10
3919,3919,in addition this proposal complements other strands in the commissions ai policy based onpreventive regulatory and supervisory requirements aimed directly at avoiding fundamentalrights breaches such as discrimination,6,8
3920,3920,these are the ai act the general data protectionregulation the digital services act and eu law on non discrimination and equal treatment,6,11
3921,3921,at the same time this proposal does not create or harmonise the duties of care or the liabilityof various entities whose activity is regulated under those legal acts and therefore does notcreate new liability claims or affect the exemptions from liability under those other legal acts,6,1
3922,3922,this proposal only introduces alleviations of the burden of proof for the victims of damagecaused by ai systems in claims that can be based on national law or on these other eu laws,6,10
3923,3923,by complementing these other strands this proposal protects the victims right tocompensation under private law including compensation for fundamental rights breaches,6,10
3924,3924,budgetary implicationsthis proposal will not have implications for the budget of the european union,6,2
3925,3925,other elementsimplementation plans and monitoring evaluation monitoring programme andtargeted reviewthis proposal puts forward a staged approach,6,17
3926,3926,to ensure that sufficient evidence is availablefor the targeted review in the second stage the commission will draw up a monitoring plandetailing how and how often data and other necessary evidence will be collected,6,19
3927,3927,the monitoring mechanism could cover the following types of data and evidence reporting and information sharing by member states regarding application ofmeasure to ease the burden of proof in national judicial or out of courtsettlement procedures information collected by the commission or market surveillance authoritiesunder the ai act in particular article  or other relevant instruments information and analyses supporting the evaluation of the ai act and thereports to be prepared by the commission on implementation of that act information and analyses supporting the assessment of relevant future policymeasures under the old approach safety legislation to ensure that productsplaced on the union market meet high health safety and environmentalrequirements information and analyses supporting the commissions report on theapplication of the motor insurance directive to technological developments inparticular autonomous and semi autonomous vehicles pursuant to itsarticle c,6,1
3928,3928,detailed explanation of the specific provisions in the proposal,6,27
3929,3929,subject matter and scope article the purpose of this directive is to improve the functioning of the internal market by layingdown uniform requirements for certain aspects of non contractual civil liability for damagecaused with the involvement of ai systems,6,10
3930,3930,it follows up on the european parliamentsresolution inl and adapts private law to the needs of the transition to the digitaleconomy,6,2
3931,3931,the choice of suitable legal tools is limited given the nature of the burden of proof issue andthe specific characteristics of ai that pose a problem for existing liability rules,6,10
3932,3932,in this respectthis directive eases the burden of proof in a very targeted and proportionate manner throughthe use of disclosure and rebuttable presumptions,6,10
3933,3933,it establishes for those seekingcompensation for damage a possibility to obtain information on high risk ai systems to berecordeddocumented pursuant to the ai act,6,5
3934,3934,in addition to this the rebuttable presumptionswill give those seeking compensation for damage caused by ai systems a more reasonableburden of proof and a chance to succeed with justified liability claims,6,10
3935,3935,such tools are not new they can be found in national legislative systems,6,15
3936,3936,hence thesenational tools constitute helpful reference points on how to address the issues raised by ai forexisting liability rules in a way which interferes as little as possible with the different nationallegal regimes,6,10
3937,3937,in addition when asked about more far reaching changes such as a reversal of the burden ofproof or an irrebuttable presumption businesses provided negative feedback in consultations,6,22
3938,3938,targeted measures to ease the burden of proof in form of rebuttable presumptions werechosen as pragmatic and appropriate ways to help victims meet their burden of proof in themost targeted and proportionate manner possible,6,10
3939,3939,article  indicates the subject matter and scope of this directive it applies to non contractualcivil law claims for damages caused by an ai system where such claims are brought underfault based liability regimes,6,10
3940,3940,this means namely regimes that provide for a statutoryresponsibility to compensate for damage caused intentionally or by a negligent act oromission,6,10
3941,3941,the measures provided in this directive can fit without friction in existing civilliability systems since they reflect an approach that does not touch on the definition offundamental concepts like fault or damage given that the meaning of those conceptsvaries considerably across the member states,6,10
3942,3942,thus beyond the presumptions it establishesthis directive does not affect union or national rules determining for instance which partyhas the burden of proof what degree of certainty is required as regards the standard of proofor how fault is defined,6,10
3943,3943,in addition this directive does not affect existing rules regulating the conditions of liability inthe transport sector and those set by the digital services act,6,1
3944,3944,while this directive does not apply with respect to criminal liability it may be applicablewith respect to state liability,6,10
3945,3945,state authorities are also covered by the provisions of the ai actas subjects of the obligations prescribed therein,6,1
3946,3946,this directive does not apply retroactively but only to claims for compensation of damagesthat occur as from the date of its transposition,6,10
3947,3947,the proposal for this directive has been adopted together with the proposal for a revision ofthe product liability directive eec in a package aiming to adapt liability rules to thedigital age and ai ensuring the necessary alignment between these two complementary legalinstruments,6,10
3948,3948,definitions article the definitions in article  follow those of the ai act to ensure consistency,6,1
3949,3949,article  provides that claims for damages can be brought not only by the injured personbut also by persons that have succeeded in or have been subrogated into the injured personsrights,6,10
3950,3950,subrogation is the assumption by a third party such as an insurance company ofanother partys legal right to collect a debt or damages,6,10
3951,3951,thus one person is entitled to enforcethe rights of another for their own benefit,6,16
3952,3952,subrogation would also cover heirs of a deceasedvictim,6,26
3953,3953,in addition article  provides that an action for damages can also be brought bysomeone acting on behalf of one or more injured parties in accordance with union or nationallaw,6,10
3954,3954,this provision aims to give more possibilities to persons injured by an ai system to havetheir claims assessed by a court even in cases where individual actions may seem too costlyor too cumbersome to bring or where joint actions may entail a benefit of scale,6,10
3955,3955,to enablevictims of damage caused by ai systems to enforce their rights in relation to this directivethrough representative actions article  amends annex i to directive eu,6,2
3956,3956,disclosure of evidence article this directive aims to provide persons seeking compensation for damage caused by high riskai systems with effective means to identify potentially liable persons and relevant evidencefor a claim,6,10
3957,3957,at the same time such means serve to exclude falsely identified potentialdefendants saving time and costs for the parties involved and reducing the case load forcourts,6,10
3958,3958,in this respect article  of the directive provides that a court may order the disclosure ofrelevant evidence about specific high risk ai systems that are suspected of having causeddamage,6,10
3959,3959,requests for evidence are addressed to the provider of an ai system a person who issubject to the providers obligations laid down by article  or article   of the ai act ora user pursuant to the ai act,6,10
3960,3960,the requests should be supported by facts and evidencesufficient to establish the plausibility of the contemplated claim for damages and therequested evidence should be at the addressees disposal,6,10
3961,3961,requests cannot be addressed toparties that bear no obligations under the ai act and therefore have no access to the evidence,6,2
3962,3962,according to article  the claimant can request the disclosure of evidence from providersor users that are not defendants only in case all proportionate attempts were unsuccessfullymade to gather the evidence from the defendant,6,10
3963,3963,in order for the judicial means to be effective article  of the directive provides that acourt may also order the preservation of such evidence,6,19
3964,3964,as provided in article  first subparagraph the court may order such disclosure only tothe extent necessary to sustain the claim given that the information could be critical evidenceto the injured persons claim in the case of damage that involve ai systems,6,10
3965,3965,by limiting the obligation to disclose or preserve to necessary and proportionate evidencearticle   first subparagraph aims to ensure proportionality in disclosing evidence ie,6,19
3966,3966,tolimit the disclosure to the necessary minimum and prevent blanket requests,6,14
3967,3967,the second and third subparagraphs of article  further aim to strike a balance between theclaimants rights and the need to ensure that such disclosure would be subject to safeguards toprotect the legitimate interests of all parties concerned such as trade secrets or confidentialinformation,6,19
3968,3968,in the same context the fourth subparagraph of article  aims to ensure that proceduralremedies against the order of disclosure or preservation are at the disposal of the personsubject to it,6,19
3969,3969,article  introduces a presumption of non compliance with a duty of care,6,1
3970,3970,this is aprocedural tool relevant only in cases where it is the actual defendant in a claim for damageswho bears the consequences of not complying with a request to disclose or preserve evidence,6,10
3971,3971,the defendant will have the right to rebut that presumption,6,10
3972,3972,the measure set out in thisparagraph aims to promote disclosure but also to expedite court proceedings,6,19
3973,3973,presumption of causal link in the case of fault article with respect to damage caused by ai systems this directive aims to provide an effectivebasis for claiming compensation in connection with the fault consisting in the lack ofcompliance with a duty of care under union or national law,6,10
3974,3974,it can be challenging for claimants to establish a causal link between such non complianceand the output produced by the ai system or the failure of the ai system to produce an outputthat gave rise to the relevant damage,6,10
3975,3975,therefore a targeted rebuttable presumption ofcausality has been laid down in article   regarding this causal link,6,10
3976,3976,such presumption isthe least burdensome measure to address the need for fair compensation of the victim,6,10
3977,3977,the fault of the defendant has to be proven by the claimant according to the applicable unionor national rules,6,10
3978,3978,such fault can be established for example for non compliance with a dutyof care pursuant to the ai act or pursuant to other rules set at union level such as thoseregulating the use of automated monitoring and decision making for platform work or thoseregulating the operation of unmanned aircraft,6,10
3979,3979,such fault can also be presumed by the courton the basis of a non compliance with a court order for disclosure or preservation of evidenceunder article,6,10
3980,3980,still it is only appropriate to introduce a presumption of causality when itcan be considered likely that the given fault has influenced the relevant ai system output orlack thereof which can be assessed on the basis of the overall circumstances of the case,6,10
3981,3981,atthe same time the claimant still has to prove that the ai system ie,6,10
3982,3982,its output or failure toproduce one gave rise to the damage,6,10
3983,3983,paragraphs  and  differentiate between on the one hand claims brought against theprovider of a high risk ai system or against a person subject to the providers obligationsunder the ai act and on the other hand claims brought against the user of such systems,6,10
3984,3984,inthis respect it follows the respective provisions and relevant conditions of the ai act,6,5
3985,3985,in thecase of claims based on article  the defendants compliance with the obligations listed inthat paragraph have to be assessed also in the light of the risk management system and itsresults ie,6,10
3986,3986,risk management measures under the ai act,6,3
3987,3987,in case of high risk ai systems as defined by the ai act article  establishes an exceptionfrom the presumption of causality where the defendant demonstrates that sufficient evidenceand expertise is reasonably accessible for the claimant to prove the causal link,6,10
3988,3988,thispossibility can incentivise defendants to comply with their disclosure obligations withmeasures set by the ai act to ensure a high level of transparency of the ai or withdocumenting and recording requirements,6,10
3989,3989,in the case of non high risk ai systems article  establishes a condition for theapplicability of the presumption of causality whereby the latter is subject to the courtdetermining that it is excessively difficult for the claimant to prove the causal link,6,10
3990,3990,suchdifficulties are to be assessed in light of the characteristics of certain ai systems such asautonomy and opacity which render the explanation of the inner functioning of the ai systemvery difficult in practice negatively affecting the ability of the claimant to prove the causallink between the fault of the defendant and the ai output,6,10
3991,3991,in cases where the defendant uses the ai system in the course of a personal non professionalactivity article  provides that the presumption of causality should only apply if thedefendant has materially interfered with the conditions of the operation of the ai system or ifthe defendant was required and able to determine the conditions of operation of the ai systemand failed to do so,6,10
3992,3992,this condition is justified by the need to balance the interests of injuredpersons and non professional users by exempting from the application of the presumption ofcausality the cases in which non professional users do not add risk through their behaviour,6,10
3993,3993,finally article  provides that the defendant has the right to rebut the causalitypresumption based on article,6,10
3994,3994,such effective civil liability rules have the additional advantage that they give all thoseinvolved in activities related to ai systems an additional incentive to respect their obligationsregarding their expected conduct,6,10
3995,3995,evaluation and targeted review article various national legal systems provide for different strict liability regimes,6,10
3996,3996,elements for sucha regime at union level were also suggested by the european parliament in its own initiativeresolution of  october  consisting of a limited strict liability regime for certain ai enabled technologies and a facilitated burden of proof under fault based liability rules,6,2
3997,3997,thepublic consultations also highlighted a preference for such a regime among respondentsexcept for non smes businesses whether or not coupled with mandatory insurance,6,27
3998,3998,however the proposal takes into account the differences between national legal traditions andthe fact that the kind of products and services equipped with ai systems that could affect thepublic at large and put at risk important legal rights such as the right to life health andproperty and therefore could be subject to a strict liability regime are not yet widelyavailable on the market,6,5
3999,3999,a monitoring programme is put in place to provide the commission with information onincidents involving ai systems,6,4
4000,4000,the targeted review will assess whether additional measureswould be needed such as introducing a strict liability regime andor mandatory insurance,6,17
4001,4001,transposition article when notifying the commission of national transposition measures to comply with thisdirective member states should also provide explanatory documents which give sufficientlyclear and precise information and state for each provision of this directive the nationalprovisions ensuring its transposition,6,1
4002,4002,this is necessary to enable the commission to identifyfor each provision of the directive requiring transposition the relevant part of nationaltransposition measures creating the corresponding legal obligation in the national legal orderwhatever the form chosen by the member states,6,19
4003,4003,codproposal for adirective of the european parliament and of the councilon adapting non contractual civil liability rules to artificial intelligenceai liability directivetext with eea relevancethe european parliament and the council of the european unionhaving regard to the treaty on the functioning of the european union and in particulararticle  thereofhaving regard to the proposal from the european commissionafter transmission of the draft legislative act to the national parliamentshaving regard to the opinion of the european economic and social committeehaving regard to the opinion of the committee of the regionsacting in accordance with the ordinary legislative procedurewhereas artificial intelligence ai is a set of enabling technologies which can contribute to awide array of benefits across the entire spectrum of the economy and society,6,2
4004,4004,it has alarge potential for technological progress and allows new business models in manysectors of the digital economy,6,0
4005,4005,at the same time depending on the circumstances of its specific application and useai can generate risks and harm interests and rights that are protected by union ornational law,6,0
4006,4006,for instance the use of ai can adversely affect a number of fundamentalrights including life physical integrity and in respect to non discrimination and equaltreatment,6,4
4007,4007,regulation eu of the european parliament and of the council theai actprovides for requirements intended to reduce risks to safety and fundamentalrights while other union law instruments regulate generaland sectoral productsafety rules applicable also to ai enabled machinery productsand radio equipment,6,2
4008,4008,while such requirements intended to reduce risks to safety and fundamental rights aremeant to prevent monitor and address risks and thus address societal concerns theydo not provide individual relief to those that have suffered damage caused by ai,6,5
4009,4009,existing requirements provide in particular for authorisations checks monitoring andadministrative sanctions in relation to ai systems in order to prevent damage,6,5
4010,4010,they donot provide for compensation of the injured person for damage caused by an output orthe failure to produce an output by an ai system,6,10
4011,4011,when an injured person seeks compensation for damage suffered member statesgeneral fault based liability rules usually require that person to prove a negligent orintentionally damaging act or omission fault by the person potentially liable for thatdamage as well as a causal link between that fault and the relevant damage,6,10
4012,4012,howeverwhen ai is interposed between the act or omission of a person and the damage thespecific characteristics of certain ai systems such as opacity autonomous behaviourand complexity may make it excessively difficult if not impossible for the injuredperson to meet this burden of proof,6,10
4013,4013,in particular it may be excessively difficult toprove that a specific input for which the potentially liable person is responsible hadcaused a specific ai system output that led to the damage at stake,6,10
4014,4014,in such cases the level of redress afforded by national civil liability rules may belower than in cases where technologies other than ai are involved in causing damage,6,10
4015,4015,such compensation gaps may contribute to a lower level of societal acceptance of aiand trust in ai enabled products and services,6,18
4016,4016,to reap the economic and societal benefits of ai and promote the transition to thedigital economy it is necessary to adapt in a targeted manner certain national civilliability rules to those specific characteristics of certain ai systems,6,4
4017,4017,such adaptationsshould contribute to societal and consumer trust and thereby promote the roll out ofai,6,0
4018,4018,such adaptations should also maintain trust in the judicial system by ensuring thatvictims of damage caused with the involvement of ai have the same effectivecompensation as victims of damage caused by other technologies,6,10
4019,4019,interested stakeholders  injured persons suffering damage potentially liable personsinsurers  face legal uncertainty as to how national courts when confronted with thespecific challenges of ai might apply the existing liability rules in individual cases inorder to achieve just results,6,10
4020,4020,in the absence of union action at least some memberstates are likely to adapt their civil liability rules to address compensation gaps andlegal uncertainty linked to the specific characteristics of certain ai systems,6,10
4021,4021,thiswould create legal fragmentation and internal market barriers for businesses thatdevelop or provide innovative ai enabled products or services,6,0
4022,4022,small and medium sized enterprises would be particularly affected,6,0
4023,4023,the purpose of this directive is to contribute to the proper functioning of the internalmarket by harmonising certain national non contractual fault based liability rules soas to ensure that persons claiming compensation for damage caused to them by an aisystem enjoy a level of protection equivalent to that enjoyed by persons claimingcompensation for damage caused without the involvement of an ai system,6,10
4024,4024,thisobjective cannot be sufficiently achieved by the member states because the relevantinternal market obstacles are linked to the risk of unilateral and fragmented regulatorymeasures at national level,6,26
4025,4025,given the digital nature of the products and services fallingwithin the scope of this directive the latter is particularly relevant in a cross bordercontext,6,23
4026,4026,the objective of ensuring legal certainty and preventing compensation gaps in caseswhere ai systems are involved can thus be better achieved at union level,6,10
4027,4027,thereforethe union may adopt measures in accordance with the principle of subsidiarity as setout in article  teu,6,1
4028,4028,in accordance with the principle of proportionality as set out inthat article this directive does not go beyond what is necessary in order to achievethat objective,6,1
4029,4029,it is therefore necessary to harmonise in a targeted manner specific aspects of fault based liability rules at union level,6,10
4030,4030,such harmonisation should increase legal certaintyand create a level playing field for ai systems thereby improving the functioning ofthe internal market as regards the production and dissemination of ai enabledproducts and services,6,5
4031,4031,to ensure proportionality it is appropriate to harmonise in a targeted manner onlythose fault based liability rules that govern the burden of proof for persons claimingcompensation for damage caused by ai systems,6,10
4032,4032,this directive should not harmonisegeneral aspects of civil liability which are regulated in different ways by national civilliability rules such as the definition of fault or causality the different types of damagethat give rise to claims for damages the distribution of liability over multipletortfeasors contributory conduct the calculation of damages or limitation periods,6,10
4033,4033,the laws of the member states concerning the liability of producers for damagecaused by the defectiveness of their products are already harmonised at union level bycouncil directive eec,6,10
4034,4034,those laws do not however affect member statesrules of contractual or non contractual liability such as warranty fault or strictliability based on other grounds than the defect of the product,6,10
4035,4035,while at the same timethe revision of council directive eec seeks to clarify and ensure that injuredperson can claim compensation for damages caused by defective ai enabled productsit should therefore be clarified that the provisions of this directive do not affect anyrights which an injured person may have under national rules implementing directiveeec,6,10
4036,4036,in addition in the field of transport union law regulating the liability oftransport operators should remain unaffected by this directive,6,1
4037,4037,the digital services act dsa fully harmonises the rules applicable to providersof intermediary services in the internal market covering the societal risks stemmingfrom the services offered by those providers including as regards the ai systems theyuse,6,23
4038,4038,this directive does not affect the provisions of the digital services act dsathat provide a comprehensive and fully harmonised framework for due diligenceobligations for algorithmic decision making by hosting service providers includingthe exemption from liability for the dissemination of illegal content uploaded byrecipients of their services where the conditions of that regulation are met,6,23
4039,4039,other than in respect of the presumptions it lays down this directive does notharmonise national laws regarding which party has the burden of proof or whichdegree of certainty is required as regards the standard of proof,6,26
4040,4040,this directive should follow a minimum harmonisation approach,6,27
4041,4041,such an approachallows claimants in cases of damage caused by ai systems to invoke more favourablerules of national law,6,10
4042,4042,thus national laws could for example maintain reversals of theburden of proof under national fault based regimes or national no fault liabilityreferred to as strict liability regimes of which there are already a large variety innational laws possibly applying to damage caused by ai systems,6,10
4043,4043,consistency with the ai act should also be ensured,6,18
4044,4044,it is therefore appropriate forthis directive to use the same definitions in respect of ai systems providers and users,6,26
4045,4045,in addition this directive should only cover claims for damages when the damage iscaused by an output or the failure to produce an output by an ai system through thefault of a person for example the provider or the user under the ai act,6,10
4046,4046,there is noneed to cover liability claims when the damage is caused by a human assessmentfollowed by a human act or omission while the ai system only provided informationor advice which was taken into account by the relevant human actor,6,10
4047,4047,in the latter caseit is possible to trace back the damage to a human act or omission as the ai systemoutput is not interposed between the human act or omission and the damage andthereby establishing causality is not more difficult than in situations where an aisystem is not involved,6,10
4048,4048,access to information about specific high risk ai systems that are suspected of havingcaused damage is an important factor to ascertain whether to claim compensation andto substantiate claims for compensation,6,10
4049,4049,moreover for high risk ai systems the aiact provides for specific documentation information and logging requirements butdoes not provide a right to the injured person to access that information,6,10
4050,4050,it is thereforeappropriate to lay down rules on the disclosure of relevant evidence by those that haveit at their disposal for the purposes of establishing liability,6,10
4051,4051,this should also providean additional incentive to comply with the relevant requirements laid down in the aiact to document or record the relevant information,6,19
4052,4052,the large number of people usually involved in the design development deploymentand operation of high risk ai systems makes it difficult for injured persons to identifythe person potentially liable for damage caused and to prove the conditions for a claimfor damages,6,10
4053,4053,to allow injured persons to ascertain whether a claim for damages iswell founded it is appropriate to grant potential claimants a right to request a court toorder the disclosure of relevant evidence before submitting a claim for damages,6,10
4054,4054,suchdisclosure should only be ordered where the potential claimant presents facts andinformation sufficient to support the plausibility of a claim for damages and it hasmade a prior request to the provider the person subject to the obligations of a provideror the user to disclose such evidence at their disposal about specific high risk aisystems that are suspected of having caused damage which has been refused,6,10
4055,4055,orderingdisclosure should lead to a reduction of unnecessary litigation and avoid costs for thepossible litigants caused by claims which are unjustified or likely to be unsuccessful,6,10
4056,4056,the refusal of the provider the person subject to the obligations of a provider or theuser prior to the request to the court to disclose evidence should not trigger thepresumption of non compliance with relevant duties of care by the person who refusessuch disclosure,6,23
4057,4057,the limitation of disclosure of evidence as regards high risk ai systems is consistentwith the ai act which provides certain specific documentation record keeping andinformation obligations for operators involved in the design development anddeployment of high risk ai systems,6,5
4058,4058,such consistency also ensures the necessaryproportionality by avoiding that operators of ai systems posing lower or no risk wouldbe expected to document information to a level similar to that required for high risk aisystems under the ai act,6,5
4059,4059,national courts should be able in the course of civil proceedings to order thedisclosure or preservation of relevant evidence related to the damage caused by high risk ai systems from persons who are already under an obligation to document orrecord information pursuant to the ai act be they providers persons under the sameobligations as providers or users of an ai system either as defendants or third partiesto the claim,6,10
4060,4060,there could be situations where the evidence relevant for the case is heldby entities that would not be parties to the claim for damages but which are under anobligation to document or record such evidence pursuant to the ai act,6,10
4061,4061,it is thusnecessary to provide for the conditions under which such third parties to the claim canbe ordered to disclose the relevant evidence,6,10
4062,4062,to maintain the balance between the interests of the parties involved in the claim fordamages and of third parties concerned the courts should order the disclosure ofevidence only where this is necessary and proportionate for supporting the claim orpotential claim for damages,6,10
4063,4063,in this respect disclosure should only concern evidencethat is necessary for a decision on the respective claim for damages for example onlythe parts of the relevant records or data sets required to prove non compliance with arequirement laid down by the ai act,6,10
4064,4064,to ensure the proportionality of suchdisclosure or preservation measures national courts should have effective means tosafeguard the legitimate interests of all parties involved for instance the protection oftrade secrets within the meaning of directive eu  of the europeanparliament and of the council and of confidential information such as informationrelated to public or national security,6,10
4065,4065,in respect of trade secrets or alleged trade secretswhich the court has identified as confidential within the meaning of directive eu national courts should be empowered to take specific measures to ensurethe confidentiality of trade secrets during and after the proceedings while achieving afair and proportionate balance between the trade secret holders interest in maintainingsecrecy and the interest of the injured person,6,2
4066,4066,this should include measures to restrictaccess to documents containing trade secrets and access to hearings or documents andtranscripts thereof to a limited number of people,6,1
4067,4067,when deciding on such measuresnational courts should take into account the need to ensure the right to an effectiveremedy and to a fair trial the legitimate interests of the parties and where appropriateof third parties and any potential harm to either party or where appropriate to thirdparties resulting from the granting or rejection of such measures,6,21
4068,4068,moreover to ensurea proportionate application of a disclosure measure towards third parties in claims fordamages the national courts should order disclosure from third parties only if theevidence cannot be obtained from the defendant,6,10
4069,4069,while national courts have the means of enforcing their orders for disclosure throughvarious measures any such enforcement measures could delay claims for damages andthus potentially create additional expenses for the litigants,6,10
4070,4070,for injured persons suchdelays and additional expenses may make their recourse to an effective judicialremedy more difficult,6,10
4071,4071,therefore where a defendant in a claim for damages fails todisclose evidence at its disposal ordered by a court it is appropriate to lay down apresumption of non compliance with those duties of care which that evidence wasintended to prove,6,10
4072,4072,this rebuttable presumption will reduce the duration of litigationand facilitate more efficient court proceedings,6,10
4073,4073,the defendant should be able to rebutthat presumption by submitting evidence to the contrary,6,10
4074,4074,in order to address the difficulties to prove that a specific input for which thepotentially liable person is responsible had caused a specific ai system output that ledto the damage at stake it is appropriate to provide under certain conditions for apresumption of causality,6,10
4075,4075,while in a fault based claim the claimant usually has toprove the damage the human act or omission constituting fault of the defendant andthe causality link between the two this directive does not harmonise the conditionsunder which national courts establish fault,6,10
4076,4076,they remain governed by the applicablenational law and where harmonised by applicable union law,6,1
4077,4077,similarly thisdirective does not harmonise the conditions related to the damage for instance whatdamages are compensable which are also regulated by applicable national and unionlaw,6,10
4078,4078,for the presumption of causality under this directive to apply the fault of thedefendant should be established as a human act or omission which does not meet aduty of care under union law or national law that is directly intended to protect againstthe damage that occurred,6,10
4079,4079,thus this presumption can apply for example in a claimfor damages for physical injury when the court establishes the fault of the defendantfor non complying with the instructions of use which are meant to prevent harm tonatural persons,6,10
4080,4080,non compliance with duties of care that were not directly intended toprotect against the damage that occurred do not lead to the application of thepresumption for example a providers failure to file required documentation withcompetent authorities would not lead to the application of the presumption in claimsfor damages due to physical injury,6,10
4081,4081,it should also be necessary to establish that it canbe considered reasonably likely based on the circumstances of the case that the faulthas influenced the output produced by the ai system or the failure of the ai system toproduce an output,6,10
4082,4082,finally the claimant should still be required to prove that theoutput or failure to produce an output gave rise to the damage,6,10
4083,4083,such a fault can be established in respect of non compliance with union rules whichspecifically regulate high risk ai systems like the requirements introduced for certainhigh risk ai systems by the ai act requirements which may be introduced by futuresectoral legislation for other high risk ai systems according to article  of the aiact or duties of care which are linked to certain activities and which are applicableirrespective whether ai is used for that activity,6,5
4084,4084,at the same time this directiveneither creates nor harmonises the requirements or the liability of entities whoseactivity is regulated under those legal acts and therefore does not create new liabilityclaims,6,10
4085,4085,establishing a breach of such a requirement that amounts to fault will be doneaccording to the provisions of those applicable rules of union law since thisdirective neither introduces new requirements nor affects existing requirements,6,10
4086,4086,forexample the exemption of liability for providers of intermediary services and the duediligence obligations to which they are subject pursuant to the digital services actare not affected by this directive,6,23
4087,4087,similarly the compliance with requirementsimposed on online platforms to avoid unauthorised communication to the public ofcopyright protected works is to be established under directive eu  oncopyright and related rights in the digital single market and other relevant unioncopyright law,6,7
4088,4088,in areas not harmonised by union law national law continues to apply and fault isestablished under the applicable national law,6,10
4089,4089,all national liability regimes have dutiesof care taking as a standard of conduct different expressions of the principle how areasonable person should act which also ensure the safe operation of ai systems inorder to prevent damage to recognised legal interests,6,10
4090,4090,such duties of care could forinstance require users of ai systems to choose for certain tasks a particular ai systemwith concrete characteristics or to exclude certain segments of a population from beingexposed to a particular ai system,6,4
4091,4091,national law can also introduce specific obligationsmeant to prevent risks for certain activities which are applicable irrespective whetherai is used for that activity for example traffic rules or obligations specificallydesigned for ai systems such as additional national requirements for users of high risk ai systems pursuant to article   of the ai act,6,1
4092,4092,this directive neitherintroduces such requirements nor affects the conditions for establishing fault in case ofbreach of such requirements,6,1
4093,4093,even when fault consisting of a non compliance with a duty of care directly intendedto protect against the damage that occurred is established not every fault should leadto the application of the rebuttable presumption linking it to the output of the ai,6,10
4094,4094,sucha presumption should only apply when it can be considered reasonably likely from thecircumstances in which the damage occurred that such fault has influenced the outputproduced by the ai system or the failure of the ai system to produce an output thatgave rise to the damage,6,10
4095,4095,it can be for example considered reasonably likely that thefault has influenced the output or failure to produce an output when that fault consistsin breaching a duty of care in respect of limiting the perimeter of operation of the aisystem and the damage occurred outside the perimeter of operation,6,10
4096,4096,on the contrary abreach of a requirement to file certain documents or to register with a given authorityeven though this might be foreseen for that particular activity or even be applicableexpressly to the operation of an ai system could not be considered as reasonablylikely to have influenced the output produced by the ai system or the failure of the aisystem to produce an output,6,5
4097,4097,this directive covers the fault constituting non compliance with certain listedrequirements laid down in chapters  and  of the ai act for providers and users ofhigh risk ai systems the non compliance with which can lead under certainconditions to a presumption of causality,6,10
4098,4098,the ai act provides for full harmonisationof requirements for ai systems unless otherwise explicitly laid down therein,6,5
4099,4099,itharmonises the specific requirements for high risk ai systems,6,5
4100,4100,hence for thepurposes of claims for damages in which a presumption of causality according to thisdirective is applied the potential fault of providers or persons subject to theobligations of a provider pursuant to the ai act is established only through a non compliance with such requirements,6,10
4101,4101,given that in practice it may be difficult for theclaimant to prove such non compliance when the defendant is a provider of the aisystem and in full consistency with the logic of the ai act this directive shouldalso provide that the steps undertaken by the provider within the risk managementsystem and the results of the risk management system ie,6,10
4102,4102,the decision to adopt or notto adopt certain risk management measures should be taken into account in thedetermination of whether the provider has complied with the relevant requirementsunder the ai act referred to in this directive,6,3
4103,4103,the risk management system put inplace by the provider pursuant to the ai act is a continuous iterative process runthroughout the lifecycle of the high risk ai system whereby the provider ensurescompliance with mandatory requirements meant to mitigate risks and can thereforebe a useful element for the purpose of the assessment of this compliance,6,5
4104,4104,thisdirective also covers the cases of users fault when this fault consists in non compliance with certain specific requirements set by the ai act,6,10
4105,4105,in addition thefault of users of high risk ai systems may be established following non compliancewith other duties of care laid down in union or national law in light of article  of the ai act,6,5
4106,4106,while the specific characteristics of certain ai systems like autonomy and opacitycould make it excessively difficult for the claimant to meet the burden of proof therecould be situations where such difficulties do not exist because there could besufficient evidence and expertise available to the complainant to prove the causal link,6,10
4107,4107,this could be the case for example in respect of high risk ai systems where theclaimant could reasonably access sufficient evidence and expertise throughdocumentation and logging requirements pursuant to the ai act,6,10
4108,4108,in such situationsthe court should not apply the presumption,6,10
4109,4109,the presumption of causality could also apply to ai systems that are not high risk aisystems because there could be excessive difficulties of proof for the claimant,6,10
4110,4110,forexample such difficulties could be assessed in light of the characteristics of certain aisystems such as autonomy and opacity which render the explanation of the innerfunctioning of the ai system very difficult in practice negatively affecting the abilityof the claimant to prove the causal link between the fault of the defendant and the aioutput,6,10
4111,4111,a national court should apply the presumption where the claimant is in anexcessively difficult position to prove causation since it is required to explain how theai system was led by the human act or omission that constitutes fault to produce theoutput or the failure to produce an output which gave rise to the damage,6,10
4112,4112,however theclaimant should neither be required to explain the characteristics of the ai systemconcerned nor how these characteristics make it harder to establish the causal link,6,14
4113,4113,the application of the presumption of causality is meant to ensure for the injuredperson a similar level of protection as for situations where ai is not involved andwhere causality may therefore be easier to prove,6,10
4114,4114,nevertheless alleviating the burdenof proving causation is not always appropriate under this directive where thedefendant is not a professional user but rather a person using the ai system for itsprivate activities,6,10
4115,4115,in such circumstances in order to balance interests between theinjured person and the non professional user it needs to be taken into account whethersuch non professional users can add to the risk of an ai system causing damagethrough their behaviour,6,9
4116,4116,if the provider of an ai system has complied with all itsobligations and in consequence that system was deemed sufficiently safe to be put onthe market for a given use by non professional users and it is then used for that task apresumption of causality should not apply for the simple launch of the operation ofsuch a system by such non professional users,6,10
4117,4117,a non professional user that buys an aisystem and simply launches it according to its purpose without interfering materiallywith the conditions of operations should not be covered by the causality presumptionlaid down by this directive,6,25
4118,4118,however if a national court determines that a non professional user materially interfered with the conditions of operation of an aisystem or was required and able to determine the conditions of operation of the aisystem and failed to do so then the presumption of causality should apply where allthe other conditions are fulfilled,6,10
4119,4119,this could be the case for example when the non professional user does not comply with the instructions of use or with other applicableduties of care when choosing the area of operation or when setting performanceconditions of the ai system,6,25
4120,4120,this is without prejudice to the fact that the providershould determine the intended purpose of an ai system including the specific contextand conditions of use and eliminate or minimise the risks of that system asappropriate at the time of the design and development taking into account theknowledge and expertise of the intended user,6,4
4121,4121,since this directive introduces a rebuttable presumption the defendant should be ableto rebut it in particular by showing that its fault could not have caused the damage,6,10
4122,4122,it is necessary to provide for a review of this directive five years after the end of thetransposition period,6,22
4123,4123,in particular that review should examine whether there is a needto create no fault liability rules for claims against the operator as long as not alreadycovered by other union liability rules in particular directive eec combinedwith a mandatory insurance for the operation of certain ai systems as suggested bythe european parliamentin accordance with the principle of proportionality it isappropriate to assess such a need in the light of relevant technological and regulatorydevelopments in the coming years taking into account the effect and impact on theroll out and uptake of ai systems especially for smes,6,2
4124,4124,such a review shouldconsider among others risks involving damage to important legal values like lifehealth and property of unwitting third parties through the operation of ai enabledproducts or services,6,2
4125,4125,that review should also analyse the effectiveness of the measuresprovided for in this directive in dealing with such risks as well as the development ofappropriate solutions by the insurance market,6,17
4126,4126,to ensure the availability of theinformation necessary to conduct such a review it is necessary to collect data andother necessary evidence covering the relevant matters,6,19
4127,4127,given the need to make adaptations to national civil liability and procedural rules tofoster the rolling out of ai enabled products and services under beneficial internalmarket conditions societal acceptance and consumer trust in ai technology and thejustice system it is appropriate to set a deadline of not later than two years after theentry into force of this directive for member states to adopt the necessarytransposition measures,6,2
4128,4128,in accordance with the joint political declaration of  september  of memberstates and the commission on explanatory documents  member states haveundertaken to accompany in justified cases the notification of their transpositionmeasures with one or more documents explaining the relationship between thecomponents of a directive and the corresponding parts of national transpositioninstruments,6,1
4129,4129,with regard to this directive the legislator considers the transmission ofsuch documents to be justifiedhave adopted this directivearticle subject matter and scope,6,19
4130,4130,this directive lays down common rules on the disclosure of evidence on high risk artificial intelligence ai systems toenable a claimant to substantiate a non contractual fault based civil law claimfor damages the burden of proof in the case of non contractual fault based civil law claimsbrought before national courts for damages caused by an ai system,6,10
4131,4131,this directive applies to non contractual fault based civil law claims for damages incases where the damage caused by an ai system occurs after the end of thetransposition period,6,10
4132,4132,this directive does not apply to criminal liability,6,10
4133,4133,this directive shall not affect rules of union law regulating conditions of liability in the field of transport any rights which an injured person may have under national rulesimplementing directive eec the exemptions from liability and the due diligence obligations as laid down inthe digital services act and national rules determining which party has the burden of proof which degreeof certainty is required as regards the standard of proof or how fault is definedother than in respect of what is provided for in articles  and,6,10
4134,4134,member states may adopt or maintain national rules that are more favourable forclaimants to substantiate a non contractual civil law claim for damages caused by anai system provided such rules are compatible with union law,6,10
4135,4135,article definitionsfor the purposes of this directive the following definitions shall apply ai system means an ai system as defined in article   of the ai act high risk ai system means an ai system referred to in article  of the ai act provider means a provider as defined in article   of the ai act user means a user as defined in article   of the ai act claim for damages means a non contractual fault based civil law claim forcompensation of the damage caused by an output of an ai system or the failure ofsuch a system to produce an output where such an output should have beenproduced claimant means a person bringing a claim for damages that has been injured by an output of an ai system or by the failure of such asystem to produce an output where such an output should have been produced has succeeded to or has been subrogated to the right of an injured person byvirtue of law or contract or is acting on behalf of one or more injured persons in accordance with union ornational law,6,5
4136,4136,potential claimant means a natural or legal person who is considering but has not yetbrought a claim for damages defendant means the person against whom a claim for damages is brought duty of care means a required standard of conduct set by national or union law inorder to avoid damage to legal interests recognised at national or union law levelincluding life physical integrity property and the protection of fundamental rights,6,10
4137,4137,article disclosure of evidence and rebuttable presumption of non compliance,6,10
4138,4138,member states shall ensure that national courts are empowered either upon therequest of a potential claimant who has previously asked a provider a person subjectto the obligations of a provider pursuant to article  or article  of the ai actor a user to disclose relevant evidence at its disposal about a specific high risk aisystem that is suspected of having caused damage but was refused or a claimant toorder the disclosure of such evidence from those persons,6,10
4139,4139,in support of that request the potential claimant must present facts and evidencesufficient to support the plausibility of a claim for damages,6,10
4140,4140,in the context of a claim for damages the national court shall only order thedisclosure of the evidence by one of the persons listed in paragraph  if the claimanthas undertaken all proportionate attempts at gathering the relevant evidence from thedefendant,6,10
4141,4141,member states shall ensure that national courts upon the request of a claimant areempowered to order specific measures to preserve the evidence mentioned inparagraph,6,10
4142,4142,national courts shall limit the disclosure of evidence to that which is necessary andproportionate to support a potential claim or a claim for damages and thepreservation to that which is necessary and proportionate to support such a claim fordamages,6,10
4143,4143,in determining whether an order for the disclosure or preservation of evidence isproportionate national courts shall consider the legitimate interests of all partiesincluding third parties concerned in particular in relation to the protection of tradesecrets within the meaning of article  of directive eu  and ofconfidential information such as information related to public or national security,6,10
4144,4144,member states shall ensure that where the disclosure of a trade secret or allegedtrade secret which the court has identified as confidential within the meaning ofarticle  of directive eu  is ordered national courts are empoweredupon a duly reasoned request of a party or on their own initiative to take specificmeasures necessary to preserve confidentiality when that evidence is used or referredto in legal proceedings,6,1
4145,4145,member states shall also ensure that the person ordered to disclose or to preserve theevidence mentioned in paragraphs  or  has appropriate procedural remedies inresponse to such orders,6,1
4146,4146,where a defendant fails to comply with an order by a national court in a claim fordamages to disclose or to preserve evidence at its disposal pursuant to paragraphs or  a national court shall presume the defendants non compliance with a relevantduty of care in particular in the circumstances referred to in article  or  thatthe evidence requested was intended to prove for the purposes of the relevant claimfor damages,6,10
4147,4147,the defendant shall have the right to rebut that presumption,6,10
4148,4148,article rebuttable presumption of a causal link in the case of fault,6,10
4149,4149,subject to the requirements laid down in this article national courts shall presumefor the purposes of applying liability rules to a claim for damages the causal linkbetween the fault of the defendant and the output produced by the ai system or thefailure of the ai system to produce an output where all of the following conditionsare met the claimant has demonstrated or the court has presumed pursuant to article the fault of the defendant or of a person for whose behaviour the defendant isresponsible consisting in the non compliance with a duty of care laid down in unionor national law directly intended to protect against the damage that occurred it can be considered reasonably likely based on the circumstances of the case thatthe fault has influenced the output produced by the ai system or the failure of the aisystem to produce an output the claimant has demonstrated that the output produced by the ai system or thefailure of the ai system to produce an output gave rise to the damage,6,10
4150,4150,in the case of a claim for damages against a provider of a high risk ai system subjectto the requirements laid down in chapters  and  of title iii of the ai act or aperson subject to the providers obligations pursuant to article  or article  ofthe ai act the condition of paragraph  letter  shall be met only where thecomplainant has demonstrated that the provider or where relevant the person subjectto the providers obligations failed to comply with any of the following requirementslaid down in those chapters taking into account the steps undertaken in and theresults of the risk management system pursuant to article  and article  point of the ai act the ai system is a system which makes use of techniques involving thetraining of models with data and which was not developed on the basis oftraining validation and testing data sets that meet the quality criteria referred toin article  to  of the ai act the ai system was not designed and developed in a way that meets thetransparency requirements laid down in article  of the ai act the ai system was not designed and developed in a way that allows for aneffective oversight by natural persons during the period in which the ai systemis in use pursuant to article  of the ai act the ai system was not designed and developed so as to achieve in the light ofits intended purpose an appropriate level of accuracy robustness andcybersecurity pursuant to article  and article  point  of the ai actor the necessary corrective actions were not immediately taken to bring the aisystem in conformity with the obligations laid down in title iii chapter  ofthe ai act or to withdraw or recall the system as appropriate pursuant toarticle  point  and article  of the ai act,6,5
4151,4151,in the case of a claim for damages against a user of a high risk ai system subject tothe requirements laid down in chapters  and  of title iii of the ai act thecondition of paragraph  letter  shall be met where the claimant proves that theuser did not comply with its obligations to use or monitor the ai system inaccordance with the accompanying instructions of use or where appropriatesuspend or interrupt its use pursuant to article  of the ai act or exposed the ai system to input data under its control which is not relevant inview of the systems intended purpose pursuant to article  of the act,6,10
4152,4152,in the case of a claim for damages concerning a high risk ai system a national courtshall not apply the presumption laid down in paragraph  where the defendantdemonstrates that sufficient evidence and expertise is reasonably accessible for theclaimant to prove the causal link mentioned in paragraph,6,10
4153,4153,in the case of a claim for damages concerning an ai system that is not a high risk aisystem the presumption laid down in paragraph  shall only apply where thenational court considers it excessively difficult for the claimant to prove the causallink mentioned in paragraph,6,10
4154,4154,in the case of a claim for damages against a defendant who used the ai system in thecourse of a personal non professional activity the presumption laid down inparagraph  shall apply only where the defendant materially interfered with theconditions of the operation of the ai system or if the defendant was required and ableto determine the conditions of operation of the ai system and failed to do so,6,10
4155,4155,the defendant shall have the right to rebut the presumption laid down in paragraph,6,10
4156,4156,article evaluation and targeted review,6,17
4157,4157,by date five years after the end of the transposition period the commission shallreview the application of this directive and present a report to the europeanparliament to the council and to the european economic and social committeeaccompanied where appropriate by a legislative proposal,6,2
4158,4158,the report shall examine the effects of articles  and  on achieving the objectivespursued by this directive,6,19
4159,4159,in particular it should evaluate the appropriateness of no fault liability rules for claims against the operators of certain ai systems as long asnot already covered by other union liability rules and the need for insurancecoverage while taking into account the effect and impact on the roll out and uptakeof ai systems especially for smes,6,10
4160,4160,the commission shall establish a monitoring programme for preparing the reportpursuant to paragraphs  and  setting out how and at what intervals the data andother necessary evidence will be collected,6,19
4161,4161,the programme shall specify the action tobe taken by the commission and by the member states in collecting and analysingthe data and other evidence,6,19
4162,4162,for the purposes of that programme member statescommunicate the relevant data and evidence to the commission by  december ofthe second full year following the end of the transposition period and by the end ofeach subsequent year,6,19
4163,4163,article amendment to directive eu in annex i to directive eu  the following point  is added directive eu  of the european parliament and of the council of  on adaptingnon contractual civil liability rules to artificial intelligence ai liability directive oj l  p,6,2
4164,4164,article transposition,6,14
4165,4165,member states shall bring into force the laws regulations and administrativeprovisions necessary to comply with this directive by two years after the entry intoforce at the latest,6,22
4166,4166,they shall forthwith communicate to the commission the text ofthose provisions,6,19
4167,4167,when member states adopt those provisions they shall contain a reference to thisdirective or be accompanied by such a reference on the occasion of their officialpublication,6,1
4168,4168,member states shall determine how such reference is to be made,6,1
4169,4169,member states shall communicate to the commission the text of the main provisionsof national law which they adopt in the field covered by this directive,6,1
4170,4170,article entry into forcethis directive shall enter into force on the twentieth day following that of its publication inthe official journal of the european union,6,2
4171,4171,article addresseesthis directive is addressed to the member states,6,1
4172,4172,done at brusselsfor the european parliament for the councilthe president the presidentdirective eu  of the european parliament and of the council of  november  onrepresentative actions for the protection of the collective interests of consumers and repealing directiveec oj l   p,6,2
4173,4173,applications of artificial intelligence ai increasingly permeate many aspects of our lives,7,4
4174,4174,we understand the distinct benefits that ai can bring but also the risks it can pose to the rights and freedoms of individuals,7,12
4175,4175,this is why we have developed a framework for auditing ai focusing on best practices for data protection compliance  whether you design your own ai system or implement one from a third party,7,0
4176,4176,it provides a clear methodology to audit ai applications and ensure they process personal data fairly,7,11
4177,4177,it comprises auditing tools and procedures that we will use in audits and investigations,7,24
4178,4178,this detailed guidance on ai and data protection,7,11
4179,4179,a toolkit designed to provide further practical support to organisations auditing the compliance of their own ai systems forthcoming,7,0
4180,4180,this guidance is aimed at two audiencesthose with a compliance focus such as data protection officers dpos general counsel risk managers senior management and the icos own auditors,7,0
4181,4181,technology specialists including machine learning experts data scientists software developers and engineers and cybersecurity and it risk managers,7,24
4182,4182,the guidance clarifies how you can assess the risks to rights and freedoms that ai can pose from a data protection perspective and the appropriate measures you can implement to mitigate them,7,11
4183,4183,while data protection and ai ethics overlap this guidance does not provide generic ethical or design principles for your use of ai,7,1
4184,4184,it corresponds to data protection principles and is structured as follows,7,11
4185,4185,part one addresses accountability and governance in ai including data protection impact assessments dpias,7,11
4186,4186,part two covers fair lawful and transparent processing including lawful bases assessing and improving ai system performance and mitigating potential discrimination,7,6
4187,4187,part three addresses data minimisation and security,7,11
4188,4188,part four covers compliance with individual rights including rights related to automated decision making,7,15
4189,4189,the accountability principle makes you responsible for complying with data protection and for demonstrating that compliance in any ai system that processes personal data,7,16
4190,4190,in an ai context accountability requires you to be responsible for the compliance of your system,7,16
4191,4191,assess and mitigate its risks,7,3
4192,4192,document and demonstrate how your system is compliant and justify the choices you have made,7,15
4193,4193,you should consider these issues as part of your dpia for any system you intend to use,7,25
4194,4194,you should note that in the majority of cases you are legally required to complete a dpia if you use ai systems that process personal data,7,9
4195,4195,dpias offer you an opportunity to consider how and why you are using ai systems to process personal data and what the potential risks could be,7,9
4196,4196,you also need to take care to identify and understand controller  processor relationships,7,20
4197,4197,this is due to the complexity and mutual dependency of the various kinds of processing typically involved in ai supply chains,7,4
4198,4198,as part of striking the required balance between the right to data protection and other fundamental rights in the context of your ai systems you will inevitably have to consider a range of competing considerations and interests,7,11
4199,4199,during the design stage you need to identify and assess what these may be,7,19
4200,4200,you should then determine how you can manage them in the context of the purposes of your processing and the risks it poses to the rights and freedoms of individuals,7,24
4201,4201,you should however note that if your ai system processes personal data you always have to comply with the fundamental data protection principles and cannot trade this requirement away,7,9
4202,4202,when you use ai to process personal data you must ensure that it is lawful fair and transparent,7,9
4203,4203,compliance with these principles may be challenging in an ai context,7,4
4204,4204,ai systems can exacerbate known security risks and make them more difficult to manage,7,3
4205,4205,they also present challenges for compliance with the data minimisation principle,7,11
4206,4206,two security risks that ai can increase are the potential for loss or misuse of the large amounts of personal data often required to train ai systems,7,3
4207,4207,software vulnerabilities to be introduced as a result of the introduction of new ai related code and infrastructure,7,25
4208,4208,by default the standard practices for developing and deploying ai involve processing large amounts of data,7,4
4209,4209,there is a risk that this fails to comply with the data minimisation principle,7,13
4210,4210,a number of techniques exist which help both data minimisation and effective ai development and deployment,7,11
4211,4211,the way ai systems are developed and deployed means that personal data is often managed and processed in unusual ways,7,9
4212,4212,this may make it harder to understand when and how individual rights apply to this data and more challenging to implement effective mechanisms for individuals to exercise those rights,7,9
4213,4213,this guidance covers what we think is best practice for data protection compliant ai as well as how we interpret data protection law as it applies to ai systems that process personal data,7,11
4214,4214,the guidance is not a statutory code,7,1
4215,4215,it contains advice on how to interpret relevant law as it applies to ai and recommendations on good practice for organisational and technical measures to mitigate the risks to individuals that ai may cause or exacerbate,7,2
4216,4216,we see new uses of artificial intelligence ai everyday from healthcare to recruitment to commerce and beyond,7,4
4217,4217,we understand the benefits that ai can bring to organisations and individuals but there are risks too,7,3
4218,4218,thats why ai is one of our top three strategic priorities why enabling good practice in ai is one of our regulatory priorities over the coming months and why we decided to develop a framework for auditing ai compliance with data protection obligations,7,16
4219,4219,the framework gives us a clear methodology to audit ai applications and ensure they process personal data fairly lawfully and transparently,7,11
4220,4220,ensures that the necessary measures are in place to assess and manage risks to rights and freedoms that arise from ai,7,16
4221,4221,supports the work of our investigation and assurance teams when assessing the compliance of organisations using ai,7,18
4222,4222,as well as using the framework to guide our own audit and enforcement activity we also wanted to share our thinking behind it,7,16
4223,4223,the framework therefore has three distinct outputs auditing tools and procedures which our investigation and assurance teams will use when assessing the compliance of organisations using ai,7,0
4224,4224,the specific auditing and investigation activities they undertake vary but can include off site checks on site tests and interviews and in some cases the recovery and analysis of evidence including ai systems themselves,7,0
4225,4225,this detailed guidance on ai and data protection for organisations which outlines our thinking,7,11
4226,4226,a toolkit designed to provide further practical support to organisations auditing the compliance of their own ai systems,7,0
4227,4227,this guidance covers what we think is best practice for data protection compliant ai as well as how we interpret data protection law as it applies to ai systems that process personal data,7,11
4228,4228,this guidance is not a statutory code,7,1
4229,4229,it contains advice on how to interpret relevant law as it applies to ai and recommendations on good practice for organisational and technical measures to mitigate the risks to individuals that ai may cause or exacerbate,7,2
4230,4230,there is no penalty if you fail to adopt good practice recommendations as long as you find another way to comply with the law,7,1
4231,4231,data protection law does not use the term ai so none of your legal obligations depend on exactly how it is defined,7,11
4232,4232,however it is useful to understand broadly what we mean by ai in the context of this guidance,7,4
4233,4233,ai has a variety of meanings including in the ai research community it refers to various methods for using a non human system to learn from experience and imitate human intelligent behaviour,7,4
4234,4234,in the data protection context the theory and development of computer systems able to perform tasks normally requiring human intelligence,7,11
4235,4235,we use the umbrella term ai because it has become a standard industry term for a range of technologies,7,4
4236,4236,one prominent area of ai is machine learning ml which is the use of computational techniques to create often complex statistical models using typically large quantities of data,7,4
4237,4237,those models can be used to make classifications or predictions about new data points,7,13
4238,4238,while not all ai involves ml most of the recent interest in ai is driven by ml in some way whether in image recognition speech to text or classifying credit risk,7,4
4239,4239,this guidance therefore focuses on the data protection challenges that ml based ai may present while acknowledging that other kinds of ai may give rise to other data protection challenges,7,11
4240,4240,you may already process personal data in the context of creating statistical models and using those models to make predictions about people,7,9
4241,4241,much of this guidance will still be relevant to you even if you do not class these activities as ml or ai,7,4
4242,4242,where there are important differences between types of ai for example simple regression models and deep neural networks we will refer to these explicitly,7,4
4243,4243,how does this guidance relate to other ico work on ai,7,4
4244,4244,this guidance is designed to complement existing ico resources including the big data ai and machine learning report published in  and updated in our guidance on explaining decisions made with ai produced in collaboration with the alan turing institute,7,2
4245,4245,the big data report provided a strong foundation for understanding the data protection implications of these technologies,7,11
4246,4246,as noted in the commissioners foreword to the  edition this is a complicated and fast developing area,7,19
4247,4247,new considerations have arisen in the last three years both in terms of the risks ai poses to individuals and the organisational and technical measures that can be taken to address those risks,7,3
4248,4248,through our engagement with stakeholders we gained additional insights into how organisations are using ai on the ground which go beyond those presented in the  report,7,2
4249,4249,another significant challenge raised by ai is explainability,7,14
4250,4250,as part of the governments ai sector deal in collaboration with the alan turing institute the turing we have produced guidance on how organisations can best explain their use of ai to individuals,7,14
4251,4251,this resulted in the explaining decisions made with ai  guidance which was published in may,7,14
4252,4252,while the explaining decisions made with ai guidance already covers the challenge of ai explainability for individuals in substantial detail this guidance includes some additional considerations about ai explainability within the organisation eg for internal oversight and compliance,7,14
4253,4253,the two pieces of guidance are complementary and we recommend reading them together,7,27
4254,4254,taking a risk based approach means assessing the risks to the rights and freedoms of individuals that may arise when you use ai,7,3
4255,4255,implementing appropriate and proportionate technical and organisational measures to mitigate these risks,7,3
4256,4256,these are general requirements in data protection law,7,11
4257,4257,they do not mean you can ignore the law if the risks are low and they may mean you have to stop a planned ai project if you cannot sufficiently mitigate those risks,7,1
4258,4258,to help you integrate this guidance into your existing risk management process we have organised it into several major risk areas,7,3
4259,4259,for each risk area we describe the risks involved,7,3
4260,4260,how ai may increase their likelihood andor impact,7,4
4261,4261,some possible measures which you could use to identify evaluate minimise monitor and control those risks,7,25
4262,4262,the technical and organisational measures included are those we consider good practice in a wide variety of contexts,7,18
4263,4263,however since many of the risk controls that you may need to adopt are context specific we cannot include an exhaustive or definitive list,7,3
4264,4264,this guidance covers both the ai and data protection specific risks and the implications of those risks for governance and accountability,7,3
4265,4265,regardless of whether you are using ai you should have accountability measures in place,7,16
4266,4266,however adopting ai applications may require you to re assess your existing governance and risk management practices,7,8
4267,4267,ai applications can exacerbate existing risks introduce new ones or generally make risks more difficult to assess or manage,7,3
4268,4268,decision makers in your organisation should therefore reconsider your organisations risk appetite in light of any existing or proposed ai applications,7,3
4269,4269,each of the sections of this guidance deep dives into one of the ai challenge areas and explores the associated risks processes and controls,7,3
4270,4270,this guidance does not provide generic ethical or design principles for the use of ai,7,1
4271,4271,while there may be overlaps between ai ethics and data protection with some proposed ethics principles already reflected in data protection law this guidance is focused on data protection compliance,7,11
4272,4272,although data protection does not dictate how ai designers should do their jobs if you use ai to process personal data you need to comply with the principles of data protection by design and by default,7,9
4273,4273,certain design choices are more likely to result in ai systems which infringe data protection in one way or other,7,11
4274,4274,this guidance will help designers and engineers understand those choices better so you can design high performing systems whilst still protecting the rights and freedoms of individuals,7,27
4275,4275,it is worth noting that our work focuses exclusively on the data protection challenges introduced or heightened by ai,7,11
4276,4276,therefore more general data protection considerations are not addressed in this guidance except in so far as they relate to and are challenged by ai,7,11
4277,4277,neither does it cover ai related challenges which are outside the remit of data protection,7,25
4278,4278,this guidance deals with the challenges that ai raises for data protection,7,11
4279,4279,the most relevant piece of uk legislation is the data protection act  dpa,7,11
4280,4280,the dpa  sets out the uks data protection framework alongside the uk general data protection regulation gdpr,7,11
4281,4281,please note that from january  you should read references to the gdpr as references to the equivalent articles in the uk gdpr,7,2
4282,4282,the dpa  comprises the following data protection regimes,7,11
4283,4283,part   supplements and tailors the uk gdpr,7,2
4284,4284,part   sets out a separate regime for law enforcement authorities,7,8
4285,4285,part   sets out a separate regime for the three intelligence services,7,26
4286,4286,most of this guidance will apply regardless of which part of the dpa applies to your processing,7,17
4287,4287,however where there are relevant differences between the requirements of the regimes these are explained in the text,7,26
4288,4288,you should also review our guidance on how the end of the transition period impacts data protection law,7,11
4289,4289,the impacts of ai on areas of ico competence other than data protection notably freedom of information are not considered here,7,2
4290,4290,this guidance is divided into several parts covering different data protection principles and rights,7,11
4291,4291,part one addresses issues that primarily relate to the accountability principle,7,16
4292,4292,this requires you to be responsible for complying with the data protection principles and for demonstrating that compliance,7,11
4293,4293,sections in this part deal with the ai specific implications of accountability including data protection impact assessments dpias and controller  processor responsibilities,7,17
4294,4294,part two covers the lawfulness fairness and transparency of processing personal data in ai systems with sections covering lawful bases for processing personal data in ai systems assessing and improving ai system performance and mitigating potential discrimination to ensure fair processing,7,11
4295,4295,part three covers the principles of security and data minimisation in ai systems,7,11
4296,4296,part four covers compliance with individual rights including rights relating to solely automated decisions,7,15
4297,4297,in particular part four covers how you can ensure meaningful human input in non automated or partly automated decisions and meaningful human review of solely automated decisions,7,21
4298,4298,this guidance covers best practices for data protection compliant ai,7,11
4299,4299,there are two broad audiences,7,14
4300,4300,first those with a compliance focus including data protection officers dposgeneral counselrisk managerssenior management andthe icos own auditors  in other words we will use this guidance as a basis to inform our audit functions under the data protection legislation,7,11
4301,4301,second technology specialists including machine learning developers and data scientistssoftware developers  engineerscybersecurity and it risk managers,7,3
4302,4302,the guidance is split into four sections that cover areas of data protection legislation that you need to consider,7,11
4303,4303,while this guidance is written to be accessible to both audiences some parts are aimed primarily at those in either compliance or technology roles and are signposted accordingly at the start of each section as well as in the text,7,27
4304,4304,parts one and four are primarily aimed at those working in a compliance role,7,27
4305,4305,however they do contain some technical details which may need to be discussed with relevant technology specialists in your organisation,7,24
4306,4306,parts two and three contain both legal and substantial technical material and you may therefore benefit from working through them alongside relevant technology experts in your organisation,7,24
4307,4307,in each section we discuss what you must do to comply with data protection law as well as what you should do as good practice,7,11
4308,4308,this distinction is generally marked using must when it relates to compliance with data protection law and using should where we consider it good practice but not essential to comply with the law,7,1
4309,4309,discussion of good practice is designed to help you if you are not sure what to do but it is not prescriptive,7,27
4310,4310,it should give you enough flexibility to develop ai systems which conform to data protection law in your own way taking a proportionate and risk based approach,7,11
4311,4311,the guidance assumes familiarity with key data protection terms and concepts,7,11
4312,4312,we also discuss in more detail data protection related terms and concepts where it helps to explain the risks that ai creates and exacerbates,7,11
4313,4313,the guidance also assumes familiarity with ai related terms and concepts,7,14
4314,4314,we have included a glossary at the end of the guidance as a quick reference point for concepts and measures included in the main text,7,24
4315,4315,the guidance focuses on specific risks and controls to ensure your ai system is compliant with data protection law and provides safeguards for individuals rights and freedoms,7,11
4316,4316,it is not intended as an exhaustive guide to data protection compliance,7,11
4317,4317,you need to make sure you are aware of all your obligations and you should read this guidance alongside our other guidance,7,27
4318,4318,your dpia process should incorporate measures to comply with your data protection obligations generally as well as conform to the specific standards in this guidance,7,11
4319,4319,this section is about the accountability principle which makes you responsible for complying with data protection law and for demonstrating that compliance in any ai system that processes personal data,7,11
4320,4320,a data protection impact assessment dpia is an ideal way to demonstrate your compliance,7,11
4321,4321,the section will also explain the importance of identifying and understanding controller processor relationships,7,14
4322,4322,finally it covers striking the required balance between the right to data protection and other fundamental rights in the context of your ai system,7,11
4323,4323,this section is aimed at senior management and those in compliance focused roles including data protection officers dpos who are accountable for the governance and data protection risk management of an ai system,7,11
4324,4324,there are some terms and techniques described that may require the input of a technical specialist,7,15
4325,4325,if used well ai has the potential to make organisations more efficient effective and innovative,7,0
4326,4326,however ai also raises significant risks for the rights and freedoms of individuals as well as compliance challenges for organisations,7,4
4327,4327,different technological approaches will either exacerbate or mitigate some of these issues but many others are much broader than the specific technology,7,25
4328,4328,as the rest of this guidance suggests the data protection implications of ai are heavily dependent on the specific use cases the population they are deployed on other overlapping regulatory requirements as well as social cultural and political considerations,7,8
4329,4329,while ai increases the importance of embedding data protection by design and default into an organisations culture and processes the technical complexities of ai systems can make this more difficult,7,11
4330,4330,demonstrating how you have addressed these complexities is an important element of accountability,7,16
4331,4331,you cannot delegate these issues to data scientists or engineering teams,7,25
4332,4332,your senior management including dpos are also accountable for understanding and addressing them appropriately and promptly although overall accountability for data protection compliance lies with the controller ie your organisation,7,16
4333,4333,to do so in addition to their own upskilling your senior management will need diverse well resourced teams to support them in carrying out their responsibilities,7,27
4334,4334,you also need to align your internal structures roles and responsibilities maps training requirements policies and incentives to your overall ai governance and risk management strategy,7,16
4335,4335,it is important that you do not underestimate the initial and ongoing level of investment of resources and effort that is required,7,22
4336,4336,your governance and risk management capabilities need to be proportionate to your use of ai,7,3
4337,4337,this is particularly true now while ai adoption is still in its initial stages and the technology itself as well as the associated laws regulations governance and risk management best practices are still developing quickly,7,8
4338,4338,we have also developed a more general accountability framework,7,16
4339,4339,this is not specific to ai but provides a baseline for demonstrating your accountability under the uk gdpr on which you could build your approach to ai accountability,7,16
4340,4340,the risk based approach of data protection law requires you to comply with your obligations and implement appropriate measures in the context of your particular circumstances  the nature scope context and purposes of the processing you intend to do and the risks this poses to individuals rights and freedoms,7,11
4341,4341,your compliance considerations therefore involve assessing the risks to the rights and freedoms of individuals and judging what is appropriate in those circumstances,7,12
4342,4342,in all cases you need to ensure you comply with data protection requirements,7,11
4343,4343,this applies to the use of ai just as to other technologies that process personal data,7,9
4344,4344,in the context of ai the specific nature of the risks posed and the circumstances of your processing will require you to strike an appropriate balance between competing interests as you go about ensuring data protection compliance,7,11
4345,4345,this may in turn impact the outcome of your processing,7,17
4346,4346,it is unrealistic to adopt a zero tolerance approach to risks to rights and freedoms and indeed the law does not require you to do so,7,22
4347,4347,it is about ensuring that these risks are identified managed and mitigated,7,3
4348,4348,we talk about trade offs and how you should manage them below and provide examples of some trade offs throughout the guidance,7,24
4349,4349,to manage the risks to individuals that arise from processing personal data in your ai systems it is important that you develop a mature understanding of fundamental rights risks and how to balance these and other interests,7,9
4350,4350,ultimately it is necessary for you to assess the risks to individual rights that your use of ai poses,7,9
4351,4351,determine how you will address these,7,24
4352,4352,establish the impact this has on your use of ai,7,4
4353,4353,you should ensure your approach fits both your organisation and the circumstances of your processing,7,21
4354,4354,where appropriate you should also use risk assessment frameworks,7,3
4355,4355,this is a complex task which can take time to get right,7,22
4356,4356,however it will give you as well as the ico a fuller and more meaningful view of your risk positions and the adequacy of your compliance and risk management approaches,7,17
4357,4357,the following sections deal with the ai specific implications of accountability including how you should undertake data protection impact assessments for ai systems,7,11
4358,4358,how you can identify whether you are a controller or processor for specific processing operations involved in the development and deployment of ai systems and the resulting implications for your responsibilities,7,4
4359,4359,how you should assess the risks to the rights and freedoms of individuals and how you should address them when you design or decide to use an ai system,7,16
4360,4360,how you should justify document and demonstrate the approach you take including your decision to use ai for the processing in question,7,21
4361,4361,dpias are a key part of data protection laws focus on accountability and data protection by design,7,11
4362,4362,you should not see dpias as simply a box ticking compliance exercise,7,25
4363,4363,they can effectively act as roadmaps for you to identify and control the risks to rights and freedoms that using ai can pose,7,4
4364,4364,they are also an ideal opportunity for you to consider and demonstrate your accountability for the decisions you make in the design or procurement of ai systems,7,4
4365,4365,in the vast majority of cases the use of ai will involve a type of processing likely to result in a high risk to individuals rights and freedoms and will therefore trigger the legal requirement for you to undertake a dpia,7,4
4366,4366,you will need to make this assessment on a case by case basis,7,24
4367,4367,in those cases where you assess that a particular use of ai does not involve high risk processing you still need to document how you have made this assessment,7,19
4368,4368,if the result of an assessment indicates residual high risk to individuals that you cannot sufficiently reduce you must consult with the ico prior to starting the processing,7,21
4369,4369,in addition to conducting a dpia you may also be required to undertake other kinds of impact assessments or do so voluntarily,7,17
4370,4370,for example public sector organisations are required to undertake equality impact assessments while other organisations voluntarily undertake algorithm impact assessments,7,17
4371,4371,similarly the machine learning community has proposed model cards and datasheets which describe how ml models may perform under different conditions and the context behind the datasets they are trained on which may help inform an impact assessment,7,13
4372,4372,there is no reason why you cannot combine these exercises so long as the assessment encompasses all the requirements of a dpia,7,25
4373,4373,the ico has produced detailed guidance on dpias that explains when they are required and how to complete them,7,27
4374,4374,this section sets out some of the things you should think about when carrying out a dpia for the processing of personal data in ai systems,7,9
4375,4375,we acknowledge that not all uses of ai will involve types of processing that are likely to result in a high risk to rights and freedoms,7,5
4376,4376,however you should note that article  of the uk gdpr requires you to undertake a dpia if your use of ai involves systematic and extensive evaluation of personal aspects based on automated processing including profiling on which decisions are made that produce legal or similarly significant effects,7,21
4377,4377,large scale processing of special categories of personal data,7,24
4378,4378,systematic monitoring of publicly accessible areas on a large scale,7,24
4379,4379,beyond this ai can also involve several processing operations that are themselves likely to result in a high risk such as use of new technologies or novel application of existing technologies data matching invisible processing and tracking of location or behaviour,7,4
4380,4380,when these involve things like evaluation or scoring systematic monitoring and large scale processing the requirement to do a dpia is triggered,7,17
4381,4381,in any case if you have a major project that involves the use of personal data it is also good practice to do a dpia,7,9
4382,4382,read our list of processing operations likely to result in high risk for examples of operations that require a dpia and further detail on which criteria are high risk in combination with others,7,25
4383,4383,your dpia needs to describe the nature scope context and purposes of any processing of personal data,7,9
4384,4384,it needs to make clear how and why you are going to use ai to process the data,7,4
4385,4385,you need to detailhow you will collect store and use data,7,24
4386,4386,the volume variety and sensitivity of the data,7,24
4387,4387,the nature of your relationship with individuals,7,14
4388,4388,the intended outcomes for individuals or wider society as well as for you,7,17
4389,4389,whether a system using ai is generally more or less risky than a system not using ai depends on the specific circumstances,7,3
4390,4390,you therefore need to evaluate this based on your own context,7,21
4391,4391,your dpia should show evidence of your consideration of less risky alternatives if any that achieve the same purpose of the processing and why you didnt choose them,7,21
4392,4392,this consideration is particularly relevant where you are using public task or legitimate interests as a lawful basis,7,12
4393,4393,see how do we identify our purposes and lawful basis,7,12
4394,4394,in the context of the ai lifecycle a dpia will best serve its purpose if you undertake it at the earliest stages of project development,7,4
4395,4395,it should feature at a minimum the following key components,7,5
4396,4396,your dpia should include a systematic description of the processing activity including data flows and the stages when ai processes and automated decisions may produce effects on individuals,7,4
4397,4397,an explanation of any relevant variation or margins of error in the performance of the system which may affect the fairness of the personal data processing see what do we need to do about statistical accuracy,7,6
4398,4398,a description of the scope and context of the processing including what data you will process,7,24
4399,4399,the number of data subjects involved,7,24
4400,4400,the source of the data,7,24
4401,4401,how far individuals are likely to expect the processing,7,21
4402,4402,your dpia should identify and record the degree of any human involvement in the decision making process and at what stage this takes place,7,21
4403,4403,where automated decisions are subject to human intervention or review you should implement processes to ensure this is meaningful and also detail the fact that decisions can be overturned,7,21
4404,4404,it can be difficult to describe the processing activity of ai systems particularly when they involve complex models and data sources,7,4
4405,4405,however such a description is necessary as part of a dpia,7,14
4406,4406,in some cases although it is not a legal requirement it may be good practice for you to maintain two versions of an assessment with the first presenting a thorough technical description for specialist audiences,7,18
4407,4407,the second containing a more high level description of the processing and explaining the logic of how the personal data inputs relate to the outputs affecting individuals this may also support you in fulfilling your obligation to explain ai decisions to individuals,7,14
4408,4408,your dpia should set out your roles and obligations as a controller and include any processors involved,7,23
4409,4409,where ai systems are partly or wholly outsourced to external providers both you and any other organisations involved should also assess whether joint controllership exists under article  of the uk gdpr and if so collaborate in the dpia process as appropriate,7,25
4410,4410,if you use a processor you can illustrate some of the more technical elements of the processing activity in a dpia by reproducing information from that processor,7,25
4411,4411,for example a flow diagram from a processors manual,7,24
4412,4412,however you should generally avoid copying large sections of a processors literature into your own assessment,7,24
4413,4413,you must where appropriate seek and document the views of individuals or their representatives unless there is a good reason not to,7,1
4414,4414,consult all relevant internal stakeholders,7,24
4415,4415,consult with your processor if you use one,7,25
4416,4416,consider seeking legal advice or other expertise,7,10
4417,4417,unless there is a good reason not to do so you should seek and document the views of individuals or their representatives on the intended processing operation during a dpia,7,25
4418,4418,it is therefore important that you can describe the processing in a way that those you consult can understand,7,14
4419,4419,however if you can demonstrate that consultation would compromise commercial confidentiality undermine security or be disproportionate or impracticable these can be reasons not to consult,7,14
4420,4420,the deployment of an ai system to process personal data needs to be driven by evidence that there is a problem and a reasoned argument that ai is a sensible solution to that problem not by the mere availability of the technology,7,9
4421,4421,by assessing necessity in a dpia you can evidence that you couldnt accomplish these purposes in a less intrusive way,7,14
4422,4422,a dpia also allows you to demonstrate that your processing of personal data by an ai system is a proportionate activity,7,4
4423,4423,when assessing proportionality you need to weigh up your interests in using ai against the risks it may pose to the rights and freedoms of individuals,7,6
4424,4424,for ai systems you need to think about any detriment to individuals that could follow from bias or inaccuracy in the algorithms and data sets being used,7,6
4425,4425,within the proportionality element of a dpia you need to assess whether individuals would reasonably expect an ai system to conduct the processing,7,4
4426,4426,if ai systems complement or replace human decision making you should document in the dpia how the project might compare human and algorithmic accuracy side by side to better justify its use,7,21
4427,4427,you should also describe any trade offs that are made for example between statistical accuracy and data minimisation and document the methodology and rationale for these,7,24
4428,4428,the dpia process will help you to objectively identify the relevant risks to individuals interests,7,17
4429,4429,you should assign a score or level to each risk measured against the likelihood and the severity of the impact on individuals,7,3
4430,4430,the use of personal data in the development and deployment of ai systems may not just pose risks to individuals information rights,7,9
4431,4431,when considering sources of risk your dpia should consider the potential impact of other material and non material damage or harm on individuals,7,3
4432,4432,for example machine learning systems may reproduce discrimination from historic patterns in data which could fall foul of equalities legislation,7,6
4433,4433,similarly ai systems that stop content being published based on the analysis of the creators personal data could impact their freedom of expression,7,9
4434,4434,in these contexts you should consider the relevant legal frameworks beyond data protection,7,11
4435,4435,against each identified risk to individuals interests you should consider options to reduce the level of assessed risk further,7,21
4436,4436,examples of this could be data minimisation techniques or providing opportunities for individuals to opt out of the processing,7,11
4437,4437,you should ask your dpo if you have one for advice when considering ways to reduce or avoid these risks and you should record in your dpia whether your chosen measure reduces or eliminates the risk in question,7,3
4438,4438,it is important that dpos or other information governance professionals or both are involved in ai projects from the earliest stages,7,23
4439,4439,there must be clear and open channels of communication between them and the project teams,7,25
4440,4440,this will ensure that they can identify and address these risks early in the ai lifecycle,7,3
4441,4441,data protection should not be an afterthought and a dpos professional opinion should not come as a surprise at the eleventh hour,7,11
4442,4442,you can use a dpia to document the safeguards you put in place to ensure the individuals responsible for the development testing validation deployment and monitoring of ai systems are adequately trained and have an understanding of the data protection implications of the processing,7,11
4443,4443,your dpia can also evidence the organisational measures you have put in place such as appropriate training to mitigate risks associated with human error,7,23
4444,4444,you should also document any technical measures designed to reduce risks to the security and accuracy of personal data processed in your ai system,7,11
4445,4445,once you have introduced measures to mitigate the risks you have identified the dpia should document the residual levels of risk posed by the processing,7,17
4446,4446,you are not required to eliminate every risk identified,7,25
4447,4447,however if your assessment indicates a high risk to the data protection rights of individuals that you are unable to sufficiently reduce you are required to consult the ico before you can go ahead with the processing,7,9
4448,4448,you should record what additional measures you plan to take,7,22
4449,4449,whether each risk has been eliminated reduced or accepted,7,22
4450,4450,the overall level of residual risk after taking additional measures,7,3
4451,4451,the opinion of your dpo if you have one,7,22
4452,4452,whether you need to consult the ico,7,25
4453,4453,although you must carry out your dpia before the processing of personal data begins you should also consider it to be a live document,7,9
4454,4454,this means reviewing the dpia regularly and undertaking a reassessment where appropriate eg if the nature scope context or purpose of the processing and the risks posed to individuals alter for any reason,7,19
4455,4455,for example depending on the deployment it could be that the demographics of the target population may shift or that people adjust their behaviour over time in response to the processing itself,7,6
4456,4456,this is a phenomenon in ai known as concept drift,7,13
4457,4457,often several different organisations will be involved in developing and deploying ai systems which process personal data,7,9
4458,4458,the uk gdpr recognises that not all organisations involved in the processing will have the same degree of control or responsibility,7,9
4459,4459,it is important to be able to identify who is acting as a controller a joint controller or a processor so you understand which uk gdpr obligations apply to which organisation,7,2
4460,4460,you should use our existing guidance on controllers and processors to help you with this,7,25
4461,4461,this is a complicated area but some key points from that guidance are you should take the time to assess and document the status of each organisation you work with in respect of all the personal data and processing activities you carry out,7,24
4462,4462,if you exercise overall control of the purpose and means of the processing of personal data  you decide what data to process and why  you are a controller,7,9
4463,4463,if you dont have any purpose of your own for processing the data and you only act on a clients instructions you are likely to be a processor  even if you make some technical decisions about how you process the data,7,25
4464,4464,organisations that determine the purposes and means of processing will be controllers regardless of how they are described in any contract about processing services,7,23
4465,4465,as ai usually involves processing personal data in several different phases or for several different purposes it is possible that you may be a controller or joint controller for some phases or purposes and a processor for others,7,4
4466,4466,our guidance says that if you make any of the following overarching decisions you will be a controller to collect personal data in the first place,7,9
4467,4467,what types of personal data to collect,7,9
4468,4468,the purpose or purposes the data are to be used for,7,9
4469,4469,which individuals to collect the data about,7,24
4470,4470,how long to retain the data,7,22
4471,4471,how to respond to requests made in line with individuals rights,7,21
4472,4472,our guidance says that you are likely to be a processor if you dont have any purpose of your own for processing the data and you only act on a clients instructions,7,25
4473,4473,you may still be able to make some technical decisions as a processor,7,25
4474,4474,for example where allowed in the contract you may use your technical knowledge to decide the it systems and methods you use to process personal data,7,24
4475,4475,how you store the data,7,24
4476,4476,the security measures that will protect it,7,11
4477,4477,how you retrieve transfer delete or dispose of that data,7,24
4478,4478,our work has identified that when ai systems involve a number of organisations in the processing of personal data assigning the roles of controller and processor can become complex for instance when some of the processing happens in the cloud,7,4
4479,4479,this raises questions of policy and we plan to consult with stakeholders including government to explore these areas with a view to addressing these issues in more detail when we revise our cloud computing guidance in,7,2
4480,4480,this guidance will also be subject to external stakeholder consultation prior to its finalisation,7,27
4481,4481,as we review our cloud computing guidance we will consult on the scenarios which could result in an organisation becoming a controller which may include when organisations make decisions about the source and nature of the data used to train an ai model,7,20
4482,4482,the target output of the model what is being predicted or classified,7,20
4483,4483,the broad kinds of ml algorithms that will be used to create models from the data eg regression models decision trees random forests neural networks,7,13
4484,4484,feature selection  the features that may be used in each model,7,20
4485,4485,key model parameters eg how complex a decision tree can be or how many models will be included in an ensemble,7,20
4486,4486,key evaluation metrics and loss functions such as the trade off between false positives and false negatives,7,18
4487,4487,how any models will be continuously tested and updated how often using what kinds of data and how ongoing performance will be assessed,7,20
4488,4488,we will also consider questions regarding when an organisation is depending on the terms of their contract able to make decisions to support the provision of ai services and still remain a processor for instance in areas such as the specific implementation of generic ml algorithms such as the programming language and code libraries they are written in,7,4
4489,4489,how the data and models are stored such as the formats they are serialised and stored in and local caching,7,20
4490,4490,measures to optimise learning algorithms and models to minimise their consumption of computing resources eg by implementing them as parallel processes,7,13
4491,4491,architectural details of how models will be deployed such as the choice of virtual machines microservices apis,7,20
4492,4492,as we develop our cloud computing guidance we will work with stakeholders to develop a range of scenarios when the organisation remains a data processor as it provides ai services,7,23
4493,4493,in our work to date we have developed some indicative example scenarios,7,14
4494,4494,an organisation provides a cloud based service consisting of a dedicated cloud computing environment with processing and storage and a suite of common tools for ml,7,24
4495,4495,these services enable clients to build and run their own models with data they have chosen but using the tools and infrastructure the organisation provides in the cloud,7,20
4496,4496,the clients will be controllers and the provider is likely to be a processor,7,23
4497,4497,the clients are controllers as they take the overarching decisions about what data and models they want to use the key model parameters and the processes for evaluating testing and updating those models,7,20
4498,4498,the provider as a processor could still decide what programming languages and code libraries those tools are written in the configuration of storage solutions the graphical user interface and the cloud architecture,7,26
4499,4499,an organisation provides live ai prediction and classification services to clients,7,13
4500,4500,it develops its own ai models and allows clients to send queries via an api what objects are in this image,7,4
4501,4501,to get responses a classification of objects in the image,7,13
4502,4502,first the prediction service provider decides how to create and train the model that powers its services and processes data for these purposes,7,20
4503,4503,it is likely to be a controller for this element of the processing,7,19
4504,4504,second the provider processes data to make predictions and classifications about particular examples for each client,7,13
4505,4505,the client is more likely to be the controller for this element of the processing and the provider is likely to be a processor,7,23
4506,4506,an ai service provider isolates different client specific models,7,20
4507,4507,this enables each client to make overarching decisions about their model including whether to further process personal data from their own context to improve their own model,7,20
4508,4508,as long as the isolation between different controllers is complete and auditable the client will be the sole controller and the provider will be a processor,7,25
4509,4509,your use of ai must comply with the requirements of data protection law,7,11
4510,4510,however there can be a number of different values and interests to consider and these may at times pull in different directions,7,21
4511,4511,these are commonly referred to as trade offs and the risk based approach of data protection law can help you navigate them,7,11
4512,4512,there are several significant examples relating to ai which we discuss in detail elsewhere the interests in training a sufficiently accurate ai system and in reducing the quantity of personal data processed to train that system see how should we balance data minimisation and statistical accuracy,7,13
4513,4513,producing an ai system which is sufficiently statistically accurate and which avoids discrimination see what are the technical approaches to mitigate discrimination risk in ml models,7,6
4514,4514,striking the appropriate balance between explainability and statistical accuracy security and commercial secrecy see the explaining decisions made with ai guidance and what about ai security risks exacerbated by explainable ai,7,14
4515,4515,if you are using ai to process personal data you therefore need to identify and assess these interests as part of your broader consideration of the risks to the rights and freedoms of individuals and how you will meet your obligations under the law,7,9
4516,4516,the right balance depends on the specific sectoral and social context you operate in and the impact the processing may have on individuals,7,6
4517,4517,however there are methods you can use to assess and mitigate trade offs that are relevant to many use cases,7,25
4518,4518,in most cases striking the right balance between these multiple trade offs is a matter of judgement specific to the use case and the context an ai system is meant to be deployed in,7,4
4519,4519,whatever choices you make you need to be accountable for them,7,16
4520,4520,your efforts should be proportionate to the risks the ai system you are considering to deploy poses to individuals,7,25
4521,4521,you should identify and assess any existing or potential trade offs when designing or procuring an ai system and assess the impact it may have on individuals,7,4
4522,4522,consider available technical approaches to minimise the need for any trade offs,7,25
4523,4523,consider any techniques which you can implement with a reasonable level of investment and effort,7,25
4524,4524,have clear criteria and lines of accountability about the final trade off decisions,7,21
4525,4525,this should include a robust risk based and independent approval process,7,21
4526,4526,where appropriate take steps to explain any trade offs to individuals or any human tasked with reviewing ai outputs,7,14
4527,4527,review trade offs on a regular basis taking into account among other things the views of individuals or their representatives and any emerging techniques or best practices to reduce them,7,14
4528,4528,you should document these processes and their outcomes to an auditable standard,7,24
4529,4529,this will help you to demonstrate that your processing is fair necessary proportionate adequate relevant and limited,7,6
4530,4530,this is part of your responsibility as a controller under article  and your compliance with the accountability principle under article,7,16
4531,4531,you must also capture them with an appropriate level of detail where required as part of a dpia or a legitimate interests assessment lia,7,12
4532,4532,you should also document how you have considered the risks to the individuals that are having their personal data processed,7,9
4533,4533,the methodology for identifying and assessing the trade offs in scope the reasons for adopting or rejecting particular technical approaches if relevant,7,0
4534,4534,the prioritisation criteria and rationale for your final decision,7,21
4535,4535,how the final decision fits within your overall risk appetite,7,21
4536,4536,you should also be ready to halt the deployment of any ai systems if it is not possible to achieve a balance that ensures compliance with data protection requirements,7,25
4537,4537,when you either buy an ai solution from a third party or outsource it altogether you need to conduct an independent evaluation of any trade offs as part of your due diligence process,7,25
4538,4538,you are also required to specify your requirements at the procurement stage rather than addressing trade offs afterwards,7,27
4539,4539,recital  of the uk gdpr says producers of ai solutions should be encouraged to take into account the right to data protection when developing and designing their systems,7,11
4540,4540,make sure that controllers and processors are able to fulfil their data protection obligations,7,11
4541,4541,you should ensure that any system you procure aligns with what you consider to be the appropriate trade offs,7,25
4542,4542,if you are unable to assess whether the use of a third party solution would be data protection compliant then you should as a matter of good practice opt for a different solution,7,25
4543,4543,since new risks and compliance considerations may arise during the course of the deployment you should regularly review any outsourced services and be able to modify them or switch to another provider if their use is no longer compliant in your circumstances,7,25
4544,4544,for example a vendor may offer a cv screening tool which effectively scores promising job candidates but may ostensibly require a lot of information about each candidate to assist with the assessment,7,27
4545,4545,if you are procuring such a system you need to consider whether you can justify collecting so much personal data from candidates and if not request the provider modify their system or seek another provider,7,9
4546,4546,you need to make significant judgement calls when determining the appropriate trade offs,7,21
4547,4547,while effective risk management processes are essential the culture of your organisation also plays a fundamental role,7,3
4548,4548,undertaking this kind of exercise will require collaboration between different teams within the organisation,7,25
4549,4549,diversity incentives to work collaboratively as well as an environment in which staff feel encouraged to voice concerns and propose alternative approaches are all important,7,27
4550,4550,the social acceptability of ai in different contexts and the best practices in relation to trade offs are the subject of ongoing societal debates,7,4
4551,4551,consultation with stakeholders outside your organisation including those affected by the trade off can help you understand the value you should place on different criteria,7,0
4552,4552,in some cases you can precisely quantify elements of the trade offs,7,0
4553,4553,a number of mathematical and computer science techniques known as constrained optimisation aim to find the optimal solutions for minimising trade offs,7,11
4554,4554,for example the theory of differential privacy provides a framework for quantifying and minimising trade offs between the knowledge that can be gained from a dataset or statistical model and the privacy of the people in it,7,26
4555,4555,similarly various methods exist to create ml models which optimise statistical accuracy while also minimising mathematically defined measures of discrimination,7,13
4556,4556,while these approaches provide theoretical guarantees it can be hard to meaningfully put them into practice,7,14
4557,4557,in many cases values like privacy and fairness are difficult to meaningfully quantify,7,6
4558,4558,for example differential privacy may be able to measure the likelihood of an individual being uniquely identified from a particular dataset but not the sensitivity of that identification,7,11
4559,4559,therefore they may not always be appropriate,7,1
4560,4560,if you do decide to use them you should always supplement these methods with a more qualitative and holistic approach,7,27
4561,4561,but the inability to precisely quantify the values at stake does not mean you can avoid assessing and justifying the trade off altogether you still need to justify your choices,7,21
4562,4562,in many cases trade offs are not precisely quantifiable but this should not lead to arbitrary decisions,7,21
4563,4563,you should perform contextual assessments documenting and justifying your assumptions about the relative value of different requirements for specific ai use cases,7,24
4564,4564,this section explains the lawfulness fairness and transparency principles,7,11
4565,4565,compliance with these principles may be challenging in an ai context,7,4
4566,4566,this section is aimed at compliance focused roles including senior management who are responsible for ensuring the processing using ai is lawful fair and transparent,7,27
4567,4567,there are several techniques described that would require the input of a technical specialist,7,25
4568,4568,firstly the development and deployment of ai systems involve processing personal data in different ways for different purposes,7,4
4569,4569,you must break down and separate each distinct processing operation and identify the purpose and an appropriate lawful basis for each one in order to comply with the principle of lawfulness,7,21
4570,4570,second if you use an ai system to infer data about people in order for this processing to be fair you need to ensure that the system is sufficiently statistically accurate and avoids discrimination,7,6
4571,4571,you consider the impact of individuals reasonable expectations,7,17
4572,4572,for example an ai system used to predict loan repayment rates is likely to breach the fairness principle if it makes predictions which frequently turn out to be incorrect,7,6
4573,4573,leads to disparities in outcomes between groups eg between men and women which could not be justified as a proportionate means of achieving a legitimate aim,7,6
4574,4574,uses personal data in ways which individuals would not reasonably expect,7,9
4575,4575,thirdly you need to be transparent about how you process personal data in an ai system to comply with the principle of transparency,7,9
4576,4576,the core issues regarding ai and the transparency principle are addressed in explaining decisions made with ai guidance so are not discussed in detail here,7,14
4577,4577,whenever you are processing personal data  whether to train a new ai system or make predictions using an existing one  you must have an appropriate lawful basis to do so,7,9
4578,4578,different lawful bases may apply depending on your particular circumstances,7,12
4579,4579,however some lawful bases may be more likely to be appropriate for the training and  or deployment of ai than others,7,25
4580,4580,in some cases more than one lawful basis may be appropriate,7,26
4581,4581,at the same time you must remember that it is your responsibility to decide which lawful basis applies to your processing,7,21
4582,4582,you must always choose the lawful basis that most closely reflects the true nature of your relationship with the individual and the purpose of the processing,7,21
4583,4583,you should make this determination before you start your processing,7,21
4584,4584,you should document your decision,7,21
4585,4585,you cannot swap lawful bases at a later date without good reason,7,22
4586,4586,you must include your lawful basis in your privacy notice along with the purposes,7,12
4587,4587,if you are processing special categories of data you need both a lawful basis and an additional condition for processing,7,26
4588,4588,in many cases when determining your purposes and lawful basis it will make sense for you to separate the research and development phase including conceptualisation design training and model selection of ai systems from the deployment phase,7,20
4589,4589,this is because these are distinct and separate purposes with different circumstances and risks,7,26
4590,4590,therefore it may sometimes be more appropriate to choose different lawful bases for your ai development and deployment,7,25
4591,4591,for example you need to do this where the ai system was developed for a general purpose task and you then deploy it in different contexts for different purposes,7,20
4592,4592,for example a facial recognition system could be trained to recognise faces but that functionality could be used for multiple purposes such as preventing crime authentication and tagging friends in a social network,7,12
4593,4593,each of these further applications might require a different lawful basis,7,26
4594,4594,in cases where you implement an ai system from a third party any processing of personal data undertaken by the developer will have been for a different purpose eg to develop the system to what you intend to use the system for therefore you may need to identify a different lawful basis,7,9
4595,4595,processing of personal data for the purposes of training a model may not directly affect the individuals but once the model is deployed it may automatically make decisions which have legal or significant effects,7,9
4596,4596,this means the provisions on automated decision making apply as a result a different range of available lawful bases may apply at the development and deployment stages,7,26
4597,4597,the following sections outline some ai related considerations for each of the uk gdprs lawful bases,7,2
4598,4598,they do not consider part  of the dpa law enforcement processing at this stage,7,23
4599,4599,consent may be an appropriate lawful basis in cases where you have a direct relationship with the individuals whose data you want to process,7,9
4600,4600,however you must ensure that consent is freely given specific informed and unambiguous and involves a clear affirmative act on the part of the individuals,7,21
4601,4601,the advantage of consent is that it can lead to more trust and buy in from individuals when they are using your service,7,21
4602,4602,providing individuals with control can also be a factor in your dpias,7,22
4603,4603,however for consent to apply individuals must have a genuine choice about whether you can use their data,7,9
4604,4604,this may have implications depending on what you intend to do with the data  it can be difficult to ensure you collect valid consent for more complicated processing operations such as those involved in ai,7,9
4605,4605,for example the more things you want to do with the data the more difficult it is to ensure that consent is genuinely specific and informed,7,9
4606,4606,the key is that individuals understand how you are using their personal data and have consented to this use,7,9
4607,4607,for example if you want to collect a wide range of features to explore different models to predict a variety of outcomes consent may be an appropriate lawful basis provided that you inform individuals about these activities and obtain valid consent,7,21
4608,4608,consent may also be an appropriate lawful basis for the use of an individuals data during deployment of an ai system eg for purposes such as personalising the service or making a prediction or recommendation,7,9
4609,4609,however you should be aware that for consent to be valid individuals must also be able to withdraw consent as easily as they gave it,7,21
4610,4610,if you are relying on consent as the basis of processing data with an ai system during deployment eg to drive personalised content you should be ready to accommodate the withdrawal of consent for this processing,7,21
4611,4611,this lawful basis applies where the processing using ai is objectively necessary to deliver a contractual service to the relevant individual or to take steps prior to entering into a contract at the individuals request eg to provide an ai derived quote for a service,7,21
4612,4612,if there is a less intrusive way of processing their data to provide the same service or if the processing is not in practice objectively necessary for the performance of the contract then you cannot rely on this lawful basis for the processing of data with ai,7,9
4613,4613,furthermore even if it is an appropriate ground for the use of the system this may not be an appropriate ground for processing personal data to develop an ai system,7,9
4614,4614,if an ai system can perform well enough without being trained on the individuals personal data performance of the contract does not depend on such processing,7,9
4615,4615,since machine learning models are typically built using very large datasets whether or not a single individuals data is included in the training data should have a negligible effect on the systems performance,7,13
4616,4616,similarly even if you can use performance of a contract as a lawful basis to provide a quote prior to a contract this does not mean you can also use it to justify using that data to develop the ai system,7,9
4617,4617,you should also note that you are unlikely to be able to rely on this basis for processing personal data for purposes such as service improvement of your ai system,7,9
4618,4618,this is because in most cases collection of personal data about the use of a service details of how users engage with that service or for the development of new functions within that service are not objectively necessary for the provision of a contract,7,9
4619,4619,this is because the service can be delivered without such processing,7,23
4620,4620,conversely use of ai to process personal data for purposes of personalising content may be regarded as necessary for the performance of a contract  but only in some cases,7,9
4621,4621,whether this processing can be regarded as intrinsic to your service depends on the nature of the service,7,23
4622,4622,the expectations of individuals,7,22
4623,4623,whether you can provide your service without this processing ie if the personalisation of content by means of an ai system is not integral to the service you should consider an alternative lawful basis,7,9
4624,4624,there are some examples in which the use of an ai system to process personal data may be a legal obligation,7,9
4625,4625,you may also be required to audit your ai systems to ensure they are compliant with various legislation including but not limited to data protection and this may involve processing of personal data,7,9
4626,4626,for example to test how the system performs on different kinds of people,7,24
4627,4627,such processing could rely on legal obligation as a basis but this would only cover the auditing and testing of the system not any other use of that data,7,9
4628,4628,you must be able to identify the obligation in question either by reference to the specific legal provision or else by pointing to an appropriate source of advice or guidance that sets it out clearly,7,1
4629,4629,similarly if you use ai as part of the exercise of your official authority or to perform a task in the public interest set out by law the necessary processing of personal data involved may be based on those grounds,7,9
4630,4630,this is likely to be relevant to public authorities using ai to deliver public services,7,1
4631,4631,in a limited number of cases the processing of personal data by an ai system might be based on protecting the vital interests of the individuals,7,9
4632,4632,for example for emergency medical diagnosis of patients who are otherwise incapable of providing consent eg processing an fmri scan of an unconscious patient by an ai diagnostic system,7,4
4633,4633,it is however very unlikely that vital interests could also provide a basis for developing an ai system because this would rarely directly and immediately result in protecting the vital interests of those individuals even if the models that are eventually built might later be used to save the lives of other individuals,7,4
4634,4634,for the development of potentially life saving ai systems it would be better to rely on other lawful bases,7,25
4635,4635,depending on your circumstances you could base your processing of personal data for both development and ongoing use of ai on the legitimate interests lawful basis,7,9
4636,4636,it is important to note that while legitimate interests is the most flexible lawful basis for processing it is not always the most appropriate,7,21
4637,4637,for example if the way you intend to use peoples data would be unexpected or cause unnecessary harm,7,9
4638,4638,it also means you are taking on additional responsibility for considering and protecting peoples rights and interests,7,16
4639,4639,you must also be able to demonstrate the necessity and proportionality of the processing,7,14
4640,4640,additionally if you are a public authority you can only rely on legitimate interests if you are processing for a legitimate reason other than performing your tasks as a public authority,7,26
4641,4641,there are three elements to the legitimate interests lawful basis and it can help to think of these as the three part test,7,26
4642,4642,you need toidentify a legitimate interest the purpose test,7,0
4643,4643,show that the processing is necessary to achieve it the necessity test,7,19
4644,4644,balance it against the individuals interests rights and freedoms the balancing test,7,22
4645,4645,there can be a wide range of interests that constitute legitimate interests in data protection law,7,11
4646,4646,these can be your own or those of third parties as well as commercial or societal interests,7,0
4647,4647,however the key is understanding that while legitimate interests may be more flexible it comes with additional responsibilities,7,0
4648,4648,it requires you to assess the impact of your processing on individuals and be able to demonstrate that there is a compelling benefit to the processing,7,17
4649,4649,you should address and document these considerations as part of your legitimate interests assessment lia,7,21
4650,4650,as described above in the initial research and development phase of your ai system your purposes may be quite broad but as more specific purposes are identified you may need to review your lia accordingly or identify a different lawful basis,7,20
4651,4651,an organisation seeks to rely on legitimate interests for processing personal data for the purposes of training a machine learning model,7,9
4652,4652,legitimate interests may allow the organisation the most room to experiment with different variables for its model,7,0
4653,4653,however as part of its legitimate interests assessment the organisation has to demonstrate that the range of variables and models it intends to use is a reasonable approach to achieving its outcome,7,0
4654,4654,it can best achieve this by properly defining all of its purposes and justifying the use of each type of data collected  this will allow the organisation to work through the necessity and balancing aspects of its lia,7,24
4655,4655,over time as purposes are refined the lia is revisited,7,22
4656,4656,for example the mere possibility that some data might be useful for a prediction is not by itself sufficient for the organisation to demonstrate that processing this data is necessary for building the model,7,9
4657,4657,if you intend to use ai to process special category data or data about criminal offences then you will need to ensure you comply with the requirements of articles  and  of the uk gdpr as well as the dpa,7,8
4658,4658,special category data is personal data that needs more protection because it is sensitive,7,11
4659,4659,in order to process it you need a lawful basis under article  as well as a separate condition under article  although these do not have to be linked,7,26
4660,4660,for more detail see our detailed guidance on special category data and how should we address risks of bias and discrimination,7,6
4661,4661,data protection law applies to all automated individual decision making and profiling,7,11
4662,4662,article  of the uk gdpr has additional rules to protect individuals if you are carrying out solely automated decision making that has legal or similarly significant effects on them,7,21
4663,4663,this may apply in the ai context eg where you are using an ai system to make these kinds of decisions,7,21
4664,4664,however you can only carry out this type of decision making where the decision is necessary for the entry into or performance of a contract,7,21
4665,4665,authorised by law that applies to you,7,1
4666,4666,based on the individuals explicit consent,7,12
4667,4667,you therefore have to identify if your processing falls under article  and where it does make sure that you give individuals information about the processing,7,19
4668,4668,introduce simple ways for them to request human intervention or challenge a decision,7,21
4669,4669,carry out regular checks to make sure your systems are working as intended,7,15
4670,4670,statistical accuracy refers to the proportion of answers that an ai system gets correct or incorrect,7,18
4671,4671,this section explains the controls you can implement so that your ai systems are sufficiently statistically accurate to ensure that the processing of personal data complies with the fairness principle,7,6
4672,4672,it is important to note that the word accuracy has a different meaning in the contexts of data protection and ai,7,18
4673,4673,accuracy in data protection is one of the fundamental principles requiring you to ensure that personal data is accurate and where necessary kept up to date,7,11
4674,4674,it requires you to take all reasonable steps to make sure the personal data you process is not incorrect or misleading as to any matter of fact and where necessary is corrected or deleted without undue delay,7,9
4675,4675,broadly accuracy in ai and more generally in statistical modelling refers to how often an ai system guesses the correct answer measured against correctly labelled test data,7,18
4676,4676,the test data is usually separated from the training data prior to training or drawn from a different source or both,7,13
4677,4677,in many contexts the answers the ai system provides will be personal data,7,9
4678,4678,for example an ai system might infer someones demographic information or their interests from their behaviour on a social network,7,4
4679,4679,so for clarity in this guidance we use the terms accuracy to refer to the accuracy principle of data protection law,7,18
4680,4680,statistical accuracy to refer to the accuracy of an ai system itself,7,18
4681,4681,fairness in a data protection context generally means that you should handle personal data in ways that people would reasonably expect and not use it in ways that have unjustified adverse effects on them,7,11
4682,4682,improving the statistical accuracy of your ai systems outputs is one of your considerations to ensure compliance with the fairness principle,7,6
4683,4683,data protections accuracy principle applies to all personal data whether it is information about an individual used as an input to an ai system or an output of the system,7,9
4684,4684,however this does not mean that an ai system needs to be  statistically accurate to comply with the accuracy principle,7,18
4685,4685,in many cases the outputs of an ai system are not intended to be treated as factual information about the individual,7,14
4686,4686,instead they are intended to represent a statistically informed guess as to something which may be true about the individual now or in the future,7,14
4687,4687,to avoid such personal data being misinterpreted as factual you should ensure that your records indicate that they are statistically informed guesses rather than facts,7,9
4688,4688,your records should also include information about the provenance of the data and the ai system used to generate the inference,7,24
4689,4689,you should also record if it becomes clear that the inference was based on inaccurate data or the ai system used to generate it is statistically flawed in a way which may have affected the quality of the inference,7,18
4690,4690,similarly if the processing of the incorrect inference may have an impact on them an individual may request the inclusion of additional information in their record countering the incorrect inference,7,10
4691,4691,this helps ensure that any decisions taken on the basis of the potentially incorrect inference are informed by any evidence that it may be wrong,7,18
4692,4692,the uk gdpr mentions statistical accuracy in the context of profiling and automated decision making at recital,7,18
4693,4693,this states organisations should put in place appropriate mathematical and statistical procedures for the profiling of individuals as part of their technical measures,7,18
4694,4694,you should ensure any factors that may result in inaccuracies in personal data are corrected and the risk of errors is minimised,7,9
4695,4695,if you use an ai system to make inferences about people you need to ensure that the system is sufficiently statistically accurate for your purposes,7,4
4696,4696,this does not mean that every inference has to be correct but you do need to factor in the possibility of them being incorrect and the impact this may have on any decisions that you may  take on the basis of them,7,18
4697,4697,failure to do this could mean that your processing is not compliant with the fairness principle,7,6
4698,4698,it may also impact on your compliance with the data minimisation principle as personal data which includes inferences must be adequate and relevant for your purpose,7,9
4699,4699,your ai system therefore needs to be sufficiently statistically accurate to ensure that any personal data generated by it is processed lawfully and fairly,7,9
4700,4700,however overall statistical accuracy is not a particularly useful measure and usually needs to be broken down into different measures,7,18
4701,4701,it is important to measure and prioritise the right ones,7,18
4702,4702,if you are in a compliance role and are unsure what these terms mean you should consult colleagues in the relevant technical roles,7,1
4703,4703,statistical accuracy as a general measure is about how closely an ai systems predictions match the correct labels as defined in the test data,7,18
4704,4704,for example if an ai system is used to classify emails as spam or not spam a simple measure of statistical accuracy is the number of emails that were correctly classified as spam or not spam as a proportion of all the emails that were analysed,7,18
4705,4705,however such a measure could be misleading,7,18
4706,4706,for example if  of all emails received to an inbox are spam then you could create a  accurate classifier by simply labelling everything as spam,7,13
4707,4707,but this would defeat the purpose of the classifier as no genuine email would get through,7,13
4708,4708,for this reason you should use alternative measures of statistical accuracy to assess how good a system is,7,18
4709,4709,if you are in a compliance role you should work with colleagues in technical roles to ensure that you have in place appropriate measures of statistical accuracy given your context and the purposes of processing,7,18
4710,4710,these measures should reflect the balance between two different kinds of errors,7,18
4711,4711,a false positive or type i error these are cases that the ai system incorrectly labels as positive eg emails classified as spam when they are genuine,7,0
4712,4712,a false negative or type ii error these are cases that the ai system incorrectly labels as negative when they are actually positive eg emails classified as genuine when they are actually spam,7,0
4713,4713,it is important to strike the right balance between these two types of errors,7,22
4714,4714,there are more useful measures which reflect these two types of errors,7,18
4715,4715,precision the percentage of cases identified as positive that are in fact positive also called positive predictive value,7,18
4716,4716,for example if nine out of  emails that are classified as spam are actually spam the precision of the ai system is,7,18
4717,4717,recall or sensitivity the percentage of all cases that are in fact positive that are identified as such,7,18
4718,4718,for example if  out of  emails are actually spam but the ai system only identifies seven of them then its recall is,7,0
4719,4719,there are trade offs between precision and recall which can be assessed using statistical measures,7,18
4720,4720,if you place more importance on finding as many of the positive cases as possible maximising recall this may come at the cost of some false positives lowering precision,7,25
4721,4721,in addition there may be important differences between the consequences of false positives and false negatives on individuals which could affect the fairness of the processing,7,6
4722,4722,if a cv filtering system being used to assist with selecting qualified candidates for an interview produces a false positive then an unqualified candidate may be invited to interview wasting the employers and the applicants time unnecessarily,7,6
4723,4723,if it produces a false negative a qualified candidate will miss an employment opportunity and the organisation will miss a good candidate,7,6
4724,4724,you should prioritise avoiding certain kinds of error based on the severity and nature of the risks,7,3
4725,4725,in general statistical accuracy as a measure depends on how possible it is to compare the performance of a systems outputs to some ground truth ie checking the results of the ai system against the real world,7,18
4726,4726,for example a medical diagnostic tool designed to detect malignant tumours could be evaluated against high quality test data containing known patient outcomes,7,18
4727,4727,in some other areas a ground truth may be unattainable,7,14
4728,4728,this could be because no high quality test data exists or because what you are trying to predict or classify is subjective eg whether a social media post is offensive,7,13
4729,4729,there is a risk that statistical accuracy is misconstrued in these situations so that ai systems are seen as being highly statistically accurate even though they are reflecting the average of what a set of human labellers thought rather than objective truth,7,18
4730,4730,to avoid this your records should indicate where ai outputs are not intended to reflect objective facts and any decisions taken on the basis of such personal data should reflect these limitations,7,9
4731,4731,this is also an example of where you must take into account the accuracy principle  for more information see our guidance on the accuracy principle which refers to accuracy of opinions,7,14
4732,4732,finally statistical accuracy is not a static measure,7,18
4733,4733,while it is usually measured on static test data held back from the training data in real life situations ai systems are applied to new and changing populations,7,18
4734,4734,just because a system is statistically accurate about an existing populations data eg customers in the last year it may not continue to perform well if there is a change in the characteristics of that population or any other population who the system is applied to in future,7,18
4735,4735,behaviours may change either of their own accord or because they are adapting in response to the system and the ai system may become less statistically accurate with time,7,4
4736,4736,this phenomenon is referred to in machine learning as concept  model drift and various methods exist for detecting it,7,13
4737,4737,for example you can measure the distance between classification errors over time increasingly frequent errors may suggest drift,7,18
4738,4738,you should regularly assess drift and retrain the model on new data where necessary,7,20
4739,4739,as part of your accountability responsibilities you should decide and document appropriate thresholds for determining whether your model needs to be retrained based on the nature scope context and purposes of the processing and the risks it poses,7,20
4740,4740,for example if your model is scoring cvs as part of a recruitment exercise and the kinds of skills candidates need in a particular job are likely to change every two years you should anticipate assessing the need to re train your fresh data at least that often,7,20
4741,4741,in other application domains where the main features dont change so often eg recognising handwritten digits you can anticipate less drift,7,22
4742,4742,you will need to assess this based on your own circumstances,7,24
4743,4743,you should always think carefully from the start whether it is appropriate to automate any prediction or decision making process,7,21
4744,4744,this should include assessing the effectiveness of the ai system in making statistically accurate predictions about the individuals whose personal data it processes,7,18
4745,4745,you should assess the merits of using a particular ai system in light of consideration of its effectiveness in making statistically accurate and therefore valuable predictions,7,21
4746,4746,not all ai systems demonstrate a sufficient level of statistical accuracy to justify their use,7,18
4747,4747,if you decide to adopt an ai system then to comply with the data protection principles you should ensure that all functions and individuals responsible for its development testing validation deployment and monitoring are adequately trained to understand the associated statistical accuracy requirements and measures,7,18
4748,4748,make sure data is clearly labelled as inferences and predictions and is not claimed to be factual,7,14
4749,4749,ensure you have managed trade offs and reasonable expectations,7,22
4750,4750,adopt a common terminology that staff can use to discuss statistical accuracy performance measures including their limitations and any adverse impact on individuals,7,18
4751,4751,as part of your obligation to implement data protection by design and by default you should consider statistical accuracy and the appropriate measures to evaluate it from the design phase and test these measures throughout the ai lifecycle,7,11
4752,4752,after deployment you should implement monitoring the frequency of which should be proportional to the impact an incorrect output may have on individuals,7,25
4753,4753,the higher the impact the more frequently you should monitor and report on it,7,17
4754,4754,you should also review your statistical accuracy measures regularly to mitigate the risk of concept drift,7,18
4755,4755,your change policy procedures should take this into account from the outset,7,22
4756,4756,statistical accuracy is also an important consideration if you outsource the development of an ai system to a third party either fully or partially or purchase an ai solution from an external vendor,7,18
4757,4757,in these cases you should examine and test any claims made by third parties as part of the procurement process,7,10
4758,4758,similarly you should agree regular updates and reviews of statistical accuracy to guard against changing population data and concept model drift,7,18
4759,4759,if you are a provider of ai services you should ensure that they are designed in such a way as to allow organisations to fulfil their data protection obligations,7,11
4760,4760,finally the vast quantity of personal data you may hold and process as part of your ai systems is likely to put pressure on any pre existing non ai processes you use to identify and if necessary rectify delete inaccurate personal data whether it is used as input or training test data,7,9
4761,4761,therefore you need to review your data governance practices and systems to ensure they remain fit for purpose,7,9
4762,4762,as ai systems learn from data which may be unbalanced andor reflect discrimination they may produce outputs which have discriminatory effects on people based on their gender race age health religion disability sexual orientation or other characteristics,7,6
4763,4763,the fact that ai systems learn from data does not guarantee that their outputs will not lead to discriminatory effects,7,6
4764,4764,the data used to train and test ai systems as well as the way they are designed and used might lead to ai systems which treat certain groups less favourably without objective justification,7,6
4765,4765,the following sections give guidance on interpreting the discrimination related requirements of data protection law in the context of ai as well as making some suggestions about best practice,7,11
4766,4766,the following sections do not aim to provide guidance on legal compliance with the uks anti discrimination legal framework notably the uk equality act,7,1
4767,4767,this sits alongside data protection law and applies to a range of organisations,7,11
4768,4768,it gives individuals protection from direct and indirect discrimination whether generated by a human or an automated decision making system or some combination of the two,7,6
4769,4769,demonstrating that an ai system is not unlawfully discriminatory under the ea is a complex task but it is separate and additional to your obligations relating to discrimination under data protection law,7,6
4770,4770,compliance with one will not guarantee compliance with the other,7,26
4771,4771,data protection law addresses concerns about unjust discrimination in several ways,7,6
4772,4772,first processing of personal data must be fair,7,9
4773,4773,fairness means you should handle personal data in ways people reasonably expect and not use it in ways that have unjustified adverse effects on them,7,6
4774,4774,any processing of personal data using ai that leads to unjust discrimination between people will violate the fairness principle,7,6
4775,4775,second data protection aims to protect individuals rights and freedoms with regard to the processing of their personal data,7,11
4776,4776,this includes the right to privacy but also the right to non discrimination,7,12
4777,4777,specifically the requirements of data protection by design and by default mean you have to implement appropriate technical and organisational measures to take into account the risks to the rights and freedoms of data subjects and implement the data protection principles effectively,7,11
4778,4778,similarly a data protection impact assessment should contain measures to address and mitigate those risks which include the risk of discrimination,7,6
4779,4779,third the uk gdpr specifically notes that processing personal data for profiling and automated decision making may give rise to discrimination and that you should use appropriate technical and organisational measures to prevent this,7,6
4780,4780,before addressing what data protection law requires you to do about the risk of ai and discrimination and suggesting best practices for compliance it is helpful to understand how these risks might arise,7,11
4781,4781,the following content contains some technical details so understanding how it may apply to your organisation may require attention of staff in both compliance and technical roles,7,19
4782,4782,a bank develops an ai system to calculate the credit risk of potential customers,7,0
4783,4783,the bank will use the ai system to approve or reject loan applications,7,0
4784,4784,the system is trained on a large dataset containing a range of information about previous borrowers such as their occupation income age and whether or not they repaid their loan,7,0
4785,4785,during testing the bank wants to check against any possible gender bias and finds the ai system tends to give women lower credit scores,7,0
4786,4786,in this case the ai system puts members of a certain group women at a disadvantage and so would appear to be discriminatory,7,6
4787,4787,note that this may not constitute unlawful discrimination under equalities law if the deployment of the ai system can be shown to be a proportionate means of achieving a legitimate aim,7,6
4788,4788,there are many different reasons why the system may be giving women lower credit scores,7,0
4789,4789,one is imbalanced training data,7,13
4790,4790,the proportion of different genders in the training data may not be balanced,7,6
4791,4791,for example the training data may include a greater proportion of male borrowers because in the past fewer women applied for loans and therefore the bank doesnt have enough data about women,7,9
4792,4792,machine learning algorithms used to create an ai system are designed to be the best fit for the data it is trained and tested on,7,13
4793,4793,if the men are over represented in the training data the model will pay more attention to the statistical relationships that predict repayment rates for men and less to statistical patterns that predict repayment rates for women which might be different,7,6
4794,4794,put another way because they are statistically less important the model may systematically predict lower loan repayment rates for women even if women in the training dataset were on average more likely to repay their loans than men,7,6
4795,4795,these issues will apply to any population under represented in the training data,7,6
4796,4796,for example if a facial recognition model is trained on a disproportionate number of faces belonging to a particular ethnicity and gender eg white men it will perform better when recognising individuals in that group and worse on others,7,6
4797,4797,another reason is that the training data may reflect past discrimination,7,6
4798,4798,for example if in the past loan applications from women were rejected more frequently than those from men due to prejudice then any model based on such training data is likely to reproduce the same pattern of discrimination,7,6
4799,4799,certain domains where discrimination has historically been a significant problem are more likely to experience this problem more acutely such as police stop and search of young black men or recruitment for traditionally male roles,7,6
4800,4800,these issues can occur even if the training data does not contain any protected characteristics like gender or race,7,6
4801,4801,a variety of features in the training data are often closely correlated with protected characteristics eg occupation,7,12
4802,4802,these proxy variables enable the model to reproduce patterns of discrimination associated with those characteristics even if its designers did not intend this,7,6
4803,4803,these problems can occur in any statistical model so the following considerations may apply to you even if you dont consider your statistical models to be ai,7,25
4804,4804,however they are more likely to occur in ai systems because they can include a greater number of features and may identify complex combinations of features which are proxies for protected characteristics,7,4
4805,4805,many modern ml methods are more powerful than traditional statistical approaches because they are better at uncovering non linear patterns in high dimensional data,7,13
4806,4806,however these may also include patterns that reflect discrimination,7,6
4807,4807,other causes of potentially discriminatory ai systems include prejudices or bias in the way variables are measured labelled or aggregated,7,6
4808,4808,biased cultural assumptions of developers,7,6
4809,4809,inappropriately defined objectives eg where the best candidate for a job embeds assumptions about gender race or other characteristics,7,6
4810,4810,the way the model is deployed eg via a user interface which doesnt meet accessibility requirements,7,20
4811,4811,while discrimination is a broader problem that cannot realistically be fixed through technology various approaches exist which aim to mitigate ai driven discrimination,7,6
4812,4812,computer scientists and others have been developing different mathematical techniques to measure how ml models treat individuals from different groups in potentially discriminatory ways and reduce them,7,6
4813,4813,this field is often referred to as algorithmic fairness,7,6
4814,4814,the techniques it proposes do not necessarily align with relevant non discrimination law in the uk and in some cases may contradict it so should not be relied upon as a means of complying with such obligations,7,6
4815,4815,however depending on your context some of these approaches may be appropriate technical measures to ensure personal data processing is fair and to minimise the risks of discrimination arising from it,7,11
4816,4816,in cases of imbalanced training data it may be possible to balance it out by adding or removing data about under overrepresented subsets of the population eg adding more data points on loan applications from women,7,6
4817,4817,in cases where the training data reflects past discrimination you could either modify the data change the learning process or modify the model after training,7,13
4818,4818,in order to measure whether these techniques are effective there are various mathematical fairness measures against which you can measure the results,7,18
4819,4819,simply removing any protected characteristics from the inputs the model uses to make a prediction is unlikely to be enough as there are often variables which are proxies for the protected characteristics,7,20
4820,4820,other measures involve comparing how the ai system distributes positive or negative outcomes or errors between protected groups,7,6
4821,4821,some of these measures conflict with each other meaning you cannot satisfy all of them at the same time,7,26
4822,4822,which of these measures are most appropriate and in what combinations if any will depend on your context as well as any applicable relevant laws eg equality law,7,24
4823,4823,you should also consider the impact of these techniques on the statistical accuracy of the ai systems performance,7,4
4824,4824,for example to reduce the potential for discrimination you might modify a credit risk model so that the proportion of positive predictions between people with different protected characteristics eg men and women are equalised,7,6
4825,4825,this may help prevent discriminatory outcomes but it could also result in a higher number of statistical errors overall which you will also need to manage as well,7,6
4826,4826,in practice there may not always be a tension between statistical accuracy and avoiding discrimination,7,6
4827,4827,for example if discriminatory outcomes in the model are driven by a relative lack of data about a statistically small minority of the population then statistical accuracy of the model could be increased by collecting more data about them whilst also equalising the proportions of correct predictions,7,6
4828,4828,however in that case you would face a different choice between collecting more data on the minority population in the interests of reducing the disproportionate number of statistical errors they face,7,6
4829,4829,not collecting such data due to the risks doing so may pose to the other rights and freedoms of those individuals,7,9
4830,4830,depending on your context you may also have other sector specific regulatory obligations regarding statistical accuracy or discrimination which you will need to be consider alongside your data protection obligations,7,9
4831,4831,if you need to process data in a certain way to meet those obligations data protection does not prevent you from doing so,7,11
4832,4832,in order to assess and address the potential for discrimination in an ai system you may need a dataset containing example individuals with labels for the protected characteristics of interest such as those outlined in the equality act,7,6
4833,4833,you could then use this dataset to test how the system would perform with each protected group and also potentially to re train the model to avoid discriminatory effects,7,6
4834,4834,before doing this kind of analysis you need to ensure you have an appropriate lawful basis to process the data for such purposes,7,24
4835,4835,there are different data protection considerations depending on the kinds of discrimination you are testing for,7,6
4836,4836,if you are testing a system for discriminatory impact by age or sex gender there are no special data protection conditions for processing these protected characteristics because they are not classified as special category data in data protection law,7,6
4837,4837,you still need to consider the broader questions of lawfulness fairness and the risks the processing poses as a whole,7,6
4838,4838,the possibility for the data to either be special category data anyway or becoming so during the processing ie if the processing involves analysing or inferring any data to do with health or genetic status,7,9
4839,4839,you should also note that when you are dealing with personal data that results from specific technical processing about the physical physiological or behavioural characteristics of an individual and allows or confirms that individuals unique identification that data is biometric data,7,9
4840,4840,where you use biometric data for the purpose of uniquely identifying an individual it is also special category data,7,12
4841,4841,so if you use biometric data for testing and mitigating discrimination in your ai system but not for the purpose of confirming the identity of the individuals within the dataset or making any kind of decision in relation to them the biometric data does not come under article,7,9
4842,4842,the data is still regarded as biometric data under the uk gdpr but is not special category data,7,9
4843,4843,similarly if the personal data does not allow or confirm an individuals unique identification then it is not biometric data or special category data,7,9
4844,4844,however some of the protected characteristics outlined in the equality act are classified as special category data,7,6
4845,4845,these include race religion or belief and sexual orientation,7,6
4846,4846,they may also include disability pregnancy and gender reassignment in so far as they may reveal information about a persons health,7,12
4847,4847,similarly because civil partnerships were until recently only available to same sex couples data that indicates someone is in a civil partnership may indirectly reveal their sexual orientation,7,9
4848,4848,if you are testing an ai system for discriminatory impact on the basis of these characteristics you are likely to need to process special category data,7,6
4849,4849,in order to do this lawfully in addition to having a lawful basis under article  you need to meet one of the conditions in article  of the uk gdpr,7,2
4850,4850,some of these also require additional basis or authorisation in uk law which can be found in schedule  of the dpa,7,26
4851,4851,which if any of these conditions for processing special category data are appropriate depends on your individual circumstances,7,24
4852,4852,an organisation using a cv scoring ai system to assist with recruitment decisions needs to test whether its system might be discriminating by religious or philosophical beliefs,7,6
4853,4853,while the system does not directly use information about the applicants religion there might be features in the system which are indirect proxies for religion such as previous occupation or qualifications,7,26
4854,4854,in a labour market where certain religious groups have been historically excluded from particular professions a cv scoring system may unfairly under rate candidates on the basis of those proxies,7,6
4855,4855,the organisation collects the religious beliefs of a sample of job applicants in order to assess whether the system is indeed producing disproportionately negative outcomes or erroneous predictions for applicants with particular religious beliefs,7,6
4856,4856,the organisation relies on the substantial public interest condition in article  and the equality of opportunity or treatment condition in schedule   of the dpa,7,17
4857,4857,this provision can be used to identify or keep under review the existence or absence of equality of opportunity or treatment between certain protected groups with a view to enabling such equality to be promoted or maintained,7,6
4858,4858,a university researcher is investigating whether facial recognition systems perform differently on the faces of people of different racial or ethnic origin as part of a research project,7,12
4859,4859,in order to do this the researcher assigns racial labels to an existing dataset of faces that the system will be tested on thereby processing special category data,7,6
4860,4860,they rely on the archiving research and statistics condition in article  read with schedule  paragraph  of the dpa,7,22
4861,4861,finally if the protected characteristics you are using to assess and improve potentially discriminatory ai were originally processed for a different purpose you should consider whether your new purpose is compatible with the original purpose,7,6
4862,4862,how you will obtain fresh consent if required,7,21
4863,4863,for example if the data was initially collected on the basis of consent even if the new purpose is compatible you still need to collect a fresh consent for the new purpose,7,9
4864,4864,if the new purpose is incompatible how you will ask for consent,7,21
4865,4865,using special category data to assess the potential discriminatory impacts of ai systems does not usually constitute automated decision making under data protection law,7,6
4866,4866,this is because it does not involve directly making any decisions about individuals,7,21
4867,4867,similarly re training a discriminatory model with data from a more diverse population to reduce its discriminatory effects does not involve directly making decisions about individuals and is therefore not classed as a decision with legal or similarly significant effect,7,6
4868,4868,however in some cases simply re training the ai model with a more diverse training set may not be enough to sufficiently mitigate its discriminatory impact,7,6
4869,4869,rather than trying to make a model fair by ignoring protected characteristics when making a prediction some approaches directly include such characteristics when making a classification to ensure members of potentially disadvantaged groups are protected,7,6
4870,4870,including protective characteristics could one of the measures you take to comply with the requirement to make reasonable adjustments under the equality act,7,27
4871,4871,for example if you were using an ai system to assist with sorting job applicants rather than attempting to create a model which ignores a persons disability it may be more effective to include their disability status in order to ensure the system does not indirectly discriminate against them,7,6
4872,4872,not including disability status as an input to the automated decision could mean the system is more likely to indirectly discriminate against people with a disability because it will not factor in the effect of their condition on other features used to make a prediction,7,6
4873,4873,however if you process disability status using an ai system to make decisions about individuals which produce legal or similarly significant effects on them you must have explicit consent from the individual or be able to meet one of the substantial public interest conditions laid out in schedule  of the dpa,7,21
4874,4874,you need to carefully assess which conditions in schedule  may apply,7,22
4875,4875,for example the equality of opportunity monitoring provision mentioned above cannot be relied on in such contexts because the processing is carried out for the purposes of decisions about a particular individual,7,26
4876,4876,therefore such approaches will only be lawful if based on a different substantial public interest condition in schedule,7,21
4877,4877,there are many contexts in which non protected characteristics such as the postcode you live in are proxies for a protected characteristic like race,7,12
4878,4878,recent advances in machine learning such as deep learning have made it even easier for ai systems to detect patterns in the world that are reflected in seemingly unrelated data,7,13
4879,4879,unfortunately this also includes detecting patterns of discrimination using complex combinations of features which might be correlated with protected characteristics in non obvious ways,7,6
4880,4880,for example an ai system used to score job applications to assist a human decision maker with recruitment decisions might be trained on examples of previously successful candidates,7,4
4881,4881,the information contained in the application itself may not include protected characteristics like race disability or mental health,7,27
4882,4882,however if the examples of employees used to train the model were discriminated against on those grounds eg by being systematically under rated in performance reviews the algorithm may learn to reproduce that discrimination by inferring those characteristics from proxy data contained in the job application despite the designer never intending it to,7,6
4883,4883,so even if you dont use protected characteristics in your model it is very possible that you may inadvertently use a model which has detected patterns of discrimination based on those protected characteristics and is reproducing them in its outputs,7,20
4884,4884,as described above some of those protected characteristics are also special category data,7,11
4885,4885,special category data is defined as personal data that reveals or concerns the special categories,7,9
4886,4886,if the model learns to use particular combinations of features that are sufficiently revealing of a special category then the model may be processing special category data,7,20
4887,4887,as stated in our guidance on special category data if you use profiling with the intention of inferring special category data then this is special category data irrespective of whether the inferences are incorrect,7,18
4888,4888,furthermore for the reasons stated above there may also be situations where your model infers special category as an intermediate step to another non special category data inference,7,20
4889,4889,you may not be able to tell if your model is doing this just by looking at the data that went into the model and the outputs that it produces,7,20
4890,4890,it may do so with high statistical accuracy even though you did not intend for it to do so,7,18
4891,4891,if you are using machine learning with personal data you should proactively  assess the chances that your model might be inferring protected characteristics or special category data or both in order to make predictions and actively monitor this possibility throughout the lifecycle of the system,7,20
4892,4892,if your system is indeed inferring special category or criminal conviction data whether unintentional or not you must have an appropriate article  or  condition for processing,7,12
4893,4893,if it is unclear whether or not your system may be inferring such data you may want to identify a condition to cover that possibility and reduce your compliance risk although this is not a legal requirement,7,9
4894,4894,as noted above if you are using such a model to make legal or similarly significant decisions in a solely automated way this is only lawful if you have the persons consent or you meet the substantial public interest condition and an appropriate provision in schedule,7,21
4895,4895,the most appropriate approach to managing the risk of discriminatory outcomes in ml systems will depend on the particular domain and context you are operating in,7,6
4896,4896,you should determine and document your approach to bias and discrimination mitigation from the very beginning of any ai application lifecycle so that you can take into account and put in place the appropriate safeguards and technical measures during the design and build phase,7,6
4897,4897,establishing clear policies and good practices for the procurement and lawful processing of high quality training and test data is important especially if you do not have enough data internally,7,9
4898,4898,whether procured internally or externally you should satisfy yourself that the data is representative of the population you apply the ml system to although for reasons stated above this will not be sufficient to ensure fairness,7,6
4899,4899,for example for a high street bank operating in the uk the training data could be compared against the most recent census,7,24
4900,4900,your senior management should be responsible for signing off the chosen approach to manage discrimination risk and be accountable for its compliance with data protection law,7,6
4901,4901,while they are able to leverage expertise from technology leads and other internal or external subject matter experts to be accountable your senior leaders still need to have a sufficient understanding of the limitations and advantages of the different approaches,7,16
4902,4902,this is also true for dpos and senior staff in oversight functions as they will be expected to provide ongoing advice and guidance on the appropriateness of any measures and safeguards put in place to mitigate discrimination risk,7,6
4903,4903,in many cases choosing between different risk management approaches will require trade offs,7,3
4904,4904,this includes choosing between safeguards for different protected characteristics and groups,7,11
4905,4905,you need to document and justify the approach you choose,7,21
4906,4906,trade offs driven by technical approaches are not always obvious to non technical staff so data scientists should highlight and explain these proactively to business owners as well as to staff with responsibility for risk management and data protection compliance,7,0
4907,4907,your technical leads should also be proactive in seeking domain specific knowledge including known proxies for protected characteristics to inform algorithmic fairness approaches,7,6
4908,4908,you should undertake robust testing of any anti discrimination measures and should monitor your ml systems performance on an ongoing basis,7,6
4909,4909,your risk management policies should clearly set out both the process and the person responsible for the final validation of an ml system both before deployment and where appropriate after an update,7,20
4910,4910,for discrimination monitoring purposes your organisational policies should set out any variance tolerances against the selected key performance metrics as well as escalation and variance investigation procedures,7,6
4911,4911,you should also clearly set variance limits above which the ml system should stop being used,7,22
4912,4912,if you are replacing traditional decision making systems with ai you should consider running both concurrently for a period of time,7,21
4913,4913,you should investigate any significant difference in the type of decisions eg loan acceptance or rejection for different protected groups between the two systems and any differences in how the ai system was predicted to perform and how it does in practice,7,21
4914,4914,beyond the requirements of data protection law a diverse workforce is a powerful tool in identifying and managing bias and discrimination in ai systems and in the organisation more generally,7,6
4915,4915,finally this is an area where best practice and technical approaches continue to develop,7,27
4916,4916,you should invest the time and resources to ensure you continue to follow best practice and your staff remain appropriately trained on an ongoing basis,7,27
4917,4917,in some cases ai may actually provide an opportunity to uncover and address existing discrimination in traditional decision making processes and allow you to address any underlying discriminatory practices,7,6
4918,4918,this section explains how ai systems can exacerbate known security risks and make them more difficult to manage,7,11
4919,4919,it also presents the challenges for compliance with the data minimisation principle,7,11
4920,4920,a number of techniques are presented to help both data minimisation and effective ai development and deployment,7,11
4921,4921,this section is aimed at technical specialists who are best placed to assess the security of an ai system and what personal data is required,7,11
4922,4922,it will also be useful for those in compliance focused roles to understand the risks associated with security and data minimisation in ai,7,11
4923,4923,you must process personal data in a manner that ensures appropriate levels of security against its unauthorised or unlawful processing accidental loss destruction or damage,7,11
4924,4924,in this section we focus on the way ai can adversely affect security by making known risks worse and more challenging to control,7,3
4925,4925,there is no one size fits all approach to security,7,11
4926,4926,the appropriate security measures you should adopt depend on the level and type of risks that arise from specific processing activities,7,25
4927,4927,using ai to process any personal data has important implications for your security risk profile and you need to assess and manage these carefully,7,9
4928,4928,some implications may be triggered by the introduction of new types of risks eg adversarial attacks on machine learning models see section what types of privacy attacks apply to ai models,7,3
4929,4929,information security is a key component of our ai auditing framework but is also central to our work as the information rights regulator,7,0
4930,4930,the ico is planning to expand its general security guidance to take into account the additional requirements set out in the new uk gdpr,7,11
4931,4931,while this guidance will not be ai specific it will cover a range of topics that are relevant for organisations using ai including software supply chain security and increasing use of open source software,7,2
4932,4932,some of the unique characteristics of ai mean compliance with data protection laws security requirements can be more challenging than with other more established technologies both from a technological and human perspective,7,11
4933,4933,from a technological perspective ai systems introduce new kinds of complexity not found in more traditional it systems that you may be used to using,7,4
4934,4934,depending on the circumstances your use of ai systems is also likely to rely heavily on third party code relationships with suppliers or both,7,25
4935,4935,also your existing systems need to be integrated with several other new and existing it components which are also intricately connected,7,25
4936,4936,since ai systems operate as part of a larger chain of software components data flows organisational workflows and business processes you should take a holistic approach to security,7,11
4937,4937,this complexity may make it more difficult to identify and manage some security risks and may increase others such as the risk of outages,7,3
4938,4938,from a human perspective the people involved in building and deploying ai systems are likely to have a wider range of backgrounds than usual including traditional software engineering systems administration data scientists statisticians as well as domain experts,7,4
4939,4939,security practices and expectations may vary significantly and for some there may be less understanding of broader security compliance requirements as well as those of data protection law more specifically,7,11
4940,4940,security of personal data may not always have been a key priority especially if someone was previously building ai applications with non personal data or in a research capacity,7,9
4941,4941,further complications arise because common practices about how to process personal data securely in data science and ai engineering are still under development,7,11
4942,4942,as part of your compliance with the security principle you should ensure that you actively monitor and take into account the state of the art security practices when using personal data in an ai context,7,9
4943,4943,it is not possible to list all known security risks that might be exacerbated when you use ai to process personal data,7,9
4944,4944,the impact of ai on security depends on the way the technology is built and deployed,7,4
4945,4945,the complexity of the organisation deploying it,7,20
4946,4946,the strength and maturity of the existing risk management capabilities,7,3
4947,4947,the nature scope context and purposes of the processing of personal data by the ai system and the risks posed to individuals as a result,7,9
4948,4948,the following hypothetical scenarios are intended to raise awareness of some of the known security risks and challenges that ai can exacerbate,7,25
4949,4949,the following content contains some technical details so understanding how it may apply to your organisation may require attention of staff in both compliance and technical roles,7,19
4950,4950,our key message is that you should review your risk management practices ensuring personal data is secure in an ai context,7,3
4951,4951,ml systems require large sets of training and testing data to be copied and imported from their original context of processing shared and stored in a variety of formats and places including with third parties,7,13
4952,4952,this can make them more difficult to keep track of and manage,7,22
4953,4953,your technical teams should record and document all movements and storing of personal data from one location to another,7,24
4954,4954,this will help you apply appropriate security risk controls and monitor their effectiveness,7,11
4955,4955,clear audit trails are also necessary to satisfy accountability and documentation requirements,7,18
4956,4956,in addition you should delete any intermediate files containing personal data as soon as they are no longer required eg compressed versions of files created to transfer data between systems,7,22
4957,4957,depending on the likelihood and severity of the risk to individuals you may also need to apply de identification techniques to training data before it is extracted from its source and shared internally or externally,7,13
4958,4958,for example you may need to remove certain features from the data or apply privacy enhancing technologies pets before sharing it with another organisation,7,9
4959,4959,how should we ensure security of externally maintained software used to build ai systems,7,25
4960,4960,very few organisations build ai systems entirely in house,7,25
4961,4961,in most cases the design building and running of ai systems will be provided at least in part by third parties that you may not always have a contractual relationship with,7,25
4962,4962,even if you hire your own ml engineers you may still rely significantly on third party frameworks and code libraries,7,25
4963,4963,many of the most popular ml development frameworks are open source,7,26
4964,4964,using third party and open source code is a valid option,7,25
4965,4965,developing all software components of an ai system from scratch requires a large investment of time and resources that many organisations cannot afford and especially compared to open source tools would not benefit from the rich ecosystem of contributors and services built up around existing frameworks,7,25
4966,4966,however one important drawback is that these standard ml frameworks often depend on other pieces of software being already installed on an it system,7,26
4967,4967,to give a sense of the risks involved a recent study found the most popular ml development frameworks include up to  lines of code and rely on  external dependencies,7,3
4968,4968,therefore implementing ai will require changes to an organisations software stack and possibly hardware that may introduce additional security risks,7,25
4969,4969,the recruiter hires an ml engineer to build the automated cv filtering system using a python based ml framework,7,27
4970,4970,the ml framework depends on a number of specialist open source programming libraries which needed to be downloaded on the recruiters it system,7,26
4971,4971,one of these libraries contains a software function to convert the raw training data into the format required to train the ml model,7,13
4972,4972,it is later discovered the function has a security vulnerability,7,25
4973,4973,due to an unsafe default configuration an attacker introduced and executed malicious code remotely on the system by disguising it as training data,7,13
4974,4974,this is not a far fetched example in january of  such a vulnerability was discovered in numpy a popular library for the python programming language used by many machine learning developers,7,13
4975,4975,whether ai systems are built in house externally or a combination of both you will need to assess them for security risks,7,25
4976,4976,as well as ensuring the security of any code developed in house you need to assess the security of any externally maintained code and frameworks,7,25
4977,4977,in many respects the standard requirements for maintaining code and managing security risks will apply to ai applications,7,11
4978,4978,for example your external code security measures should include subscribing to security advisories to be notified of vulnerabilities,7,25
4979,4979,your internal code security measures should include adhering to coding standards and instituting source code review processes,7,11
4980,4980,whatever your approach you should ensure that your staff have appropriate skills and knowledge to address these security risks,7,25
4981,4981,having a secure pipeline from development to deployment will further mitigate security risks associated with third party code by separating the ml development environment from the rest of your it infrastructure where possible,7,20
4982,4982,using virtual machines or containers emulations of a computer system that run inside but isolated from the rest of the it system may help here these can be pre configured specifically for ml tasks,7,25
4983,4983,in addition it is possible to train an ml model using a programming language and framework suitable for exploratory development but then convert the model into another more secure format for deployment,7,20
4984,4984,the ico is developing further security guidance which will include additional recommendations for the oversight and review of externally maintained source code from a data protection perspective as well as its implications for security and data protection by design,7,11
4985,4985,the personal data of the people who an ai system was trained on might be inadvertently revealed by the outputs of the system itself,7,9
4986,4986,it is normally assumed that the personal data of the individuals whose data was used to train an ai system cannot be inferred by simply observing the predictions the system returns in response to new inputs,7,9
4987,4987,however new types of privacy attacks on ml models suggest that this is sometimes possible,7,20
4988,4988,in this section we focus on two kinds of these privacy attacks  model inversion and membership inference,7,12
4989,4989,in a model inversion attack if attackers already have access to some personal data belonging to specific individuals included in the training data they can infer further personal information about those same individuals by observing the inputs and outputs of the ml model,7,20
4990,4990,the information attackers can learn about goes beyond generic inferences about individuals with similar characteristics,7,12
4991,4991,an early demonstration of this kind of attack concerned a medical model designed to predict the correct dosage of an anticoagulant using patient data including genetic biomarkers,7,13
4992,4992,it proved that an attacker with access to some demographic information about the individuals included in the training data could infer their genetic biomarkers from the model despite not having access to the underlying training data,7,13
4993,4993,another recent example demonstrates that attackers could reconstruct images of faces that a facial recognition technology frt system has been trained to recognise,7,12
4994,4994,frt systems are often designed to allow third parties to query the model,7,20
4995,4995,when the model is given the image of a person whose face it recognises the model returns its best guess as to the name of the person and the associated confidence rate,7,12
4996,4996,attackers could probe the model by submitting many different randomly generated face images,7,12
4997,4997,by observing the names and the confidence scores returned by the model they could reconstruct the face images associated with the individuals included in the training data,7,12
4998,4998,while the reconstructed face images were imperfect researchers found that they could be matched by human reviewers to the individuals in the training data with  accuracy,7,18
4999,4999,membership inference attacks allow malicious actors to deduce whether a given individual was present in the training data of a ml model,7,20
5000,5000,however unlike in model inversion they dont necessarily learn any additional personal data about the individual,7,9
5001,5001,for example if hospital records are used to train a model which predicts when a patient will be discharged attackers could use that model in combination with other data about a particular individual that they already have to work out if they were part of the training data,7,13
5002,5002,this would not reveal any individuals data from the training data set itself but in practice it would reveal that they had visited one of the hospitals that generated the training data during the period the data was collected,7,9
5003,5003,similar to the earlier frt example membership inference attacks can exploit confidence scores provided alongside a models prediction,7,20
5004,5004,if an individual was in the training data then the model will be disproportionately confident in a prediction about that person because it has seen them before,7,13
5005,5005,this allows the attacker to infer that the person was in the training data,7,13
5006,5006,the gravity of the consequences of models vulnerability to membership inference will depend on how sensitive or revealing membership might be,7,20
5007,5007,if a model is trained on a large number of people drawn from the general population then membership inference attacks pose less risk,7,6
5008,5008,but if the model is trained on a vulnerable or sensitive population eg patients with dementia or hiv then merely revealing that someone is part of that population may be a serious privacy risk,7,9
5009,5009,there is an important distinction between black box and white box attacks on models,7,20
5010,5010,these two approaches correspond to different operational models,7,20
5011,5011,in white box attacks the attacker has complete access to the model itself and can inspect its underlying code and properties although not the training data,7,20
5012,5012,for example some ai providers give third parties an entire pre trained model and allow them to run it locally,7,20
5013,5013,white box attacks enable additional information to be gathered such as the type of model and parameters used which could help an attacker in inferring personal data from the model,7,20
5014,5014,in black box attacks the attacker only has the ability to query the model and observe the relationships between inputs and outputs,7,20
5015,5015,for example many ai providers enable third parties to access the functionality of an ml model online to send queries containing input data and receive the models response,7,4
5016,5016,the examples we have highlighted above are both black box attacks,7,25
5017,5017,white and black box attacks can be performed by providers customers or anyone else with either authorised or unauthorised access to either the model itself or its query or response functionality,7,20
5018,5018,model inversion and membership inferences show that ai models can inadvertently contain personal data,7,9
5019,5019,you should also note that there are certain kinds of ml models which actually contain parts of the training data in its raw form within them by design,7,13
5020,5020,for example support vector machines svms and k nearest neighbours knn models contain some of the training data in the model itself,7,13
5021,5021,in these cases if the training data is personal data access to the model by itself means that the organisation purchasing the model will already have access to a subset of the personal data contained in the training data without having to exert any further efforts,7,9
5022,5022,providers of such ml models and any third parties procuring them should be aware that they may contain personal data in this way,7,9
5023,5023,unlike model inversion and membership inference personal data contained in models like this is not an attack vector,7,9
5024,5024,any personal data contained in these models would be there by design and easily retrievable by the third party,7,9
5025,5025,storing and using these models therefore constitutes processing of personal data and as such the standard data protection provisions apply,7,11
5026,5026,if you train models and provide them to others you should assess whether those models may contain personal data or are at risk of revealing it if attacked and take appropriate steps to mitigate these risks,7,20
5027,5027,you should assess whether the training data contains identified or identifiable personal data of individuals either directly or by those who may have access to the model,7,9
5028,5028,you should assess the means that may be reasonably likely to be used in light of the vulnerabilities described above,7,25
5029,5029,as this is a rapidly developing area you should stay up to date with the state of the art in both methods of attack and mitigation,7,25
5030,5030,security and ml researchers are still working to understand what factors make ml models more or less vulnerable to these kinds of attacks and how to design effective protections and mitigation strategies,7,20
5031,5031,one possible cause of ml models being vulnerable to privacy attacks is known as overfitting,7,20
5032,5032,this is where the model pays too much attention to the details of the training data effectively almost remembering particular examples from the training data rather than just the general patterns,7,13
5033,5033,overfitting can happen where there are too many features included or where there are too few examples in the training data or both,7,13
5034,5034,model inversion and membership inference attacks can exploit this,7,20
5035,5035,avoiding overfitting will help both in mitigating the risk of privacy attacks and also in ensuring that the model is able to make good inferences on new examples it hasnt seen before,7,20
5036,5036,however avoiding overfitting will not completely eliminate the risks,7,25
5037,5037,even models which are not overfitted to the training data can still be vulnerable to privacy attacks,7,20
5038,5038,in cases where confidence information provided by a ml system can be exploited as in the frt example above the risk could be mitigated by not providing it to the end user,7,9
5039,5039,this would need to be balanced against the need for genuine end users to know whether or not to rely on its output and will depend on the particular use case and context,7,25
5040,5040,if you are going to provide a whole model to others via an application programming interface api you will not be subject to white box attacks in this way because the apis users will not have direct access to the model itself,7,20
5041,5041,however you might still be subjected to black box attacks,7,25
5042,5042,to mitigate this risk you could monitor queries from the apis users in order to detect whether it is being used suspiciously,7,25
5043,5043,this may indicate a privacy attack and would require prompt investigation and potential suspension or blocking of a particular user account,7,12
5044,5044,such measures may become part of common real time monitoring techniques used to protect against other security threats such as rate limiting reducing the number of queries that can be performed by a particular user in a given time limit,7,11
5045,5045,if your model is going to be provided in whole to a third party rather than being merely accessible to them via an api then you will need to consider the risk of white box attacks,7,20
5046,5046,as the model provider you will be less easily able to monitor the model during deployment and thereby assess and mitigate the risk of privacy attacks on it,7,20
5047,5047,however you remain responsible for assessing and mitigating the risk that personal data used to train your models may be exposed as a result of the way your clients have deployed the model,7,20
5048,5048,you may not be able to fully assess this risk without collaborating with your clients to understand the particular deployment contexts and associated threat models,7,3
5049,5049,as part of your procurement policy there should be sufficient information sharing between each party to perform your respective assessments as necessary,7,1
5050,5050,in some cases ml model providers and clients will be joint controllers and therefore need to perform a joint risk assessment,7,20
5051,5051,in cases where the model actually contains examples from the training data by default as in svms and knns this is a transfer of personal data and you should treat it as such,7,13
5052,5052,recent research has demonstrated how some proposed methods to make ml models explainable can unintentionally make it easier to conduct privacy attacks on models,7,14
5053,5053,for example when providing an explanation to individuals there may be a risk that doing so reveals proprietary information about how the ai model works,7,14
5054,5054,however you must take care not to conflate commercial interests with data protection requirements eg commercial security and data protection security and instead you should consider the extent to which such a trade off genuinely exists,7,11
5055,5055,given that the kind of explanations you may need to provide to data subjects about ai need to be in a concise transparent intelligible and easily accessible form using clear and plain language they will not normally risk commercially sensitive information,7,14
5056,5056,however there may be cases where you need to consider the right of individuals to receive an explanation and for example the interests of businesses to maintain trade secrets noting that data protection compliance cannot be traded away,7,14
5057,5057,both of these risks are active areas of research and their likelihood and severity are the subject of debate and investigation,7,3
5058,5058,we will continue to monitor and review these risks and may update this guidance accordingly,7,3
5059,5059,while the main data protection concerns about ai involve accidentally revealing personal data there are other potential novel ai security risks such as adversarial examples,7,11
5060,5060,these are examples fed to an ml model which have been deliberately modified so that they are reliably misclassified,7,13
5061,5061,these can be images which have been manipulated or even real world modifications such as stickers placed on the surface of the item,7,12
5062,5062,examples include pictures of turtles which are classified as guns or road signs with stickers on them which a human would instantly recognise as a stop but an image recognition model does not,7,12
5063,5063,while such adversarial examples are concerning from a security perspective they might not raise data protection concerns if they dont involve personal data,7,25
5064,5064,the security principle refers to security of the personal data  protecting it against unauthorised processing,7,11
5065,5065,however adversarial attacks dont necessarily involve unauthorised processing of personal data only a compromise to the system,7,25
5066,5066,however there may be cases in which adversarial examples can be a risk to the rights and freedoms of individuals,7,12
5067,5067,for example some attacks have been demonstrated on facial recognition systems,7,12
5068,5068,by slightly distorting the face image of one individual an adversary can trick the facial recognition system into misclassifying them as another even though a human would still recognise the distorted image as the correct individual,7,12
5069,5069,this would raise concerns about the systems statistical accuracy especially if the system is used to make legal or similarly significant decisions about individuals,7,6
5070,5070,you may also need to consider the risk of adversarial examples as part of your obligations under the network and information systems regulations  nis,7,25
5071,5071,the ico is the competent authority for relevant digital service providers under nis,7,23
5072,5072,these include online search engines online marketplaces and cloud computing services,7,7
5073,5073,a nis incident includes incidents which compromise the data stored by network and information systems and the related services they provide,7,24
5074,5074,this is likely to include ai cloud computing services,7,4
5075,5075,so even if an adversarial attack does not involve personal data it may still be a nis incident and therefore within the icos remit,7,9
5076,5076,the data minimisation principle requires you to identify the minimum amount of personal data you need to fulfil your purpose and to only process that information and no more,7,11
5077,5077,however ai systems generally require large amounts of data,7,4
5078,5078,at first glance it may therefore be difficult to see how ai systems can comply with the data minimisation principle yet if you are using ai as part of your processing you are still required to do so,7,11
5079,5079,whilst it may appear challenging in practice this may not be the case,7,25
5080,5080,the data minimisation principle does not mean either process no personal data or if we process more were going to break the law,7,9
5081,5081,the key is that you only process the personal data you need for your purpose,7,9
5082,5082,how you go about determining what is adequate relevant and limited is therefore going to be specific to your circumstances and our existing guidance on data minimisation details the steps you should take,7,24
5083,5083,in the context of ai systems what is adequate relevant and limited is therefore also case specific,7,4
5084,5084,however there are a number of techniques that you can adopt in order to develop ai systems that process only the data you need while still remaining functional,7,4
5085,5085,in this section we explore some of the most relevant techniques for supervised machine learning ml systems which are currently the most common type of ai in use,7,13
5086,5086,within your organisations the individuals accountable for the risk management and compliance of ai systems need to be aware that such techniques exist and be able to discuss and assess different approaches with your technical staff,7,3
5087,5087,for example the default approach of data scientists in designing and building ai systems might involve collecting and using as much data as possible without thinking about ways they could achieve the same purposes with less data,7,9
5088,5088,you must therefore implement risk management practices designed to ensure that data minimisation and all relevant minimisation techniques are fully considered from the design phase,7,11
5089,5089,similarly if you buy in ai systems or implement systems operated by third parties or both these considerations should form part of the procurement process due diligence,7,0
5090,5090,you should also be aware that while they may help you comply with the principle of data minimisation the techniques described here do not eliminate other kinds of risk,7,11
5091,5091,also while some techniques will not require any compromise to comply with data minimisation requirements others may need you to balance data minimisation with other compliance or utility objectives,7,11
5092,5092,for example making more statistically accurate and non discriminatory ml models,7,13
5093,5093,the first step you should take towards compliance with data minimisation is to understand and map out all the ml processes in which personal data might be used,7,11
5094,5094,supervised ml algorithms can be trained to identify patterns and create models from datasets training data which include past examples of the type of instances the model will be asked to classify or predict,7,13
5095,5095,specifically the training data contains both the target variable ie the thing that the model is aiming to predict or classify and several predictor variables ie the input used to make the prediction,7,13
5096,5096,for example in the training data for a banks credit risk ml model the predictor variables might include the age income occupation and location of previous customers while the target variable will be whether or not the customers repaid their loan,7,13
5097,5097,once trained ml systems can then classify and make predictions based on new data containing examples that the system has never seen before,7,13
5098,5098,a query is sent to the ml model containing the predictor variables for a new instance eg a new customers age income occupation,7,20
5099,5099,the model responds with its best guess as to the target variable for this new instance eg whether or not the new customer will default on a loan,7,20
5100,5100,supervised ml approaches therefore use data in two main phases the training phase when training data is used to develop models based on past examples,7,13
5101,5101,the inference phase when the model is used to make a prediction or classification about new instances,7,20
5102,5102,if the model is used to make predictions or classifications about individual people then it is very likely that personal data will be used at both the training and inference phases,7,9
5103,5103,when designing and building ml applications data scientists will generally assume that all data used in training testing and operating the system will be aggregated in a centralised way and held in its full and original form by a single entity in multiple places throughout the ai systems lifecycle,7,13
5104,5104,however where this is personal data you need to consider whether it is necessary to process it for your purposes,7,9
5105,5105,if you can achieve the same outcome by processing less personal data then by definition the data minimisation principle requires you to do so,7,9
5106,5106,a number of techniques exist which can help you to minimise the amount of personal data you need to process,7,9
5107,5107,as we have explained the training phase involves applying a learning algorithm to a dataset containing a set of features for each individual which are used to generate the prediction or classification,7,13
5108,5108,however not all features included in a dataset will necessarily be relevant to your purpose,7,13
5109,5109,for example not all financial and demographic features will be useful to predict credit risk,7,26
5110,5110,therefore you need to assess which features  and therefore what data  are relevant for your purpose and only process that data,7,21
5111,5111,there are a variety of standard feature selection methods used by data scientists to select features which will be useful for inclusion in a model,7,13
5112,5112,these methods are good practice in data science but they also go some way towards meeting the data minimisation principle,7,13
5113,5113,also as discussed in the icos previous report on ai and big data the fact that some data might later in the process be found to be useful for making predictions is not enough to establish why you need to keep it for this purpose nor does it retroactively justify its collection use or retention,7,9
5114,5114,you must not collect personal data on the off chance that it might be useful in the future although you may be able to hold information for a foreseeable event that may not occur but only if you are able to justify it,7,9
5115,5115,in general when an ai system learns from data as is the case with ml models the more data it is trained on the more statistically accurate it will be,7,13
5116,5116,that is the more likely it will capture any underlying statistically useful relationships between the features in the datasets,7,13
5117,5117,as explained in the section on what do we need to do about statistical accuracy the fairness principle means that your ai system needs to be sufficiently statistically accurate for your purposes,7,6
5118,5118,for example a model for predicting future purchases based on customers purchase history would tend to be more statistically accurate the more customers are included in the training data,7,13
5119,5119,and any new features added to an existing dataset may be relevant to what the model is trying to predict,7,13
5120,5120,for example purchase histories augmented with additional demographic data might further improve the statistical accuracy of the model,7,0
5121,5121,however generally speaking the more data points collected about each person and the more people whose data is included in the data set the greater the risks to those individuals even if the data is collected for a specific purpose,7,9
5122,5122,the principle of data minimisation requires you not to use more data than is necessary for your purposes,7,11
5123,5123,so if you can achieve sufficient accuracy with fewer data points or fewer individuals being included or both you should do so,7,9
5124,5124,there are also a range of techniques for enhancing privacy which you can use to minimise the personal data being processed at the training phase including perturbation or adding noise,7,11
5125,5125,synthetic data,7,13
5126,5126,federated learning,7,13
5127,5127,some of these techniques involve modifying the training data to reduce the extent to which it can be traced back to specific individuals while retaining its use for the purposes of training well performing models,7,13
5128,5128,you can apply these types of privacy enhancing techniques to the training data after you have already collected it,7,9
5129,5129,where possible however you should apply them before collecting any personal data as a part of mitigating the risks to individuals that large datasets can pose,7,9
5130,5130,you can mathematically measure the effectiveness of these privacy enhancing techniques in balancing the privacy of individuals and the utility of a ml system using methods such as differential privacy,7,11
5131,5131,differential privacy is a way to measure whether a model created by an ml algorithm significantly depends on the data of any particular individual used to train it,7,20
5132,5132,while mathematically rigorous in theory meaningfully implementing differential privacy in practice is still challenging,7,11
5133,5133,you should monitor developments in these methods and assess whether they can provide meaningful data minimisation before attempting to implement them,7,25
5134,5134,they may not be appropriate or sufficiently mature to deploy in your particular context,7,25
5135,5135,modification could involve changing the values of data points belonging to individuals at random known as perturbing or adding noise to the data in a way that preserves some of the statistical properties of those features,7,13
5136,5136,generally speaking you can choose how much noise to inject with obvious consequences for how much you can still learn from the noisy data,7,13
5137,5137,for example smartphone predictive text systems are based on the words that users have previously typed,7,13
5138,5138,rather than always collecting a users actual keystrokes the system could be designed to create noisy ie false words at random,7,25
5139,5139,this means it makes it substantially less certain which words were noise and which words were actually typed by a specific user,7,14
5140,5140,although data would be less accurate at individual level provided the system has enough users you could still observe patterns and use these to train your ml model at an aggregate level,7,13
5141,5141,the more noise you inject the less you can learn from the data but in some cases you may be able to inject sufficient noise to render the data pseudonymous in a way which provides a meaningful level of protection,7,9
5142,5142,in some cases you may be able to develop models using synthetic data,7,13
5143,5143,this is data which does not relate to real people but has been generated artificially,7,13
5144,5144,to the extent that synthetic data cannot be related to identified or identifiable living individuals it is not personal data and therefore data protection obligations do not apply when you process it,7,9
5145,5145,however you will generally need to process some real data in order to determine realistic parameters for the synthetic data,7,13
5146,5146,where that real data can be related to identified or identifiable individuals then the processing of such data must comply with data protection laws,7,9
5147,5147,furthermore in some cases it may be possible to infer information about the real data which was used to estimate those realistic parameters by analysing the synthetic data,7,13
5148,5148,for example if the real data contains a single individual who is unusually tall rich and old and your synthetic data contains a similar individual in order to make the overall dataset statistically realistic it may be possible to infer that the individual was in the real dataset by analysing the synthetic dataset,7,13
5149,5149,avoiding such re identification may require you to change your synthetic data to the extent that it would be too unrealistic to be useful for machine learning purposes,7,13
5150,5150,a related privacy preserving technique is federated learning,7,11
5151,5151,this allows multiple different parties to train models on their own data local models,7,13
5152,5152,they then combine some of the patterns that those models have identified known as gradients into a single more accurate global model without having to share any training data with each other,7,13
5153,5153,federated learning is relatively new but has several large scale applications,7,13
5154,5154,these include auto correction and predictive text models across smartphones but also for medical research involving analysis across multiple patient databases,7,13
5155,5155,while sharing the gradient derived from a locally trained model presents a lower privacy risk than sharing the training data itself a gradient can still reveal some personal information about the individuals it was derived from especially if the model is complex with a lot of fine grained variables,7,20
5156,5156,you therefore still need to assess the risk of re identification,7,12
5157,5157,in the case of federated learning participating organisations may be considered joint controllers even though they dont have access to each others data,7,26
5158,5158,to make a prediction or classification about an individual ml models usually require the full set of predictor variables for that person to be included in the query,7,13
5159,5159,as in the training phase there are a number of techniques which you can use to minimise personal data or mitigate risks posed to that data at the inference stage including converting personal data into less human readable formats,7,9
5160,5160,making inferences locally,7,14
5161,5161,privacy preserving query approaches,7,11
5162,5162,we consider these approaches below,7,24
5163,5163,in many cases the process of converting data into a format that allows it to be classified by a model can go some way towards minimising it,7,13
5164,5164,raw personal data will usually first have to be converted into a more abstract format for the purposes of prediction,7,9
5165,5165,for example human readable words are normally translated into a series of numbers called a feature vector,7,13
5166,5166,this means that if you deploy an ai model you may not need to process the human interpretable version of the personal data contained in the query,7,9
5167,5167,for example if the conversion happens on the users device,7,25
5168,5168,however the fact that it is no longer easily human interpretable does not imply that the converted data is no longer personal,7,9
5169,5169,consider facial recognition technology frt for example,7,12
5170,5170,in order for a facial recognition model to work digital images of the faces being classified have to be converted into faceprints,7,12
5171,5171,these are mathematical representations of the geometric properties of the underlying faces eg the distance between a persons nose and upper lip,7,12
5172,5172,rather than sending facial images themselves to your servers photos could be converted to faceprints directly on the individuals device which captures them before sending them to the model for querying,7,12
5173,5173,these faceprints would be less easily identifiable to any humans than face photos,7,12
5174,5174,however faceprints are still personal indeed biometric data and therefore very much identifiable within the context of the specific facial recognition models that they are created for,7,12
5175,5175,also when used for the purposes of uniquely identifying an individual they would be special category data under data protection law,7,12
5176,5176,another way to minimise the personal data involved in prediction is to host the ml model on the device from which the query is generated and which already collects and stores the individuals personal data,7,9
5177,5177,for example an ml model could be installed on the users own device and make inferences locally rather than being hosted on a cloud server,7,20
5178,5178,for example models for predicting what news content a user might be interested in could be run locally on their smartphone,7,13
5179,5179,when the user opens the news app the days news is sent to the phone and the local model would select the most relevant stories to show to the user based on the user personal habits or profile information which are tracked and stored on the device itself and are not shared with the content provider or app store,7,20
5180,5180,the constraint is that ml models need to be sufficiently small and computationally efficient to run on the users own hardware,7,25
5181,5181,however recent advances in purpose built hardware for smartphones and embedded devices mean that this is an increasingly viable option,7,0
5182,5182,it is important to note that local processing is not necessarily out of scope of data protection law,7,9
5183,5183,even if the personal data involved in training is being processed on the users device the organisation which creates and distributes the model is still a controller in so far as it determines the means and purposes of processing,7,9
5184,5184,similarly if personal data on the users device is subsequently accessed by a third party this activity would constitute processing of that data,7,9
5185,5185,if it is not feasible to deploy the model locally other privacy enhancing techniques exist to minimise the data that is revealed in a query sent to a ml model,7,20
5186,5186,these allow one party to retrieve a prediction or classification without revealing all of this information to the party running the model in simple terms they allow you to get an answer without having to fully reveal the question,7,13
5187,5187,there are conceptual and technical similarities between data minimisation and anonymisation,7,11
5188,5188,in some cases applying privacy preserving techniques means that certain data used in ml systems is rendered pseudonymous or anonymous,7,11
5189,5189,however you should note that pseudonymisation is essentially a security and risk reduction technique and data protection law still applies to personal data that has undergone pseudonymisation,7,9
5190,5190,in contrast anonymous information means that the information in question is no longer personal data and data protection law does not apply to it,7,9
5191,5191,sometimes it may be necessary to retain training data in order to re train the model for example when new modelling approaches become available and for debugging,7,13
5192,5192,however where a model is established and unlikely to be re trained or modified the training data may no longer be needed,7,13
5193,5193,if the model is designed to use only the last  months worth of data a data retention policy should specify that data older than  months be deleted,7,22
5194,5194,this section explains the challenges to ensure individual rights in ai systems including rights relating to solely automated decision making with legal or similarly significant effect,7,21
5195,5195,it also covers the role of meaningful human oversight,7,16
5196,5196,this section is aimed at those in compliance focused roles who are responsible for responding to individual rights requests,7,27
5197,5197,the section makes reference to some technical terms and measures which may require input from a technical specialist,7,24
5198,5198,under data protection law individuals have a number of rights relating to their personal data,7,9
5199,5199,within ai these rights apply wherever personal data is used at any of the various points in the development and deployment lifecycle of an ai system,7,9
5200,5200,this therefore covers personal data contained in the training data,7,9
5201,5201,used to make a prediction during deployment and the result of the prediction itself,7,13
5202,5202,that might be contained in the model itself,7,20
5203,5203,this section describes what you may need to consider when developing and deploying ai and complying with the individual rights of information access rectification erasure and to restriction of processing data portability and objection rights referred to in articles   of the uk gdpr,7,2
5204,5204,it does not cover each right in detail but discusses general challenges to complying with these rights in an ai context and where appropriate mentions challenges to specific rights,7,1
5205,5205,rights that individuals have about solely automated decisions that affect them in legal or similarly significant ways are discussed in more detail in what is the role of human oversight as these rights raise particular challenges when using ai,7,15
5206,5206,when creating or using ml models you invariably need to obtain data to train those models,7,13
5207,5207,for example a retailer creating a model to predict consumer purchases based on past transactions needs a large dataset of customer transactions to train the model on,7,13
5208,5208,identifying the individuals that the training data is about is a potential challenge to ensuring their rights,7,9
5209,5209,typically training data only includes information relevant to predictions such as past transactions demographics or location but not contact details or unique customer identifiers,7,9
5210,5210,training data is also typically subjected to various measures to make it more amenable to ml algorithms,7,13
5211,5211,for example a detailed timeline of a customers purchases might be transformed into a summary of peaks and troughs in their transaction history,7,24
5212,5212,this process of transforming data prior to using it for training a statistical model for example transforming numbers into values between  and  is often referred to as pre processing,7,13
5213,5213,this can create confusion about terminology in data protection where processing refers to any operation or set of operations which is performed on personal data,7,11
5214,5214,so pre processing in machine learning terminology is still processing in data protection terminology and therefore data protection still applies,7,13
5215,5215,because these processes involve converting personal data from one form into another potentially less detailed form they may make training data potentially much harder to link to a particular named individual,7,9
5216,5216,however in data protection law this is not necessarily considered sufficient to take that data out of scope,7,9
5217,5217,you therefore still need to consider this data when you are responding to individuals requests to exercise their rights,7,9
5218,5218,even if the data lacks associated identifiers or contact details and has been transformed through pre processing training data may still be considered personal data,7,9
5219,5219,this is because it can be used to single out the individual it relates to on its own or in combination with other data you may process even if it cannot be associated with a customers name,7,9
5220,5220,for example the training data in a purchase prediction model might include a pattern of purchases unique to one customer,7,13
5221,5221,in this example if a customer provided a list of their recent purchases as part of their request the organisation may be able to identify the portion of the training data that relates to them,7,13
5222,5222,in these kinds of circumstances you are obliged to respond to an individuals request assuming you have taken reasonable measures to verify their identity and no other exceptions apply,7,12
5223,5223,there may be times where you are not able to identify an individual in the training data directly or indirectly,7,9
5224,5224,provided you are able to demonstrate this individual rights under articles  to  do not apply,7,1
5225,5225,however if the individual provides additional information that enables identification this is no longer the case and you need to fulfil any request they make,7,9
5226,5226,you should consult our guidance on determining what is personal data for more information about identifiability,7,9
5227,5227,we recognise that the use of personal data with ai may sometimes make it harder to fulfil individual rights to information access rectification erasure restriction of processing and notification,7,9
5228,5228,if a request is manifestly unfounded or excessive you may be able to charge a fee or refuse to act on the request,7,21
5229,5229,however you should not regard requests about such data as manifestly unfounded or excessive just because they may be harder to fulfil in the context of ai or the motivation for requesting them may be unclear in comparison to other access requests you might typically receive,7,9
5230,5230,if you outsource an ai service to another organisation this could also make the process of responding to rights requests more complicated when the personal data involved is processed by them rather than you,7,9
5231,5231,when procuring an ai service you must choose one which allows individual rights to be protected and enabled in order to meet your obligations as a controller,7,16
5232,5232,if your chosen service is not designed to easily comply with these rights this does not remove or change those obligations,7,1
5233,5233,if you are operating as a controller your contract with the processor must stipulate that the processor assist you in responding to rights requests,7,20
5234,5234,if you are operating an ai service as a joint controller you need to decide with your fellow controllers who will carry out which obligations,7,20
5235,5235,see the section how should we understand controllerprocessor relationships in ai for more details,7,4
5236,5236,in addition to these considerations about training data and individual rights in general below we outline some considerations about how particular individual rights rectification erasure portability and information may relate to training data,7,9
5237,5237,the right to rectification may apply to the use of personal data to train an ai system,7,9
5238,5238,the steps you should take for rectification depend on the data you process as well as the nature scope context and purpose of that processing,7,24
5239,5239,the more important it is that the personal data is accurate the greater the effort you should put into checking its accuracy and if necessary taking steps to rectify it,7,9
5240,5240,in the case of training data for an ai system one purpose of the processing may be to find general patterns in large datasets,7,13
5241,5241,in this context individual inaccuracies in training data may be less important as they are not likely to affect the performance of the model since they are just one data point among many when compared to personal data that you might use to take action about an individual,7,9
5242,5242,for example you may think it more important to rectify an incorrectly recorded customer delivery address than to rectify the same incorrect address in training data,7,13
5243,5243,your rationale is likely to be that the former could result in a failed delivery but the latter would barely affect the overall statistical accuracy of the model,7,18
5244,5244,however in practice the right of rectification does not allow you to disregard any requests because you think they are less important for your purposes,7,22
5245,5245,you may also receive requests for the erasure of personal data contained within training data,7,9
5246,5246,you should note that whilst the right to erasure is not absolute you still need to consider any erasure request you receive unless you are processing the data on the basis of a legal obligation or public task both of which are unlikely to be lawful bases for training ai systems,7,9
5247,5247,the erasure of one individuals personal data from the training data is unlikely to affect your ability to fulfil the purposes of training an ai system as you are likely to still have sufficient data from other individuals,7,9
5248,5248,you are therefore unlikely to have a justification for not fulfilling the request to erase their personal data from your training dataset,7,9
5249,5249,complying with a request to erase training data does not entail erasing all ml models based on this data unless the models themselves contain that data or can be used to infer it,7,13
5250,5250,individuals have the right to data portability for data they have provided to a controller where the lawful basis of processing is consent or contract,7,9
5251,5251,provided data includes data the individual has consciously input into a form but also behavioural or observational data gathered in the process of using a service,7,9
5252,5252,in most cases data used for training a model eg demographic information or spending habits counts as data provided by the individual,7,9
5253,5253,the right to data portability therefore applies in cases where this processing is based on consent or contract,7,9
5254,5254,however as discussed above pre processing methods are usually applied which significantly change the data from its original form into something that can be more effectively analysed by machine learning algorithms,7,13
5255,5255,where this transformation is significant the resulting data may no longer count as provided,7,22
5256,5256,in this case the data is not subject to data portability although it does still constitute personal data and as such other data protection rights still apply eg the right of access,7,9
5257,5257,however the original form of the data from which the pre processed data was derived is still subject to the right to data portability if provided by the individual under consent or contract and processed by automated means,7,9
5258,5258,you must inform individuals if their personal data is going to be used to train an ai system to ensure that processing is fair and transparent,7,9
5259,5259,you should provide this information at the point of collection,7,24
5260,5260,if the data was initially processed for a different purpose and you later decide to use it for the separate purpose of training an ai system you need to inform the individuals concerned as well as ensuring the new purpose is compatible with the previous one,7,9
5261,5261,in some cases you may not have obtained the training data from the individual and therefore not have had the opportunity to inform them at the time you did so,7,9
5262,5262,in such cases you should provide the individual with the information specified in article  within a reasonable period one month at the latest unless a relevant exemption from article  applies,7,22
5263,5263,since using an individuals data for the purposes of training an ai system does not normally constitute making a solely automated decision with legal or similarly significant effects you only need to provide information about these decisions when you are taking them,7,21
5264,5264,however you still need to comply with the main transparency requirements,7,25
5265,5265,for the reasons stated above it may be difficult to identify and communicate with the individuals whose personal data is contained in the training data,7,9
5266,5266,for example training data may have been stripped of any personal identifiers and contact addresses while still remaining personal data,7,9
5267,5267,in such cases it may be impossible or involve a disproportionate effort to provide information directly to the individual,7,9
5268,5268,therefore instead you should take appropriate measures to protect the individuals rights and freedoms and legitimate interests,7,16
5269,5269,for example you could provide public information explaining where you obtained the data from that you use to train your ai system and how to object,7,24
5270,5270,typically once deployed the outputs of an ai system are stored in a profile of an individual and used to take some action about them,7,20
5271,5271,for example the product offers a customer sees on a website might be driven by the output of the predictive model stored in their profile,7,20
5272,5272,where this data constitutes personal data it will generally be subject to all of the rights mentioned above unless exemptions or other limitations to those rights apply,7,9
5273,5273,whereas individual inaccuracies in training data may have a negligible effect an inaccurate output of a model could directly affect the individual,7,6
5274,5274,requests for rectification of model outputs or the personal data inputs on which they are based are therefore more likely to be made than requests for rectification of training data,7,9
5275,5275,however as said above predictions are not inaccurate if they are intended as prediction scores as opposed to statements of fact,7,18
5276,5276,if the personal data is not inaccurate then the right to rectification does not apply,7,9
5277,5277,personal data resulting from further analysis of provided data is not subject to the right to portability,7,9
5278,5278,this means that the outputs of ai models such as predictions and classifications about individuals are out of scope of the right to portability,7,14
5279,5279,in some cases some or all of the features used to train the model may themselves be the result of some previous analysis of personal data,7,9
5280,5280,for example a credit score which is itself the result of statistical analysis based on an individuals financial data might then be used as a feature in an ml model,7,13
5281,5281,in these cases the credit score is not included within scope of the right to data portability even if other features are,7,9
5282,5282,in addition to being used in the inputs and outputs of a model in some cases personal data might also be contained in a model itself,7,9
5283,5283,as explained in  what types of privacy attacks apply to ai models this could happen for two reasons by design or by accident,7,25
5284,5284,when personal data is included in models by design it is because certain types of models such as support vector machines svms contain some key examples from the training data in order to help distinguish between new examples during deployment,7,9
5285,5285,in these cases a small set of individual examples are contained somewhere in the internal logic of the model,7,20
5286,5286,the training set typically contains hundreds of thousands of examples and only a very small percentage of them end up being used directly in the model,7,13
5287,5287,therefore the chances that one of the relevant individuals makes a request are very small but remains possible,7,9
5288,5288,depending on the particular programming library in which the ml model is implemented there may be a built in function to easily retrieve these examples,7,13
5289,5289,in these cases it is likely to be practically possible for you to respond to an individuals request,7,21
5290,5290,to enable this where you are using models which contain personal data by design you should implement them in a way that allows the easy retrieval of these examples,7,9
5291,5291,if the request is for access to the data you could fulfil this without altering the model,7,20
5292,5292,if the request is for rectification or erasure of the data this may not be possible without re training the model either with the rectified data or without the erased data or deleting the model altogether,7,13
5293,5293,while it is not a legal requirement having a well organised model management system and deployment pipeline will make it easier and cheaper to accommodate these requests and re training and redeploying your ai models accordingly will be less costly,7,20
5294,5294,aside from svms and other models that contain examples from the training data by design some models might leak personal data by accident,7,13
5295,5295,in these cases unauthorised parties may be able to recover elements of the training data or infer who was in it by analysing the way the model behaves,7,20
5296,5296,the rights of access rectification and erasure may be difficult or impossible to exercise and fulfil in these scenarios,7,22
5297,5297,unless the individual presents evidence that their personal data could be inferred from the model you may not be able to determine whether personal data can be inferred and therefore whether the request has any basis,7,9
5298,5298,you should regularly and proactively evaluate the possibility of personal data being inferred from models in light of the state of the art technology so that you minimise the risk of accidental disclosure,7,9
5299,5299,there are specific provisions in data protection law covering individuals rights where processing involves solely automated individual decision making including profiling with legal or similarly significant effects,7,9
5300,5300,these provisions cover both information you have to provide proactively about the processing and individuals rights in relation to a decision made about them,7,21
5301,5301,under articles   and   you must tell people whose data you are processing that you are doing so for automated decision making and give them meaningful information about the logic involved as well as the significance and the envisaged consequences of the processing for them,7,21
5302,5302,under article   you must also tell them about this if they submit a subject access request,7,2
5303,5303,in addition data protection requires you to implement suitable safeguards when processing personal data to make solely automated decisions that have a legal or similarly significant impact on individuals,7,11
5304,5304,these safeguards include the right for individuals to obtain human intervention,7,12
5305,5305,express their point of view,7,14
5306,5306,contest the decision made about them,7,21
5307,5307,obtain an explanation about the logic of the decision,7,14
5308,5308,for processing involving solely automated decision making that falls under part  of the dpa  the applicable safeguards will depend on regulations provided in the particular law authorising the automated decision making,7,21
5309,5309,although the individual has the right to request that you reconsider the decision or take a new decision that is not based solely on automated processing,7,21
5310,5310,these safeguards cannot be token gestures,7,25
5311,5311,human intervention should involve a review of the decision which must be carried out by someone with the appropriate authority and capability to change that decision,7,21
5312,5312,that persons review should also include an assessment of all relevant data including any information an individual may provide,7,9
5313,5313,the conditions under which human intervention qualifies as meaningful are similar to which render a decision non solely automated see what is the difference between solely automated and partly automated decision making below,7,21
5314,5314,however a key difference is that in solely automated contexts human intervention is only required on a case by case basis to safeguard the individuals rights whereas for a system to qualify as not solely automated meaningful human intervention is required in every decision,7,15
5315,5315,note that if you are using automated decision making as well as implementing suitable safeguards you must also have a suitable lawful basis,7,21
5316,5316,the type and complexity of the systems involved in making solely automated decisions affect the nature and severity of the risk to peoples data protection rights and raise different considerations as well as compliance and risk management challenges,7,15
5317,5317,basic systems which automate a relatively small number of explicitly written rules are unlikely to be considered ai eg a set of clearly expressed if then rules to determine a customers eligibility for a product,7,21
5318,5318,however the resulting decisions could still constitute automated decision making within the meaning of data protection law,7,21
5319,5319,it should also be relatively easy for a human reviewer to identify and rectify any mistake if a decision is challenged by an individual because of a systems high interpretability,7,14
5320,5320,however other systems such as those based on ml may be more complex and present more challenges for meaningful human review,7,14
5321,5321,ml systems make predictions or classifications about people based on data patterns,7,13
5322,5322,even when they are highly statistically accurate they will occasionally reach the wrong decision in an individual case,7,18
5323,5323,errors may not be easy for a human reviewer to identify understand or fix,7,14
5324,5324,while not every challenge from an individual will result in the decision being overturned you should expect that many could be,7,6
5325,5325,there are two particular reasons why this may be the case in ml systems the individual is an outlier ie their circumstances are substantially different from those considered in the training data used to build the ai system,7,6
5326,5326,because the ml model has not been trained on enough data about similar individuals it can make incorrect predictions or classifications,7,13
5327,5327,assumptions in the ai design can be challenged eg a continuous variable such as age might have been broken up binned into discrete age ranges like   as part of the modelling process,7,4
5328,5328,finer grained bins may result in a different model with substantially different predictions for people of different ages,7,6
5329,5329,the validity of this data pre processing and other design choices may only come into question as a result of an individuals challenge,7,9
5330,5330,you should consider the system requirements necessary to support a meaningful human review from the design phase,7,15
5331,5331,particularly the interpretability requirements and effective user interface design to support human reviews and interventions,7,14
5332,5332,design and deliver appropriate training and support for human reviewers,7,27
5333,5333,give staff the appropriate authority incentives and support to address or escalate individuals concerns and if necessary override the ai systems decision,7,16
5334,5334,the icos and the alan turing institutes explaining decisions made with ai guidance looks at how and to what extent complex ai systems might affect your ability to provide meaningful explanations to individuals,7,14
5335,5335,however complex ai systems can also impact the effectiveness of other mandatory safeguards,7,4
5336,5336,if a system is too complex to explain it may also be too complex to meaningfully contest intervene on review or put an alternative point of view against,7,14
5337,5337,for example if an ai system uses hundreds of features and a complex non linear model to make a prediction then it may be difficult for an individual to determine which variables or correlations to object to,7,4
5338,5338,therefore safeguards around solely automated ai systems are mutually supportive and should be designed holistically and with the individual in mind,7,16
5339,5339,the information about the logic of a system and explanations of decisions should give individuals the necessary context to decide whether and on what grounds they would like to request human intervention,7,21
5340,5340,in some cases insufficient explanations may prompt individuals to resort to other rights unnecessarily,7,14
5341,5341,requests for intervention expression of views or contests are more likely to happen if individuals dont feel they have a sufficient understanding of how the decision was reached,7,21
5342,5342,the process for individuals to exercise their rights should be simple and user friendly,7,12
5343,5343,for example if you communicate the result of the solely automated decision through a website the page should contain a link or clear information allowing the individual to contact a member of staff who can intervene without any undue delays or complications,7,21
5344,5344,you are also required to keep a record of all decisions made by an ai system as part of your accountability and documentation obligations,7,9
5345,5345,this should also include whether an individual requested human intervention expressed any views contested the decision and whether you changed the decision as a result,7,21
5346,5346,you should monitor and analyse this data,7,24
5347,5347,if decisions are regularly changed in response to individuals exercising their rights you should then consider how you will amend your systems accordingly,7,15
5348,5348,where your system is based on ml this might involve including the corrected decisions into fresh training data so that similar mistakes are less likely to happen in future,7,13
5349,5349,more substantially you may identify a need to collect more or better training data to fill in the gaps that led to the erroneous decision or modify the model building process ie by changing the feature selection,7,20
5350,5350,in addition to being a compliance requirement this is also an opportunity for you to improve the performance of your ai systems and in turn build individuals trust in them,7,27
5351,5351,however if grave or frequent mistakes are identified you need to take immediate steps to understand and rectify the underlying issues and if necessary suspend the use of the automated system,7,15
5352,5352,there are also trade offs that having a human in the loop may entail,7,12
5353,5353,either in terms of a further erosion of privacy if human reviewers need to consider additional personal data in order to validate or reject an ai generated output or the possible reintroduction of human biases at the end of an automated process,7,9
5354,5354,when ai is used to inform legal or similarly significant decisions about individuals there is a risk that these decisions are made without appropriate human oversight,7,21
5355,5355,for example whether they have access to financial products or job opportunities,7,0
5356,5356,this infringes article  of the uk gdpr,7,2
5357,5357,to mitigate this risk you should ensure that people assigned to provide human oversight remain engaged critical and able to challenge the systems outputs wherever appropriate,7,15
5358,5358,you can use ai systems in two ways for automated decision making adm where the system makes a decision automatically,7,21
5359,5359,as decision support where the system only supports a human decision maker in their deliberation,7,15
5360,5360,for example you could use ai in a system which automatically approves or rejects a financial loan or merely to provide additional information to support a loan officer when deciding whether to grant a loan application,7,0
5361,5361,whether solely automated decision making is generally more or less risky than partly automated decision making depends on the specific circumstances,7,21
5362,5362,you therefore need to evaluate this based on your own context,7,21
5363,5363,regardless of their relative merits automated decisions are treated differently to human decisions in data protection law,7,21
5364,5364,specifically article  of the uk gdpr restricts fully automated decisions which have legal or similarly significant effects on individuals to a more limited set of lawful bases and requires certain safeguards to be in place,7,21
5365,5365,by contrast the use of decision support tools are not subject to these conditions,7,26
5366,5366,however the human input needs to be meaningful,7,14
5367,5367,you should be aware that a decision does not fall outside the scope of article  just because a human has rubber stamped it,7,21
5368,5368,the degree and quality of human review and intervention before a final decision is made about an individual are key factors in determining whether an ai system is being used for automated decision making or merely as decision support,7,21
5369,5369,ensuring human input is meaningful in these situations is not just the responsibility of the human using the system,7,15
5370,5370,senior leaders data scientists business owners and those with oversight functions if you have them among others are expected to play an active role in ensuring that ai applications are designed built and used as intended,7,16
5371,5371,if you are deploying ai systems which are designed as decision support tools and therefore are intended to be outside the scope of article  you should be aware of existing guidance on these issues from both the ico and the edpb,7,2
5372,5372,the key considerations are human reviewers must be involved in checking the systems recommendation and should not just apply the automated recommendation to an individual in a routine fashion,7,15
5373,5373,reviewers involvement must be active and not just a token gesture,7,0
5374,5374,they should have actual meaningful influence on the decision including the authority and competence to go against the recommendation,7,21
5375,5375,reviewers must weigh up and interpret the recommendation consider all available input data and also take into account other additional factors,7,21
5376,5376,you need to consider the meaningfulness of human input in any automated decision making system you use however basic it may be,7,21
5377,5377,however in more complex ai systems there are two additional factors that could potentially cause a system intended as decision support to inadvertently fail to ensure meaningful human input and therefore fall into the scope of article,7,4
5378,5378,they are automation bias,7,15
5379,5379,lack of interpretability,7,14
5380,5380,ai models are based on mathematics and data,7,4
5381,5381,because of this people tend to think of them as objective and trust their output regardless of how statistically accurate it is,7,18
5382,5382,the terms automation bias or automation induced complacency describe how human users routinely rely on the output generated by a decision support system and stop using their own judgement or stop questioning whether the output might be wrong,7,15
5383,5383,some types of ai systems may have outputs which are difficult for a human reviewer to interpret for example those which rely on complex high dimensional deep learning models,7,14
5384,5384,if the outputs of ai systems are not easily interpretable and other explanation tools are not available or reliable there is a risk that a human is not able to meaningfully assess the output of an ai system and factor it into their own decision making,7,14
5385,5385,if meaningful reviews are not possible the reviewer may start to just agree with the systems recommendations without judgement or challenge,7,21
5386,5386,this means the resulting decisions are effectively solely automated,7,21
5387,5387,you should take a clear view on the intended use of any ai system from the beginning,7,4
5388,5388,you should specify and document clearly whether you are using ai to support or enhance human decision making or to make solely automated decisions,7,21
5389,5389,your senior management should review and sign off the intended use of any ai system making sure that it is in line with your organisations risk appetite,7,16
5390,5390,this means senior management needs to have a solid understanding of the key risk implications associated with each option and be ready and equipped to provide an appropriate degree of challenge,7,3
5391,5391,you must also ensure clear lines of accountability and effective risk management policies are in place from the outset,7,16
5392,5392,if ai systems are only intended to support human decisions then your policies should specifically address additional risk factors such as automation bias and lack of interpretability,7,21
5393,5393,it is possible that you may not know in advance whether a solely or partly automated ai application will meet your needs best,7,21
5394,5394,believe that a solely automated ai system will more fully achieve the intended outcome of your processing but that it may carry more risks to individuals than a partly automated system,7,21
5395,5395,in these cases your risk management policies and dpias should clearly reflect this and include the risk and controls for each option throughout the ai systems lifecycle,7,3
5396,5396,you may think you can address automation bias chiefly by improving the effectiveness of the training and monitoring of human reviewers,7,15
5397,5397,while training is a key component of effective ai risk management you should have controls to mitigate automation bias in place from the start of the project including the scoping and design phases as well as development and deployment,7,3
5398,5398,during the design and build phase all relevant parts of your organisation should work together to develop design requirements that support a meaningful human review from the outset eg business owners data scientists and those with oversight functions if you have them,7,27
5399,5399,you must think about what features you expect the ai system to consider and which additional factors the human reviewers should take into account before finalising their decision,7,21
5400,5400,for example the ai system could consider quantitatively measurable properties like how many years of experience a job applicant has while a human reviewer qualitatively assesses other aspects of an application eg written communication,7,4
5401,5401,if human reviewers can only access or use the same data used by the ai system then arguably they are not taking into account other additional factors,7,9
5402,5402,this means that their review may not be sufficiently meaningful and the decision may end up being considered as solely automated,7,21
5403,5403,where necessary you should consider how to capture additional factors for consideration by the human reviewers,7,21
5404,5404,for example they might interact directly with the person the decision is about to gather such information,7,9
5405,5405,those in charge of designing the front end interface of an ai system must understand the needs thought processes and behaviours of human reviewers and allow them to effectively intervene,7,14
5406,5406,it may therefore be helpful to consult and test options with human reviewers early on,7,21
5407,5407,however the features of the ai systems you use also depend on the data available the type of models selected and other system building choices,7,4
5408,5408,you need to test and confirm any assumptions made in the design phase once the ai system has been trained and built,7,20
5409,5409,you should also consider interpretability from the design phase,7,14
5410,5410,however interpretability is challenging to define in absolute terms and can be measured in different ways,7,14
5411,5411,for example can the human reviewer predict how the systems outputs will change if given different inputs,7,15
5412,5412,identify the most important inputs contributing to a particular output,7,24
5413,5413,identify when the output might be wrong,7,18
5414,5414,this is why it is important that you define and document what interpretability means and how to measure it in the specific context of each ai system you wish to use and the personal data that system will process,7,14
5415,5415,some ai systems are more interpretable than others,7,14
5416,5416,for example models that use a small number of human interpretable features eg age and weight are likely to be easier to interpret than models that use a large number of features,7,14
5417,5417,the relationship between the input features and the models output can also be either simple or complicated,7,20
5418,5418,simple rules which set conditions under which certain inferences can be made as is the case with decision trees are easier to interpret,7,21
5419,5419,similarly linear relationships where the value of the output increases proportional to the input may be easier to interpret than relationships that are non linear where the output value is not proportional to the input or non monotonic where the output value may increase or decrease as the input increases,7,26
5420,5420,one approach to address low interpretability is the use of local explanations using methods like local interpretable model agnostic explanation lime which provides an explanation of a specific output rather than the model in general,7,14
5421,5421,limes use a simpler surrogate model to summarise the relationships between input and output pairs that are similar to those in the system you are trying to interpret,7,20
5422,5422,in addition to summaries of individual predictions limes can sometimes help detect errors eg to see what specific part of an image has led a model to classify it incorrectly,7,13
5423,5423,however they do not represent the logic underlying the ai system and its outputs and can be misleading if misused especially with certain kinds of models eg high dimensional models,7,14
5424,5424,you should therefore assess whether in your context lime and similar approaches will help the human decision maker to meaningfully interpret the ai system and its output,7,21
5425,5425,many statistical models can also be designed to provide a confidence score alongside each output which could help a human reviewer in their own decision making,7,21
5426,5426,a lower confidence score indicates that the human reviewer needs to have more input into the final decision,7,18
5427,5427,assessing the interpretability requirements should be part of the design phase allowing you to develop explanation tools as part of the system if required,7,14
5428,5428,this is why your risk management policies should establish a robust risk based and independent approval process for each processing operation that uses ai,7,3
5429,5429,they should also set out clearly who is responsible for the testing and final validation of the system before it is deployed,7,15
5430,5430,those individuals should be accountable for any negative impact on interpretability and the effectiveness of human reviews and only provide sign off if ai systems are in line with the adopted risk management policy,7,0
5431,5431,training your staff is pivotal to ensuring an ai system is considered partly automated,7,15
5432,5432,as a starting point you should train or retrain your human reviewers to understand how an ai system works and its limitations,7,14
5433,5433,anticipate when the system may be misleading or wrong and why,7,14
5434,5434,have a healthy level of scepticism in the ai systems output and given a sense of how often the system could be wrong,7,16
5435,5435,understand how their own expertise is meant to complement the system and provide them with a list of factors to take into account,7,14
5436,5436,provide meaningful explanations for either rejecting or accepting the ai systems output  a decision they should be responsible for,7,14
5437,5437,you should also have a clear escalation policy in place,7,22
5438,5438,in order for the training to be effective it is important that human reviewers have the authority to override the output generated by the ai system and they are confident that they will not be penalised for so doing,7,13
5439,5439,this authority and confidence cannot be created by policies and training alone,7,16
5440,5440,a supportive organisational culture is also crucial,7,16
5441,5441,any training programme is kept up to date in line with technological developments and changes in processes with human reviewers being offered refresher training at intervals where appropriate,7,13
5442,5442,we have focused here on the training of human reviewers however it is worth noting that you should also consider whether any other function requires additional training to provide effective oversight eg risk or internal audit,7,0
5443,5443,the analysis of why and how many times a human reviewer accepted or rejected the ai systems output is a key part in an effective risk monitoring system,7,0
5444,5444,if risk monitoring reports flag that your human reviewers are routinely agreeing with the ai systems outputs and cannot demonstrate they have genuinely assessed them then their decisions may effectively be classed as solely automated under uk gdpr,7,0
5445,5445,you need to have controls in place to keep risk within target levels,7,3
5446,5446,when outcomes go beyond target levels you should have processes to swiftly assess compliance and take action if necessary,7,17
5447,5447,this might include temporarily increasing human scrutiny or ensuring that you have an appropriate lawful basis and safeguards in case the decision making does effectively become fully automated,7,21
5448,5448,artificial intelligence ai technologies have significant potential to transform society and peoples lives  from commerce and health to transportation and cybersecurity to the environment and our planet,8,4
5449,5449,ai technologies can drive inclusive economic growth and support scientific advancements that improve the conditions of our world,8,0
5450,5450,ai technologies however also pose risks that can negatively impact individuals groups organizations communities society the environment and the planet,8,3
5451,5451,like risks for other types of technology ai risks can emerge in a variety of ways and can be characterized as longor short term highor low probability systemic or localized and highor low impact,8,3
5452,5452,the ai rmf refers to an ai system as an engineered or machine based system that can for a given set of objectives generate outputs such as predictions recommendations or decisions influencing real or virtual environments,8,4
5453,5453,ai systems are designed to operate with varying levels of autonomy,8,4
5454,5454,while there are myriad standards and best practices to help organizations mitigate the risks of traditional software or information based systems the risks posed by ai systems are in many ways unique see appendix b,8,3
5455,5455,ai systems for example may be trained on data that can change over time sometimes significantly and unexpectedly affecting system functionality and trustworthiness in ways that are hard to understand,8,15
5456,5456,ai systems and the contexts in which they are deployed are frequently complex making it difficult to detect and respond to failures when they occur,8,4
5457,5457,ai systems are inherently socio technical in nature meaning they are influenced by societal dynamics and human behavior,8,4
5458,5458,ai risks  and benefits  can emerge from the interplay of technical aspects combined with societal factors related to how a system is used its interactions with other ai systems who operates it and the social context in which it is deployed,8,3
5459,5459,these risks make ai a uniquely challenging technology to deploy and utilize both for organizations and within society,8,3
5460,5460,without proper controls ai systems can amplify perpetuate or exacerbate inequitable or undesirable outcomes for individuals and communities,8,16
5461,5461,with proper controls ai systems can mitigate and manage inequitable outcomes,8,6
5462,5462,ai risk management is a key component of responsible development and use of ai systems,8,3
5463,5463,responsible ai practices can help align the decisions about ai system design development and uses with intended aim and values,8,16
5464,5464,core concepts in responsible ai emphasize human centricity social responsibility and sustainability,8,16
5465,5465,ai risk management can drive responsible uses and practices by prompting organizations and their internal teams who design develop and deploy ai to think more critically about context and potential or unexpected negative and positive impacts,8,3
5466,5466,understanding and managing the risks of ai systems will help to enhance trustworthiness and in turn cultivate public trust,8,3
5467,5467,social responsibility can refer to the organizations responsibility for the impacts of its decisions and activities on society and the environment through transparent and ethical behavior,8,16
5468,5468,sustainability refers to the state of the global system including environmental social and economic aspects in which the needs of the present are met without compromising the ability of future generations to meet their own needs,8,16
5469,5469,responsible ai is meant to result in technology that is also equitable and accountable,8,16
5470,5470,the expectation is that organizational practices are carried out in accord with professional responsibility defined by iso as an approach that aims to ensure that professionals who design develop or deploy ai systems and applications or ai based products or systems recognize their unique position to exert influence on people society and the future of ai,8,16
5471,5471,the goal of the ai rmf is to offer a resource to the organizations designing developing deploying or using ai systems to help manage the many risks of ai and promote trustworthy and responsible development and use of ai systems,8,4
5472,5472,the framework is intended to be voluntary rights preserving non sector specific and use case agnostic providing flexibility to organizations of all sizes and in all sectors and throughout society to implement the approaches in the framework,8,26
5473,5473,the framework is designed to equip organizations and individuals  referred to here as ai actors  with approaches that increase the trustworthiness of ai systems and to help foster the responsible design development deployment and use of ai systems over time,8,16
5474,5474,ai actors are defined by the organisation for economic co operation and development oecd as those who play an active role in the ai system lifecycle including organizations and individuals that deploy or operate ai,8,4
5475,5475,the ai rmf is intended to be practical to adapt to the ai landscape as ai technologies continue to develop and to be operationalized by organizations in varying degrees and capacities so society can benefit from ai while also being protected from its potential harms,8,4
5476,5476,the framework and supporting resources will be updated expanded and improved based on evolving technology the standards landscape around the world and ai community experience and feedback,8,27
5477,5477,nist will continue to align the ai rmf and related guidance with applicable international standards guidelines and practices,8,27
5478,5478,as the ai rmf is put into use additional lessons will be learned to inform future updates and additional resources,8,2
5479,5479,the framework is divided into two parts,8,26
5480,5480,part  discusses how organizations can frame the risks related to ai and describes the intended audience,8,3
5481,5481,next ai risks and trustworthiness are analyzed outlining the characteristics of trustworthy ai systems which include valid and reliable safe secure and resilient accountable and transparent explainable and interpretable privacy enhanced and fair with their harmful biases managed,8,18
5482,5482,it describes four specific functions to help organizations address the risks of ai systems in practice,8,3
5483,5483,these functions  govern map measure and manage  are broken down further into categories and subcategories,8,26
5484,5484,while govern applies to all stages of organizations ai risk management processes and procedures the map measure and manage functions can be applied in ai system specific contexts and at specific stages of the ai lifecycle,8,3
5485,5485,ai risk management offers a path to minimize potential negative impacts of ai systems such as threats to civil liberties and rights while also providing opportunities to maximize positive impacts,8,3
5486,5486,addressing documenting and managing ai risks and potential negative impacts effectively can lead to more trustworthy ai systems,8,3
5487,5487,in the context of the ai rmf risk refers to the composite measure of an events probability of occurring and the magnitude or degree of the consequences of the corresponding event,8,3
5488,5488,the impacts or consequences of ai systems can be positive negative or both and can result in opportunities or threats,8,4
5489,5489,when considering the negative impact of a potential event risk is a function of  the negative impact or magnitude of harm that would arise if the circumstance or event occurs and  the likelihood of occurrence,8,3
5490,5490,negative impact or harm can be experienced by individuals groups communities organizations society the environment and the planet,8,17
5491,5491,risk management refers to coordinated activities to direct and control an organization with regard to risk,8,3
5492,5492,while risk management processes generally address negative impacts this framework offers approaches to minimize anticipated negative impacts of ai systems and identify opportunities to maximize positive impacts,8,3
5493,5493,effectively managing the risk of potential harms could lead to more trustworthy ai systems and unleash potential benefits to people individuals communities and society organizations and ecosystems,8,3
5494,5494,risk management can enable ai developers and users to understand impacts and account for the inherent limitations and uncertainties in their models and systems which in turn can improve overall system performance and trustworthiness and the likelihood that ai technologies will be used in ways that are beneficial,8,3
5495,5495,the ai rmf is designed to address new risks as they emerge,8,3
5496,5496,this flexibility is particularly important where impacts are not easily foreseeable and applications are evolving,8,0
5497,5497,while some ai risks and benefits are well known it can be challenging to assess negative impacts and the degree of harms,8,3
5498,5498,ai risk management efforts should consider that humans may assume that ai systems work and work well in all settings,8,3
5499,5499,for example whether correct or not ai systems are often perceived as being more objective than humans or as offering greater capabilities than general software,8,4
5500,5500,ai risks or failures that are not well defined or adequately understood are difficult to measure quantitatively or qualitatively,8,3
5501,5501,the inability to appropriately measure ai risks does not imply that an ai system necessarily poses either a high or low risk,8,3
5502,5502,third party data or systems can accelerate research and development and facilitate technology transition,8,0
5503,5503,they also may complicate risk measurement,8,3
5504,5504,risk can emerge both from third party data software or hardware itself and how it is used,8,3
5505,5505,risk metrics or methodologies used by the organization developing the ai system may not align with the risk metrics or methodologies uses by the organization deploying or operating the system,8,3
5506,5506,also the organization developing the ai system may not be transparent about the risk metrics or methodologies it used,8,3
5507,5507,risk measurement and management can be complicated by how customers use or integrate thirdparty data or systems into ai products or services particularly without sufficient internal governance structures and technical safeguards,8,3
5508,5508,regardless all parties and ai actors should manage risk in the ai systems they develop deploy or use as standalone or integrated components,8,3
5509,5509,organizations risk management efforts will be enhanced by identifying and tracking emergent risks and considering techniques for measuring them,8,3
5510,5510,ai system impact assessment approaches can help ai actors understand potential impacts or harms within specific contexts,8,17
5511,5511,the current lack of consensus on robust and verifiable measurement methods for risk and trustworthiness and applicability to different ai use cases is an ai risk measurement challenge,8,18
5512,5512,potential pitfalls when seeking to measure negative risk or harms include the reality that development of metrics is often an institutional endeavor and may inadvertently reflect factors unrelated to the underlying impact,8,3
5513,5513,in addition measurement approaches can be oversimplified gamed lack critical nuance become relied upon in unexpected ways or fail to account for differences in affected groups and contexts,8,18
5514,5514,approaches for measuring impacts on a population work best if they recognize that contexts matter that harms may affect varied groups or sub groups differently and that communities or other sub groups who may be harmed are not always direct users of a system,8,17
5515,5515,measuring risk at an earlier stage in the ai lifecycle may yield different results than measuring risk at a later stage some risks may be latent at a given point in time and may increase as ai systems adapt and evolve,8,3
5516,5516,furthermore different ai actors across the ai lifecycle can have different risk perspectives,8,3
5517,5517,for example an ai developer who makes ai software available such as pre trained models can have a different risk perspective than an ai actor who is responsible for deploying that pre trained model in a specific use case,8,3
5518,5518,such deployers may not recognize that their particular uses could entail risks which differ from those perceived by the initial developer,8,3
5519,5519,all involved ai actors share responsibilities for designing developing and deploying a trustworthy ai system that is fit for purpose,8,16
5520,5520,while measuring ai risks in a laboratory or a controlled environment may yield important insights pre deployment these measurements may differ from risks that emerge in operational real world settings,8,3
5521,5521,inscrutable ai systems can complicate risk measurement,8,3
5522,5522,inscrutability can be a result of the opaque nature of ai systems limited explainability or interpretability lack of transparency or documentation in ai system development or deployment or inherent uncertainties in ai systems,8,14
5523,5523,risk management of ai systems that are intended to augment or replace human activity for example decision making requires some form of baseline metrics for comparison,8,3
5524,5524,this is difficult to systematize since ai systems carry out different tasks and perform tasks differently than humans,8,4
5525,5525,while the ai rmf can be used to prioritize risk it does not prescribe risk tolerance,8,3
5526,5526,risk tolerance refers to the organizations or ai actors readiness to bear the risk in order to achieve its objectives,8,3
5527,5527,risk tolerance can be influenced by legal or regulatory requirements,8,3
5528,5528,risk tolerance and the level of risk that is acceptable to organizations or society are highly contextual and application and use case specific,8,3
5529,5529,risk tolerances can be influenced by policies and norms established by ai system owners organizations industries communities or policy makers,8,3
5530,5530,risk tolerances are likely to change over time as ai systems policies and norms evolve,8,3
5531,5531,different organizations may have varied risk tolerances due to their particular organizational priorities and resource considerations,8,3
5532,5532,emerging knowledge and methods to better inform harm cost benefit tradeoffs will continue to be developed and debated by businesses governments academia and civil society,8,0
5533,5533,to the extent that challenges for specifying ai risk tolerances remain unresolved there may be contexts where a risk management framework is not yet readily applicable for mitigating negative ai risks,8,3
5534,5534,the framework is intended to be flexible and to augment existing risk practices which should align with applicable laws regulations and norms,8,26
5535,5535,organizations should follow existing regulations and guidelines for risk criteria tolerance and response established by organizational domain discipline sector or professional requirements,8,3
5536,5536,some sectors or industries may have established definitions of harm or established documentation reporting and disclosure requirements,8,1
5537,5537,within sectors risk management may depend on existing guidelines for specific applications and use case settings,8,3
5538,5538,where established guidelines do not exist organizations should define reasonable risk tolerance,8,3
5539,5539,once tolerance is defined this ai rmf can be used to manage risks and to document risk management processes,8,3
5540,5540,attempting to eliminate negative risk entirely can be counterproductive in practice because not all incidents and failures can be eliminated,8,3
5541,5541,unrealistic expectations about risk may lead organizations to allocate resources in a manner that makes risk triage inefficient or impractical or wastes scarce resources,8,3
5542,5542,a risk management culture can help organizations recognize that not all ai risks are the same and resources can be allocated purposefully,8,3
5543,5543,actionable risk management efforts lay out clear guidelines for assessing trustworthiness of each ai system an organization develops or deploys,8,3
5544,5544,policies and resources should be prioritized based on the assessed risk level and potential impact of an ai system,8,3
5545,5545,the extent to which an ai system may be customized or tailored to the specific context of use by the ai deployer can be a contributing factor,8,4
5546,5546,when applying the ai rmf risks which the organization determines to be highest for the ai systems within a given context of use call for the most urgent prioritization and most thorough risk management process,8,3
5547,5547,in cases where an ai system presents unacceptable negative risk levels  such as where significant negative impacts are imminent severe harms are actually occurring or catastrophic risks are present  development and deployment should cease in a safe manner until risks can be sufficiently managed,8,3
5548,5548,if an ai systems development deployment and use cases are found to be low risk in a specific context that may suggest potentially lower prioritization,8,3
5549,5549,risk prioritization may differ between ai systems that are designed or deployed to directly interact with humans as compared to ai systems that are not,8,3
5550,5550,higher initial prioritization may be called for in settings where the ai system is trained on large datasets comprised of sensitive or protected data such as personally identifiable information or where the outputs of the ai systems have direct or indirect impact on humans,8,4
5551,5551,ai systems designed to interact only with computational systems and trained on non sensitive datasets for example data collected from the physical environment may call for lower initial prioritization,8,4
5552,5552,nonetheless regularly assessing and prioritizing risk based on context remains important because non human facing ai systems can have downstream safety or social implications,8,3
5553,5553,residual risk defined as risk remaining after risk treatment directly impacts end users or affected individuals and communities,8,3
5554,5554,documenting residual risks will call for the system provider to fully consider the risks of deploying the ai product and will inform end users about potential negative impacts of interacting with the system,8,3
5555,5555,ai risks should not be considered in isolation,8,3
5556,5556,different ai actors have different responsibilities and awareness depending on their roles in the lifecycle,8,4
5557,5557,for example organizations developing an ai system often will not have information about how the system may be used,8,15
5558,5558,ai risk management should be integrated and incorporated into broader enterprise risk management strategies and processes,8,3
5559,5559,treating ai risks along with other critical risks such as cybersecurity and privacy will yield a more integrated outcome and organizational efficiencies,8,3
5560,5560,the ai rmf may be utilized along with related guidance and frameworks for managing ai system risks or broader enterprise risks,8,3
5561,5561,some risks related to ai systems are common across other types of software development and deployment,8,3
5562,5562,examples of overlapping risks include privacy concerns related to the use of underlying data to train ai systems the energy and environmental implications associated with resource heavy computing demands security concerns related to the confidentiality integrity and availability of the system and its training and output data and general security of the underlying software and hardware for ai systems,8,3
5563,5563,organizations need to establish and maintain the appropriate accountability mechanisms roles and responsibilities culture and incentive structures for risk management to be effective,8,16
5564,5564,use of the ai rmf alone will not lead to these changes or provide the appropriate incentives,8,25
5565,5565,effective risk management is realized through organizational commitment at senior levels and may require cultural change within an organization or industry,8,3
5566,5566,in addition small to medium sized organizations managing ai risks or implementing the ai rmf may face different challenges than large organizations depending on their capabilities and resources,8,3
5567,5567,identifying and managing ai risks and potential impacts  both positive and negative  requires a broad set of perspectives and actors across the ai lifecycle,8,3
5568,5568,ideally ai actors will represent a diversity of experience expertise and backgrounds and comprise demographically and disciplinarily diverse teams,8,4
5569,5569,the ai rmf is intended to be used by ai actors across the ai lifecycle and dimensions,8,4
5570,5570,the oecd has developed a framework for classifying ai lifecycle activities according to five key socio technical dimensions each with properties relevant for ai policy and governance including risk management,8,3
5571,5571,the nist modification highlights the importance of test evaluation verification and validation tevv processes throughout an ai lifecycle and generalizes the operational context of an ai system,8,18
5572,5572,ai actors involved in these dimensions who perform or manage the design development deployment evaluation and use of ai systems and drive ai risk management efforts are the primary ai rmf audience,8,3
5573,5573,representative ai actors across the lifecycle dimensions are listed in figure  and described in detail in appendix a,8,4
5574,5574,within the ai rmf all ai actors work together to manage risks and achieve the goals of trustworthy and responsible ai,8,3
5575,5575,ai actors with tevv specific expertise are integrated throughout the ai lifecycle and are especially likely to benefit from the framework,8,4
5576,5576,performed regularly tevv tasks can provide insights relative to technical societal legal and ethical standards or norms and can assist with anticipating impacts and assessing and tracking emergent risks,8,12
5577,5577,as a regular process within an ai lifecycle tevv allows for both mid course remediation and post hoc risk management,8,3
5578,5578,the ai actors in this dimension comprise a separate ai rmf audience who informs the primary audience,8,4
5579,5579,these ai actors may include trade associations standards developing organizations researchers advocacy groups environmental groups civil society organizations end users and potentially impacted individuals and communities,8,4
5580,5580,these actors can assist in providing context and understanding potential and actual impacts,8,17
5581,5581,be a source of formal or quasi formal norms and guidance for ai risk management,8,3
5582,5582,designate boundaries for ai operation technical societal legal and ethical,8,2
5583,5583,promote discussion of the tradeoffs needed to balance societal values and priorities related to civil liberties and rights equity the environment and the planet and the economy,8,16
5584,5584,the ai rmf functions described in section  require diverse perspectives disciplines professions and experiences,8,4
5585,5585,diverse teams contribute to more open sharing of ideas and assumptions about the purposes and functions of technology making these implicit aspects more explicit,8,14
5586,5586,this broader collective perspective creates opportunities for surfacing problems and identifying existing and emergent risks,8,3
5587,5587,for ai systems to be trustworthy they often need to be responsive to a multiplicity of criteria that are of value to interested parties,8,18
5588,5588,approaches which enhance ai trustworthiness can reduce negative ai risks,8,3
5589,5589,this framework articulates the following characteristics of trustworthy ai and offers guidance for addressing them,8,18
5590,5590,characteristics of trustworthy ai systems include valid and reliable safe secure and resilient accountable and transparent explainable and interpretable privacy enhanced and fair with harmful bias managed,8,18
5591,5591,creating trustworthy ai requires balancing each of these characteristics based on the ai systems context of use,8,4
5592,5592,while all characteristics are socio technical system attributes accountability and transparency also relate to the processes and activities internal to an ai system and its external setting,8,16
5593,5593,neglecting these characteristics can increase the probability and magnitude of negative consequences,8,22
5594,5594,trustworthiness characteristics are inextricably tied to social and organizational behavior the datasets used by ai systems selection of ai models and algorithms and the decisions made by those who build them and the interactions with the humans who provide insight from and oversight of such systems,8,18
5595,5595,human judgment should be employed when deciding on the specific metrics related to ai trustworthiness characteristics and the precise threshold values for those metrics,8,18
5596,5596,addressing ai trustworthiness characteristics individually will not ensure ai system trustworthiness tradeoffs are usually involved rarely do all characteristics apply in every setting and some will be more or less important in any given situation,8,18
5597,5597,ultimately trustworthiness is a social concept that ranges across a spectrum and is only as strong as its weakest characteristics,8,18
5598,5598,when managing ai risks organizations can face difficult decisions in balancing these characteristics,8,3
5599,5599,for example in certain scenarios tradeoffs may emerge between optimizing for interpretability and achieving privacy,8,14
5600,5600,in other cases organizations might face a tradeoff between predictive accuracy and interpretability,8,14
5601,5601,or under certain conditions such as data sparsity privacy enhancing techniques can result in a loss in accuracy affecting decisions about fairness and other values in certain domains,8,6
5602,5602,dealing with tradeoffs requires taking into account the decision making context,8,21
5603,5603,these analyses can highlight the existence and extent of tradeoffs between different measures but they do not answer questions about how to navigate the tradeoff,8,22
5604,5604,those depend on the values at play in the relevant context and should be resolved in a manner that is both transparent and appropriately justifiable,8,1
5605,5605,there are multiple approaches for enhancing contextual awareness in the ai lifecycle,8,4
5606,5606,for example subject matter experts can assist in the evaluation of tevv findings and work with product and deployment teams to align tevv parameters to requirements and deployment conditions,8,27
5607,5607,when properly resourced increasing the breadth and diversity of input from interested parties and relevant ai actors throughout the ai lifecycle can enhance opportunities for informing contextually sensitive evaluations and for identifying ai system benefits and positive impacts,8,4
5608,5608,these practices can increase the likelihood that risks arising in social contexts are managed appropriately,8,3
5609,5609,understanding and treatment of trustworthiness characteristics depends on an ai actors particular role within the ai lifecycle,8,4
5610,5610,for any given ai system an ai designer or developer may have a different perception of the characteristics than the deployer,8,20
5611,5611,trustworthiness characteristics explained in this document influence each other,8,14
5612,5612,highly secure but unfair systems accurate but opaque and uninterpretable systems and inaccurate but secure privacy enhanced and transparent systems are all undesirable,8,11
5613,5613,a comprehensive approach to risk management calls for balancing tradeoffs among the trustworthiness characteristics,8,3
5614,5614,it is the joint responsibility of all ai actors to determine whether ai technology is an appropriate or necessary tool for a given context or purpose and how to use it responsibly,8,4
5615,5615,the decision to commission or deploy an ai system should be based on a contextual assessment of trustworthiness characteristics and the relative risks impacts costs and benefits and informed by a broad set of interested parties,8,21
5616,5616,validation is the confirmation through the provision of objective evidence that the requirements for a specific intended use or application have been fulfilled,8,18
5617,5617,deployment of ai systems which are inaccurate unreliable or poorly generalized to data and settings beyond their training creates and increases negative ai risks and reduces trustworthiness,8,18
5618,5618,reliability is defined in the same standard as the ability of an item to perform as required without failure for a given time interval under given conditions,8,18
5619,5619,reliability is a goal for overall correctness of ai system operation under the conditions of expected use and over a given period of time including the entire lifetime of the system,8,18
5620,5620,accuracy and robustness contribute to the validity and trustworthiness of ai systems and can be in tension with one another in ai systems,8,18
5621,5621,accuracy is defined as closeness of results of observations computations or estimates to the true values or the values accepted as being true measures of accuracy should consider computational centric measures eg false positive and false negative rates human ai teaming and demonstrate external validity generalizable beyond the training conditions,8,18
5622,5622,accuracy measurements should always be paired with clearly defined and realistic test sets  that are representative of conditions of expected use and details about test methodology these should be included in associated documentation,8,18
5623,5623,accuracy measurements may include disaggregation of results for different data segments,8,18
5624,5624,robustness or generalizability is defined as the ability of a system to maintain its level of performance under a variety of circumstances,8,26
5625,5625,robustness is a goal for appropriate system functionality in a broad set of conditions and circumstances including uses of ai systems not initially anticipated,8,5
5626,5626,robustness requires not only that the system perform exactly as it does under expected uses but also that it should perform in ways that minimize potential harms to people if it is operating in an unexpected setting,8,15
5627,5627,validity and reliability for deployed ai systems are often assessed by ongoing testing or monitoring that confirms a system is performing as intended,8,18
5628,5628,measurement of validity accuracy robustness and reliability contribute to trustworthiness and should take into consideration that certain types of failures can cause greater harm,8,18
5629,5629,ai risk management efforts should prioritize the minimization of potential negative impacts and may need to include human intervention in cases where the ai system cannot detect or correct errors,8,3
5630,5630,ai systems should not under defined conditions lead to a state in which human life health property or the environment is endangered,8,5
5631,5631,safe operation of ai systems is improved through responsible design development and deployment practices,8,20
5632,5632,clear information to deployers on responsible use of the system,8,15
5633,5633,responsible decision making by deployers and end users,8,20
5634,5634,explanations and documentation of risks based on empirical evidence of incidents,8,14
5635,5635,different types of safety risks may require tailored ai risk management approaches based on context and the severity of potential risks presented,8,3
5636,5636,safety risks that pose a potential risk of serious injury or death call for the most urgent prioritization and most thorough risk management process,8,3
5637,5637,employing safety considerations during the lifecycle and starting as early as possible with planning and design can prevent failures or conditions that can render a system dangerous,8,3
5638,5638,other practical approaches for ai safety often relate to rigorous simulation and in domain testing real time monitoring and the ability to shut down modify or have human intervention into systems that deviate from intended or expected functionality,8,3
5639,5639,ai safety risk management approaches should take cues from efforts and guidelines for safety in fields such as transportation and healthcare and align with existing sectoror application specific guidelines or standards,8,3
5640,5640,ai systems as well as the ecosystems in which they are deployed may be said to be resilient if they can withstand unexpected adverse events or unexpected changes in their environment or use  or if they can maintain their functions and structure in the face of internal and external change and degrade safely and gracefully when this is necessary,8,4
5641,5641,common security concerns relate to adversarial examples data poisoning and the exfiltration of models training data or other intellectual property through ai system endpoints,8,25
5642,5642,ai systems that can maintain confidentiality integrity and availability through protection mechanisms that prevent unauthorized access and use may be said to be secure,8,11
5643,5643,security and resilience are related but distinct characteristics,8,11
5644,5644,while resilience is the ability to return to normal function after an unexpected adverse event security includes resilience but also encompasses protocols to avoid protect against respond to or recover from attacks,8,11
5645,5645,resilience relates to robustness and goes beyond the provenance of the data to encompass unexpected or adversarial use or abuse or misuse of the model or data,8,11
5646,5646,trustworthy ai depends upon accountability,8,16
5647,5647,accountability presupposes transparency,8,16
5648,5648,transparency reflects the extent to which information about an ai system and its outputs is available to individuals interacting with such a system  regardless of whether they are even aware that they are doing so,8,14
5649,5649,meaningful transparency provides access to appropriate levels of information based on the stage of the ai lifecycle and tailored to the role or knowledge of ai actors or individuals interacting with or using the ai system,8,14
5650,5650,by promoting higher levels of understanding transparency increases confidence in the ai system,8,14
5651,5651,this characteristics scope spans from design decisions and training data to model training the structure of the model its intended use cases and how and when deployment post deployment or end user decisions were made and by whom,8,20
5652,5652,transparency is often necessary for actionable redress related to ai system outputs that are incorrect or otherwise lead to negative impacts,8,14
5653,5653,transparency should consider human ai interaction,8,14
5654,5654,for example how a human operator or user is notified when a potential or actual adverse outcome caused by an ai system is detected,8,4
5655,5655,a transparent system is not necessarily an accurate privacy enhanced secure or fair system,8,11
5656,5656,however it is difficult to determine whether an opaque system possesses such characteristics and to do so over time as complex systems evolve,8,14
5657,5657,the role of ai actors should be considered when seeking accountability for the outcomes of ai systems,8,16
5658,5658,the relationship between risk and accountability associated with ai and technological systems more broadly differs across cultural legal sectoral and societal contexts,8,16
5659,5659,when consequences are severe such as when life and liberty are at stake ai developers and deployers should consider proportionally and proactively adjusting their transparency and accountability practices,8,16
5660,5660,maintaining organizational practices and governing structures for harm reduction like risk management can help lead to more accountable systems,8,3
5661,5661,measures to enhance transparency and accountability should also consider the impact of these efforts on the implementing entity including the level of necessary resources and the need to safeguard proprietary information,8,17
5662,5662,maintaining the provenance of training data and supporting attribution of the ai systems decisions to subsets of training data can assist with both transparency and accountability,8,13
5663,5663,training data may also be subject to copyright and should follow applicable intellectual property rights laws,8,13
5664,5664,as transparency tools for ai systems and related documentation continue to evolve developers of ai systems are encouraged to test different types of transparency tools in cooperation with ai deployers to ensure that ai systems are used as intended,8,14
5665,5665,explainability refers to a representation of the mechanisms underlying ai systems operation whereas interpretability refers to the meaning of ai systems output in the context of their designed functional purposes,8,14
5666,5666,together explainability and interpretability assist those operating or overseeing an ai system as well as users of an ai system to gain deeper insights into the functionality and trustworthiness of the system including its outputs,8,14
5667,5667,the underlying assumption is that perceptions of negative risk stem from a lack of ability to make sense of or contextualize system output appropriately,8,3
5668,5668,explainable and interpretable ai systems offer information that will help end users understand the purposes and potential impact of an ai system,8,14
5669,5669,risk from lack of explainability may be managed by describing how ai systems function with descriptions tailored to individual differences such as the users role knowledge and skill level,8,14
5670,5670,explainable systems can be debugged and monitored more easily and they lend themselves to more thorough documentation audit and governance,8,14
5671,5671,risks to interpretability often can be addressed by communicating a description of why an ai system made a particular prediction or recommendation,8,14
5672,5672,transparency explainability and interpretability are distinct characteristics that support each other,8,14
5673,5673,transparency can answer the question of what happened in the system,8,14
5674,5674,explainability can answer the question of how a decision was made in the system,8,14
5675,5675,interpretability can answer the question of why a decision was made by the system and its meaning or context to the user,8,14
5676,5676,privacy refers generally to the norms and practices that help to safeguard human autonomy identity and dignity,8,12
5677,5677,these norms and practices typically address freedom from intrusion limiting observation or individuals agency to consent to disclosure or control of facets of their identities eg body data reputation,8,12
5678,5678,privacy values such as anonymity confidentiality and control generally should guide choices for ai system design development and deployment,8,11
5679,5679,privacy related risks may influence security bias and transparency and come with tradeoffs with these other characteristics,8,11
5680,5680,like safety and security specific technical features of an ai system may promote or reduce privacy,8,11
5681,5681,ai systems can also present new risks to privacy by allowing inference to identify individuals or previously private information about individuals,8,12
5682,5682,privacy enhancing technologies pets for ai as well as data minimizing methods such as de identification and aggregation for certain model outputs can support design for privacy enhanced ai systems,8,11
5683,5683,under certain conditions such as data sparsity privacyenhancing techniques can result in a loss in accuracy affecting decisions about fairness and other values in certain domains,8,6
5684,5684,fairness in ai includes concerns for equality and equity by addressing issues such as harmful bias and discrimination,8,6
5685,5685,standards of fairness can be complex and difficult to define because perceptions of fairness differ among cultures and may shift depending on application,8,6
5686,5686,organizations risk management efforts will be enhanced by recognizing and considering these differences,8,3
5687,5687,systems in which harmful biases are mitigated are not necessarily fair,8,6
5688,5688,for example systems in which predictions are somewhat balanced across demographic groups may still be inaccessible to individuals with disabilities or affected by the digital divide or may exacerbate existing disparities or systemic biases,8,6
5689,5689,bias is broader than demographic balance and data representativeness,8,6
5690,5690,nist has identified three major categories of ai bias to be considered and managed systemic computational and statistical and human cognitive,8,6
5691,5691,each of these can occur in the absence of prejudice partiality or discriminatory intent,8,6
5692,5692,systemic bias can be present in ai datasets the organizational norms practices and processes across the ai lifecycle and the broader society that uses ai systems,8,6
5693,5693,computational and statistical biases can be present in ai datasets and algorithmic processes and often stem from systematic errors due to non representative samples,8,6
5694,5694,human cognitive biases relate to how an individual or group perceives ai system information to make a decision or fill in missing information or how humans think about purposes and functions of an ai system,8,6
5695,5695,human cognitive biases are omnipresent in decision making processes across the ai lifecycle and system use including the design implementation operation and maintenance of ai,8,4
5696,5696,bias exists in many forms and can become ingrained in the automated systems that help make decisions about our lives,8,6
5697,5697,while bias is not always a negative phenomenon ai systems can potentially increase the speed and scale of biases and perpetuate and amplify harms to individuals groups communities organizations and society,8,6
5698,5698,bias is tightly associated with the concepts of transparency as well as fairness in society,8,6
5699,5699,evaluations of ai rmf effectiveness  including ways to measure bottom line improvements in the trustworthiness of ai systems  will be part of future nist activities in conjunction with the ai community,8,18
5700,5700,organizations and other users of the framework are encouraged to periodically evaluate whether the ai rmf has improved their ability to manage ai risks including but not limited to their policies processes practices implementation plans indicators measurements and expected outcomes,8,3
5701,5701,nist intends to work collaboratively with others to develop metrics methodologies and goals for evaluating the ai rmfs effectiveness and to broadly share results and supporting information,8,18
5702,5702,framework users are expected to benefit from,8,15
5703,5703,enhanced processes for governing mapping measuring and managing ai risk and clearly documenting outcomes,8,3
5704,5704,improved awareness of the relationships and tradeoffs among trustworthiness characteristics socio technical approaches and ai risks,8,3
5705,5705,explicit processes for making gono go system commissioning and deployment decisions,8,20
5706,5706,established policies processes practices and procedures for improving organizational accountability efforts related to ai system risks,8,16
5707,5707,enhanced organizational culture which prioritizes the identification and management of ai system risks and potential impacts to individuals communities organizations and society,8,3
5708,5708,better information sharing within and across organizations about risks decisionmaking processes responsibilities common pitfalls tevv practices and approaches for continuous improvement,8,3
5709,5709,greater contextual knowledge for increased awareness of downstream risks,8,3
5710,5710,strengthened engagement with interested parties and relevant ai actors,8,4
5711,5711,augmented capacity for tevv of ai systems and associated risks,8,5
5712,5712,the ai rmf core provides outcomes and actions that enable dialogue understanding and activities to manage ai risks and responsibly develop trustworthy ai systems,8,3
5713,5713,the core is composed of four functions govern map measure and manage,8,26
5714,5714,each of these high level functions is broken down into categories and subcategories,8,26
5715,5715,categories and subcategories are subdivided into specific actions and outcomes,8,24
5716,5716,actions do not constitute a checklist nor are they necessarily an ordered set of steps,8,21
5717,5717,governance is designed to be a cross cutting function to inform and be infused throughout the other three functions,8,26
5718,5718,risk management should be continuous timely and performed throughout the ai system lifecycle dimensions,8,3
5719,5719,ai rmf core functions should be carried out in a way that reflects diverse and multidisciplinary perspectives potentially including the views of ai actors outside the organization,8,4
5720,5720,having a diverse team contributes to more open sharing of ideas and assumptions about purposes and functions of the technology being designed developed deployed or evaluated  which can create opportunities to surface problems and identify existing and emergent risks,8,27
5721,5721,an online companion resource to the ai rmf the nist ai rmf playbook is available to help organizations navigate the ai rmf and achieve its outcomes through suggested tactical actions they can apply within their own contexts,8,4
5722,5722,like the ai rmf the playbook is voluntary and organizations can utilize the suggestions according to their needs and interests,8,27
5723,5723,playbook users can create tailored guidance selected from suggested material for their own use and contribute their suggestions for sharing with the broader community,8,27
5724,5724,along with the ai rmf the playbook is part of the nist trustworthy and responsible ai resource center,8,27
5725,5725,framework users may apply these functions as best suits their needs for managing ai risks based on their resources and capabilities,8,3
5726,5726,some organizations may choose to select from among the categories and subcategories others may choose and have the capacity to apply all categories and subcategories,8,26
5727,5727,assuming a governance structure is in place functions may be performed in any order across the ai lifecycle as deemed to add value by a user of the framework,8,26
5728,5728,after instituting the outcomes in govern most users of the ai rmf would start with the map function and continue to measure or manage,8,17
5729,5729,however users integrate the functions the process should be iterative with cross referencing between functions as necessary,8,13
5730,5730,similarly there are categories and subcategories with elements that apply to multiple functions or that logically should take place before certain subcategory decisions,8,26
5731,5731,the govern function cultivates and implements a culture of risk management within organizations designing developing deploying evaluating or acquiring ai systems,8,3
5732,5732,outlines processes documents and organizational schemes that anticipate identify and manage the risks a system can pose including to users and others across society  and procedures to achieve those outcomes,8,15
5733,5733,incorporates processes to assess potential impacts,8,17
5734,5734,provides a structure by which ai risk management functions can align with organizational principles policies and strategic priorities,8,3
5735,5735,connects technical aspects of ai system design and development to organizational values and principles and enables organizational practices and competencies for the individuals involved in acquiring training deploying and monitoring such systems,8,4
5736,5736,addresses full product lifecycle and associated processes including legal and other issues concerning use of third party software or hardware systems and data,8,15
5737,5737,govern is a cross cutting function that is infused throughout ai risk management and enables the other functions of the process,8,3
5738,5738,aspects of govern especially those related to compliance or evaluation should be integrated into each of the other functions,8,26
5739,5739,attention to governance is a continual and intrinsic requirement for effective ai risk management over an ai systems lifespan and the organizations hierarchy,8,3
5740,5740,strong governance can drive and enhance internal practices and norms to facilitate organizational risk culture,8,3
5741,5741,governing authorities can determine the overarching policies that direct an organizations mission goals values culture and risk tolerance,8,3
5742,5742,senior leadership sets the tone for risk management within an organization and with it organizational culture,8,3
5743,5743,management aligns the technical aspects of ai risk management to policies and operations,8,3
5744,5744,documentation can enhance transparency improve human review processes and bolster accountability in ai system teams,8,14
5745,5745,after putting in place the structures systems processes and teams described in the govern function organizations should benefit from a purpose driven culture focused on risk understanding and management,8,3
5746,5746,it is incumbent on framework users to continue to execute the govern function as knowledge cultures and needs or expectations from ai actors evolve over time,8,8
5747,5747,govern policies processes procedures and practices across the organization related to the mapping measuring and managing of ai risks are in place transparent and implemented effectively,8,3
5748,5748,govern  legal and regulatory requirements involving aiare understood managed and documented,8,8
5749,5749,govern  the characteristics of trustworthy ai are integrated into organizational policies processes procedures and practices,8,11
5750,5750,govern  processes procedures and practices are in place to determine the needed level of risk management activities based on the organizations risk tolerance,8,3
5751,5751,govern  the risk management process and its outcomes are established through transparent policies procedures and other controls based on organizational risk priorities,8,3
5752,5752,govern  ongoing monitoring and periodic review of the risk management process and its outcomes are planned and organizational roles and responsibilities clearly defined including determining the frequency of periodic review,8,3
5753,5753,govern  mechanisms are in place to inventory ai systems and are resourced according to organizational risk priorities,8,3
5754,5754,govern  processes and procedures are in place for decommissioning and phasing out ai systems safely and in a manner that does not increase risks or decrease the organizations trustworthiness,8,16
5755,5755,govern  accountability structures are in place so that the appropriate teams and individuals are empowered responsible and trained for mapping measuring and managing ai risks,8,16
5756,5756,govern  roles and responsibilities and lines of communication related to mapping measuring and managing ai risks are documented and are clear to individuals and teams throughout the organization,8,3
5757,5757,govern  the organizations personnel and partners receive ai risk management training to enable them to perform their duties and responsibilities consistent with related policies procedures and agreements,8,3
5758,5758,govern  executive leadership of the organization takes responsibility for decisions about risks associated with ai system development and deployment,8,3
5759,5759,govern workforce diversity equity inclusion and accessibility processes are prioritized in the mapping measuring and managing of ai risks throughout the lifecycle,8,6
5760,5760,govern  decision making related to mapping measuring and managing ai risks throughout the lifecycle is informed by a diverse team eg diversity of demographics disciplines experience expertise and backgrounds,8,3
5761,5761,govern  policies and procedures are in place to define and differentiate roles and responsibilities for human ai configurations and oversight of ai systems,8,16
5762,5762,govern  organizational teams are committed to a culture that considers and communicates ai risk,8,3
5763,5763,govern  organizational policies and practices are in place to foster a critical thinking and safety first mindset in the design development deployment and uses of ai systems to minimize potential negative impacts,8,3
5764,5764,govern  organizational teams document the risks and potential impacts of the ai technology they design develop deploy evaluate and use and they communicate about the impacts more broadly,8,3
5765,5765,govern  organizational practices are in place to enable ai testing identification of incidents and information sharing,8,1
5766,5766,govern processes are in place for robust engagement with relevant ai actors,8,4
5767,5767,govern  organizational policies and practices are in place to collect consider prioritize and integrate feedback from those external to the team that developed or deployed the ai system regarding the potential individual and societal impacts related to ai risks,8,3
5768,5768,govern  mechanisms are established to enable the team that developed or deployed ai systems to regularly incorporate adjudicated feedback from relevant ai actors into system design and implementation,8,4
5769,5769,govern  policies and procedures are in place to address ai risks and benefits arising from third party software and data and other supply chain issues,8,2
5770,5770,govern  policies and procedures are in place that address ai risks associated with third party entities including risks of infringement of a third partys intellectual property or other rights,8,3
5771,5771,govern  contingency processes are in place to handle failures or incidents in third party data or ai systems deemed to be high risk,8,3
5772,5772,the map function establishes the context to frame risks related to an ai system,8,3
5773,5773,the ai lifecycle consists of many interdependent activities involving a diverse set of actors see figure,8,4
5774,5774,in practice ai actors in charge of one part of the process often do not have full visibility or control over other parts and their associated contexts,8,4
5775,5775,the interdependencies between these activities and among the relevant ai actors can make it difficult to reliably anticipate impacts of ai systems,8,4
5776,5776,for example early decisions in identifying purposes and objectives of an ai system can alter its behavior and capabilities and the dynamics of deployment setting such as end users or impacted individuals can shape the impacts of ai system decisions,8,4
5777,5777,as a result the best intentions within one dimension of the ai lifecycle can be undermined via interactions with decisions and conditions in other later activities,8,4
5778,5778,this complexity and varying levels of visibility can introduce uncertainty into risk management practices,8,3
5779,5779,anticipating assessing and otherwise addressing potential sources of negative risk can mitigate this uncertainty and enhance the integrity of the decision process,8,3
5780,5780,the information gathered while carrying out the map function enables negative risk prevention and informs decisions for processes such as model management as well as an initial decision about appropriateness or the need for an ai solution,8,3
5781,5781,outcomes in the map function are the basis for the measure and manage functions,8,26
5782,5782,without contextual knowledge and awareness of risks within the identified contexts risk management is difficult to perform,8,3
5783,5783,the map function is intended to enhance an organizations ability to identify risks and broader contributing factors,8,3
5784,5784,implementation of this function is enhanced by incorporating perspectives from a diverse internal team and engagement with those external to the team that developed or deployed the ai system,8,4
5785,5785,engagement with external collaborators end users potentially impacted communities and others may vary based on the risk level of a particular ai system the makeup of the internal team and organizational policies,8,3
5786,5786,gathering such broad perspectives can help organizations proactively prevent negative risks and develop more trustworthy ai systems by improving their capacity for understanding contexts,8,3
5787,5787,checking their assumptions about context of use,8,14
5788,5788,enabling recognition of when systems are not functional within or out of their in tended context,8,15
5789,5789,identifying positive and beneficial uses of their existing ai systems,8,4
5790,5790,improving understanding of limitations in ai and ml processes,8,14
5791,5791,identifying constraints in real world applications that may lead to negative impacts,8,25
5792,5792,identifying known and foreseeable negative impacts related to intended use of ai systems,8,4
5793,5793,anticipating risks of the use of ai systems beyond intended use,8,3
5794,5794,after completing the map function framework users should have sufficient contextual knowledge about ai system impacts to inform an initial gono go decision about whether to design develop or deploy an ai system,8,4
5795,5795,if a decision is made to proceed organizations should utilize the measure and manage functions along with policies and procedures put into place in the govern function to assist in ai risk management efforts,8,3
5796,5796,it is incumbent on framework users to continue applying the map function to ai systems as context capabilities risks benefits and potential impacts evolve over time,8,4
5797,5797,map  context is established and understood,8,26
5798,5798,map  intended purposes potentially beneficial uses contextspecific laws norms and expectations and prospective settings in which the ai system will be deployed are understood and documented,8,2
5799,5799,considerations include the specific set or types of users along with their expectations potential positive and negative impacts of system uses to individuals communities organizations society and the planet assumptions and related limitations about ai system purposes uses and risks across the development or product ai lifecycle and related tevv and system metrics,8,4
5800,5800,map  interdisciplinary ai actors competencies skills and capacities for establishing context reflect demographic diversity and broad domain and user experience expertise and their participation is documented,8,4
5801,5801,opportunities for interdisciplinary collaboration are prioritized,8,27
5802,5802,map  the organizations mission and relevant goals for ai technology are understood and documented,8,2
5803,5803,map  the business value or context of business use has been clearly defined or  in the case of assessing existing ai systems  re evaluated,8,0
5804,5804,map  organizational risk tolerances are determined and documented,8,3
5805,5805,map  system requirements eg the system shall respect the privacy of its users are elicited from and understood by relevant ai actors,8,1
5806,5806,design decisions take socio technical implications into account to address ai risks,8,3
5807,5807,map  categorization of the ai system is performed,8,4
5808,5808,map  the specific tasks and methods used to implement the tasks that the ai system will support are defined eg classifiers generative models recommenders,8,4
5809,5809,map  information about the ai systems knowledge limits and how system output may be utilized and overseen by humans is documented,8,4
5810,5810,documentation provides sufficient information to assist relevant ai actors when making decisions and taking subsequent actions,8,4
5811,5811,map  scientific integrity and tevv considerations are identified and documented including those related to experimental design data collection and selection eg availability representativeness suitability system trustworthiness and construct validation,8,18
5812,5812,map  ai capabilities targeted usage goals and expected benefits and costs compared with appropriate benchmarks are understood,8,4
5813,5813,map  potential benefits of intended ai system functionality and performance are examined and documented,8,4
5814,5814,map  potential costs including non monetary costs which result from expected or realized ai errors or system functionality and trustworthiness  as connected to organizational risk tolerance  are examined and documented,8,3
5815,5815,map  targeted application scope is specified and documented based on the systems capability established context and ai system categorization,8,26
5816,5816,map  processes for operator and practitioner proficiency with ai system performance and trustworthiness  and relevant technical standards and certifications  are defined assessed and documented,8,18
5817,5817,map  processes for human oversight are defined assessed and documented in accordance with organizational policies from the govern function,8,15
5818,5818,map  risks and benefits are mapped for all components of the ai system including third party software and data,8,3
5819,5819,map  approaches for mapping ai technology and legal risks of its components  including the use of third party data or software  are in place followed and documented as are risks of infringement of a third partys intellectual property or other rights,8,2
5820,5820,map  internal risk controls for components of the ai system including third party ai technologies are identified and documented,8,3
5821,5821,map  impacts to individuals groups communities organizations and society are characterized,8,17
5822,5822,map  likelihood and magnitude of each identified impact both potentially beneficial and harmful based on expected use past uses of ai systems in similar contexts public incident reports feedback from those external to the team that developed or deployed the ai system or other data are identified and documented,8,4
5823,5823,map  practices and personnel for supporting regular engagement with relevant ai actors and integrating feedback about positive negative and unanticipated impacts are in place and documented,8,4
5824,5824,the measure function employs quantitative qualitative or mixed method tools techniques and methodologies to analyze assess benchmark and monitor ai risk and related impacts,8,18
5825,5825,it uses knowledge relevant to ai risks identified in the map function and informs the manage function,8,3
5826,5826,ai systems should be tested before their deployment and regularly while in operation,8,20
5827,5827,ai risk measurements include documenting aspects of systems functionality and trustworthiness,8,3
5828,5828,measuring ai risks includes tracking metrics for trustworthy characteristics social impact and human ai configurations,8,3
5829,5829,processes developed or adopted in the measure function should include rigorous software testing and performance assessment methodologies with associated measures of uncertainty comparisons to performance benchmarks and formalized reporting and documentation of results,8,18
5830,5830,processes for independent review can improve the effectiveness of testing and can mitigate internal biases and potential conflicts of interest,8,18
5831,5831,where tradeoffs among the trustworthy characteristics arise measurement provides a traceable basis to inform management decisions,8,18
5832,5832,options may include recalibration impact mitigation or removal of the system from design development production or use as well as a range of compensating detective deterrent directive and recovery controls,8,25
5833,5833,after completing the measure function objective repeatable or scalable test evaluation verification and validation tevv processes including metrics methods and methodologies are in place followed and documented,8,18
5834,5834,metrics and measurement methodologies should adhere to scientific legal and ethical norms and be carried out in an open and transparent process,8,18
5835,5835,new types of measurement qualitative and quantitative may need to be developed,8,18
5836,5836,the degree to which each measurement type provides unique and meaningful information to the assessment of ai risks should be considered,8,3
5837,5837,framework users will enhance their capacity to comprehensively evaluate system trustworthiness identify and track existing and emergent risks and verify efficacy of the metrics,8,18
5838,5838,measurement outcomes will be utilized in the manage function to assist risk monitoring and response efforts,8,3
5839,5839,it is incumbent on framework users to continue applying the measure function to ai systems as knowledge methodologies risks and impacts evolve over time,8,18
5840,5840,measure appropriate methods and metrics are identified and applied,8,18
5841,5841,measure  approaches and metrics for measurement of ai risks enumerated during the m a p function are selected for implementation starting with the most significant ai risks,8,3
5842,5842,the risks or trustworthiness characteristics that will not  or cannot  be measured are properly documented,8,18
5843,5843,measure  appropriateness of ai metrics and effectiveness of existing controls are regularly assessed and updated including reports of errors and potential impacts on affected communities,8,18
5844,5844,measure  internal experts who did not serve as front line developers for the system andor independent assessors are involved in regular assessments and updates,8,18
5845,5845,domain experts users ai actors external to the team that developed or deployed the ai system and affected communities are consulted in support of assessments as necessary per organizational risk tolerance,8,3
5846,5846,measure  ai systems are evaluated for trustworthy characteristics,8,18
5847,5847,measure  test sets metrics and details about the tools used during tevv are documented,8,18
5848,5848,measure  evaluations involving human subjects meet applicable requirements including human subject protection and are representative of the relevant population,8,12
5849,5849,measure  ai system performance or assurance criteria are measured qualitatively or quantitatively and demonstrated for conditions similar to deployment settings,8,18
5850,5850,measures are documented,8,18
5851,5851,measure  the functionality and behavior of the ai system and its components  as identified in the map function  are monitored when in production,8,4
5852,5852,measure  the ai system to be deployed is demonstrated to be valid and reliable,8,18
5853,5853,limitations of the generalizability beyond the conditions under which the technology was developed are documented,8,14
5854,5854,measure  the ai system is evaluated regularly for safety risks  as identified in the map function,8,3
5855,5855,the ai system to be deployed is demonstrated to be safe its residual negative risk does not exceed the risk tolerance and it can fail safely particularly if made to operate beyond its knowledge limits,8,5
5856,5856,safety metrics reflect system reliability and robustness real time monitoring and response times for ai system failures,8,18
5857,5857,measure  ai system security and resilience  as identified in the map function  are evaluated and documented,8,4
5858,5858,measure  risks associated with transparency and accountability  as identified in the map function  are examined and documented,8,18
5859,5859,measure  the ai model is explained validated and documented and ai system output is interpreted within its context  as identified in the map function  to inform responsible use and governance,8,18
5860,5860,measure  privacy risk of the ai system  as identified in the map function  is examined and documented,8,2
5861,5861,measure  fairness and bias  as identified in the map function  are evaluated and results are documented,8,6
5862,5862,measure  environmental impact and sustainability of ai model training and management activities  as identified in the map function  are assessed and documented,8,4
5863,5863,measure  effectiveness of the employed tevv metrics and processes in the measure function are evaluated and documented,8,18
5864,5864,measure  mechanisms for tracking identified ai risks over time are in place,8,3
5865,5865,measure  approaches personnel and documentation are in place to regularly identify and track existing unanticipated and emergent ai risks based on factors such as intended and actual performance in deployed contexts,8,3
5866,5866,measure  risk tracking approaches are considered for settings where ai risks are difficult to assess using currently available measurement techniques or where metrics are not yet available,8,3
5867,5867,measure  feedback processes for end users and impacted communities to report problems and appeal system outcomes are established and integrated into ai system evaluation metrics,8,18
5868,5868,measure feedback about efficacy of measurement is gathered and assessed,8,18
5869,5869,measure  measurement approaches for identifying ai risks are connected to deployment contexts and informed through consultation with domain experts and other end users,8,3
5870,5870,approaches are documented,8,14
5871,5871,measure  measurement results regarding ai system trustworthiness in deployment contexts and across the ai lifecycle are informed by input from domain experts and relevant ai actors to validate whether the system is performing consistently as intended,8,18
5872,5872,results are documented,8,14
5873,5873,measure  measurable performance improvements or declines based on consultations with relevant ai actors including affected communities and field data about contextrelevant risks and trustworthiness characteristics are identified and documented,8,18
5874,5874,the manage function entails allocating risk resources to mapped and measured risks on a regular basis and as defined by the govern function,8,3
5875,5875,risk treatment comprises plans to respond to recover from and communicate about incidents or events,8,3
5876,5876,contextual information gleaned from expert consultation and input from relevant ai actors  established in govern and carried out in map  is utilized in this function to decrease the likelihood of system failures and negative impacts,8,3
5877,5877,systematic documentation practices established in govern and utilized in map and measure bolster ai risk management efforts and increase transparency and accountability,8,3
5878,5878,processes for assessing emergent risks are in place along with mechanisms for continual improvement,8,3
5879,5879,after completing the manage function plans for prioritizing risk and regular monitoring and improvement will be in place,8,3
5880,5880,framework users will have enhanced capacity to manage the risks of deployed ai systems and to allocate risk management resources based on assessed and prioritized risks,8,3
5881,5881,it is incumbent on framework users to continue to apply the manage function to deployed ai systems as methods contexts risks and needs or expectations from relevant ai actors evolve over time,8,15
5882,5882,manage  ai risks based on assessments and other analytical output from the map and measure functions are prioritized responded to and managed,8,3
5883,5883,manage  a determination is made as to whether the ai system achieves its intended purposes and stated objectives and whether its development or deployment should proceed,8,4
5884,5884,manage  treatment of documented ai risks is prioritized based on impact likelihood and available resources or methods,8,3
5885,5885,manage  responses to the ai risks deemed high priority as identified by the map function are developed planned and documented,8,3
5886,5886,risk response options can include mitigating transferring avoiding or accepting,8,3
5887,5887,manage  negative residual risks defined as the sum of all unmitigated risks to both downstream acquirers of ai systems and end users are documented,8,3
5888,5888,manage strategies to maximize ai benefits and minimize negative impacts are planned prepared implemented documented and informed by input from relevant ai actors,8,4
5889,5889,manage  resources required to manage ai risks are taken into account  along with viable non ai alternative systems approaches or methods  to reduce the magnitude or likelihood of potential impacts,8,3
5890,5890,manage  mechanisms are in place and applied to sustain the value of deployed ai systems,8,4
5891,5891,manage  procedures are followed to respond to and recover from a previously unknown risk when it is identified,8,3
5892,5892,manage  mechanisms are in place and applied and responsibilities are assigned and understood to supersede disengage or deactivate ai systems that demonstrate performance or outcomes inconsistent with intended use,8,4
5893,5893,manage  ai risks and benefits from third party entities are managed,8,3
5894,5894,manage  ai risks and benefits from third party resources are regularly monitored and risk controls are applied and documented,8,3
5895,5895,manage  pre trained models which are used for development are monitored as part of ai system regular monitoring and maintenance,8,20
5896,5896,manage  risk treatments including response and recovery and communication plans for the identified and measured ai risks are documented and monitored regularly,8,3
5897,5897,manage  post deployment ai system monitoring plans are implemented including mechanisms for capturing and evaluating input from users and other relevant ai actors appeal and override decommissioning incident response recovery and change management,8,17
5898,5898,manage  measurable activities for continual improvements are integrated into ai system updates and include regular engagement with interested parties including relevant ai actors,8,4
5899,5899,manage  incidents and errors are communicated to relevant ai actors including affected communities,8,4
5900,5900,processes for tracking responding to and recovering from incidents and errors are followed and documented,8,15
5901,5901,ai rmf use case profiles are implementations of the ai rmf functions categories and subcategories for a specific setting or application based on the requirements risk tolerance and resources of the framework user for example an ai rmf hiring profile or an ai rmf fair housing profile,8,3
5902,5902,profiles may illustrate and offer insights into how risk can be managed at various stages of the ai lifecycle or in specific sector technology or end use applications,8,3
5903,5903,ai rmf profiles assist organizations in deciding how they might best manage ai risk that is well aligned with their goals considers legalregulatory requirements and best practices and reflects risk management priorities,8,3
5904,5904,ai rmf temporal profiles are descriptions of either the current state or the desired target state of specific ai risk management activities within a given sector industry organization or application context,8,3
5905,5905,an ai rmf current profile indicates how ai is currently being managed and the related risks in terms of current outcomes,8,3
5906,5906,a target profile indicates the outcomes needed to achieve the desired or target ai risk management goals,8,3
5907,5907,comparing current and target profiles likely reveals gaps to be addressed to meet ai risk management objectives,8,3
5908,5908,action plans can be developed to address these gaps to fulfill outcomes in a given category or subcategory,8,21
5909,5909,prioritization of gap mitigation is driven by the users needs and risk management processes,8,3
5910,5910,this risk based approach also enables framework users to compare their approaches with other approaches and to gauge the resources needed eg staffing funding to achieve ai risk management goals in a costeffective prioritized manner,8,3
5911,5911,ai rmf cross sectoral profiles cover risks of models or applications that can be used across use cases or sectors,8,3
5912,5912,cross sectoral profiles can also cover how to govern map measure and manage risks for activities or business processes common across sectors such as the use of large language models cloud based services or acquisition,8,3
5913,5913,this framework does not prescribe profile templates allowing for flexibility in implementation,8,26
5914,5914,ai design tasks are performed during the application context and data and input phases of the ai lifecycle,8,4
5915,5915,ai design actors create the concept and objectives of ai systems and are responsible for the planning design and data collection and processing tasks of the ai system so that the ai system is lawful and fit for purpose,8,4
5916,5916,tasks include articulating and documenting the systems concept and objectives underlying assumptions context and requirements gathering and cleaning data and documenting the metadata and characteristics of the dataset,8,24
5917,5917,ai actors in this category include data scientists domain experts socio cultural analysts experts in the field of diversity equity inclusion and accessibility members of impacted communities human factors experts eg uxui design governance experts data engineers data providers system funders product managers third party entities evaluators and legal and privacy governance,8,4
5918,5918,ai development tasks are performed during the ai model phase of the lifecycle,8,20
5919,5919,ai development actors provide the initial infrastructure of ai systems and are responsible for model building and interpretation tasks which involve the creation selection calibration training andor testing of models or algorithms,8,20
5920,5920,ai actors in this category include machine learning experts data scientists developers third party entities legal and privacy governance experts and experts in the socio cultural and contextual factors associated with the deployment setting,8,4
5921,5921,ai deployment tasks are performed during the task and output phase of the lifecycle,8,20
5922,5922,ai deployment actors are responsible for contextual decisions relating to how the ai system is used to assure deployment of the system into production,8,20
5923,5923,related tasks include piloting the system checking compatibility with legacy systems ensuring regulatory compliance managing organizational change and evaluating user experience,8,15
5924,5924,ai actors in this category include system integrators software developers end users operators and practitioners evaluators and domain experts with expertise in human factors socio cultural analysis and governance,8,4
5925,5925,operation and monitoring tasks are performed in the application contextoperate and monitor phase of the lifecycle,8,15
5926,5926,these tasks are carried out by ai actors who are responsible for operating the ai system and working with others to regularly assess system output and impacts,8,4
5927,5927,ai actors in this category include system operators domain experts ai designers users who interpret or incorporate the output of ai systems product developers evaluators and auditors compliance experts organizational management and members of the research community,8,4
5928,5928,test evaluation verification and validation tevv tasks are performed throughout the ai lifecycle,8,18
5929,5929,they are carried out by ai actors who examine the ai system or its components or detect and remediate problems,8,4
5930,5930,ideally ai actors carrying out verification and validation tasks are distinct from those who perform test and evaluation actions,8,18
5931,5931,tasks can be incorporated into a phase as early as design where tests are planned in accordance with the design requirement,8,19
5932,5932,tevv tasks for design planning and data may center on internal and external validation of assumptions for system design data collection and measurements relative to the intended context of deployment or application,8,20
5933,5933,tevv tasks for development ie model building include model validation and assessment,8,20
5934,5934,tevv tasks for deployment include system validation and integration in production with testing and recalibration for systems and process integration user experience and compliance with existing legal regulatory and ethical specifications,8,20
5935,5935,tevv tasks for operations involve ongoing monitoring for periodic updates testing and subject matter expert sme recalibration of models the tracking of incidents or errors reported and their management the detection of emergent properties and related impacts and processes for redress and response,8,24
5936,5936,human factors tasks and activities are found throughout the dimensions of the ai lifecycle,8,4
5937,5937,they include human centered design practices and methodologies promoting the active involvement of end users and other interested parties and relevant ai actors incorporating context specific norms and values in system design evaluating and adapting end user experiences and broad integration of humans and human dynamics in all phases of the ai lifecycle,8,4
5938,5938,human factors professionals provide multidisciplinary skills and perspectives to understand context of use inform interdisciplinary and demographic diversity engage in consultative processes design and evaluate user experience perform human centered evaluation and testing and inform impact assessments,8,27
5939,5939,domain expert tasks involve input from multidisciplinary practitioners or scholars who provide knowledge or expertise in  and about  an industry sector economic sector context or application area where an ai system is being used,8,4
5940,5940,ai actors who are domain experts can provide essential guidance for ai system design and development and interpret outputs in support of work performed by tevv and ai impact assessment teams,8,4
5941,5941,ai impact assessment tasks include assessing and evaluating requirements for ai system accountability combating harmful bias examining impacts of ai systems product safety liability and security among others,8,17
5942,5942,ai actors such as impact assessors and evaluators provide technical human factor socio cultural and legal expertise,8,4
5943,5943,procurement tasks are conducted by ai actors with financial legal or policy management authority for acquisition of ai models products or services from a third party developer vendor or contractor,8,20
5944,5944,governance and oversight tasks are assumed by ai actors with management fiduciary and legal authority and responsibility for the organization in which an ai system is designed developed andor deployed,8,16
5945,5945,key ai actors responsible for ai governance include organizational management senior leadership and the board of directors,8,16
5946,5946,these actors are parties that are concerned with the impact and sustainability of the organization as a whole,8,16
5947,5947,third party entities include providers developers vendors and evaluators of data algorithms models andor systems and related services for another organization or the organizations customers or clients,8,1
5948,5948,third party entities are responsible for ai design and development tasks in whole or in part,8,4
5949,5949,by definition they are external to the design development or deployment team of the organization that acquires its technologies or services,8,20
5950,5950,the technologies acquired from third party entities may be complex or opaque and risk tolerances may not align with the deploying or operating organization,8,0
5951,5951,end users of an ai system are the individuals or groups that use the system for specific purposes,8,4
5952,5952,these individuals or groups interact with an ai system in a specific context,8,4
5953,5953,end users can range in competency from ai experts to first time technology end users,8,27
5954,5954,affected individualscommunities encompass all individuals groups communities or organizations directly or indirectly affected by ai systems or decisions based on the output of ai systems,8,17
5955,5955,these individuals do not necessarily interact with the deployed system or application,8,25
5956,5956,other ai actors may provide formal or quasi formal norms or guidance for specifying and managing ai risks,8,3
5957,5957,they can include trade associations standards developing organizations advocacy groups researchers environmental groups and civil society organizations,8,24
5958,5958,the general public is most likely to directly experience positive and negative impacts of ai technologies,8,17
5959,5959,they may provide the motivation for actions taken by the ai actors,8,4
5960,5960,this group can include individuals communities and consumers associated with the context in which an ai system is developed or deployed,8,4
5961,5961,as with traditional software risks from ai based technology can be bigger than an enterprise span organizations and lead to societal impacts,8,3
5962,5962,ai systems also bring a set of risks that are not comprehensively addressed by current risk frameworks and approaches,8,3
5963,5963,some ai system features that present risks also can be beneficial,8,3
5964,5964,for example pre trained models and transfer learning can advance research and increase accuracy and resilience when compared to other models and approaches,8,13
5965,5965,identifying contextual factors in the map function will assist ai actors in determining the level of risk and potential management efforts,8,3
5966,5966,compared to traditional software ai specific risks that are new or increased include the following,8,3
5967,5967,the data used for building an ai system may not be a true or appropriate representation of the context or intended use of the ai system and the ground truth may either not exist or not be available,8,26
5968,5968,additionally harmful bias and other data quality issues can affect ai system trustworthiness which could lead to negative impacts,8,6
5969,5969,ai system dependency and reliance on data for training tasks combined with increased volume and complexity typically associated with such data,8,4
5970,5970,intentional or unintentional changes during training may fundamentally alter ai system performance,8,22
5971,5971,datasets used to train ai systems may become detached from their original and intended context or may become stale or outdated relative to deployment context,8,20
5972,5972,ai system scale and complexity many systems contain billions or even trillions of decision points housed within more traditional software applications,8,4
5973,5973,use of pre trained models that can advance research and improve performance can also increase levels of statistical uncertainty and cause issues with bias management scientific validity and reproducibility,8,13
5974,5974,higher degree of difficulty in predicting failure modes for emergent properties of large scale pre trained models,8,20
5975,5975,privacy risk due to enhanced data aggregation capability for ai systems,8,11
5976,5976,ai systems may require more frequent maintenance and triggers for conducting cor rective maintenance due to data model or concept drift,8,4
5977,5977,increased opacity and concerns about reproducibility,8,14
5978,5978,underdeveloped software testing standards and inability to document ai based prac tices to the standard expected of traditionally engineered software for all but thesimplest of cases,8,15
5979,5979,difficulty in performing regular ai based software testing or determining what totest since ai systems are not subject to the same controls as traditional code development,8,25
5980,5980,computational costs for developing ai systems and their impact on the environment and planet,8,4
5981,5981,inability to predict or detect the side effects of ai based systems beyond statistical measures,8,4
5982,5982,privacy and cybersecurity risk management considerations and approaches are applicable in the design development deployment evaluation and use of ai systems,8,20
5983,5983,privacy and cybersecurity risks are also considered as part of broader enterprise risk management considerations which may incorporate ai risks,8,3
5984,5984,as part of the effort to address ai trustworthiness characteristics such as secure and resilient and privacy enhanced organizations may consider leveraging available standards and guidance that provide broad guidance to organizations to reduce security and privacy risks such as but not limited to the nist cybersecurity framework the nist privacy framework the nist risk management framework and the secure software development framework,8,11
5985,5985,these frameworks have some features in common with the ai rmf,8,26
5986,5986,like most risk management approaches they are outcome based rather than prescriptive and are often structured around a core set of functions categories and subcategories,8,3
5987,5987,while there are significant differences between these frameworks based on the domain addressed  and because ai risk management calls for addressing many other types of risks  frameworks like those mentioned above may inform security and privacy considerations in the map measure and manage functions of the ai rmf,8,3
5988,5988,at the same time guidance available before publication of this ai rmf does not comprehensively address many ai system risks,8,25
5989,5989,for example existing frameworks and guidance are unable to adequately manage the problem of harmful bias in ai systems,8,14
5990,5990,confront the challenging risks related to generative ai,8,3
5991,5991,comprehensivelyaddresssecurityconcernsrelatedtoevasionmodelextractionmem bership inference availability or other machine learning attacks,8,11
5992,5992,accountforthecomplexattacksurfaceofaisystemsorothersecurityabusesenabledby ai systems,8,0
5993,5993,consider risks associated with third party ai technologies transfer learning and off label use where ai systems may be trained for decision making outside an organizations security controls or trained in one domain and then fine tuned for another,8,3
5994,5994,both ai and traditional software technologies and systems are subject to rapid innovation,8,4
5995,5995,technology advances should be monitored and deployed to take advantage of those developments and work towards a future of ai that is both trustworthy and responsible,8,18
5996,5996,organizations that design develop or deploy ai systems for use in operational settings may enhance their ai risk management by understanding current limitations of humanai interaction,8,3
5997,5997,the ai rmf provides opportunities to clearly define and differentiate the various human roles and responsibilities when using interacting with or managing ai systems,8,4
5998,5998,many of the data driven approaches that ai systems rely on attempt to convert or represent individual and social observational and decision making practices into measurable quantities,8,4
5999,5999,representing complex human phenomena with mathematical models can come at the cost of removing necessary context,8,14
6000,6000,this loss of context may in turn make it difficult to understand individual and societal impacts that are key to ai risk management efforts,8,3
6001,6001,human roles and responsibilities in decision making and overseeing ai systems need to be clearly defined and differentiated,8,4
6002,6002,human ai configurations can span from fully autonomous to fully manual,8,4
6003,6003,ai systems can autonomously make decisions defer decision making to a human expert or be used by a human decision maker as an additional opinion,8,21
6004,6004,some ai systems may not require human oversight such as models used to improve video compression,8,4
6005,6005,other systems may specifically require human oversight,8,15
6006,6006,decisions that go into the design development deployment evaluation and use of ai systems reflect systemic and human cognitive biases,8,20
6007,6007,ai actors bring their cognitive biases both individual and group into the process,8,6
6008,6008,biases can stem from end user decision making tasks and be introduced across the ai lifecycle via human assumptions expectations and decisions during design and modeling tasks,8,6
6009,6009,these biases which are not necessarily always harmful may be exacerbated by ai system opacity and the resulting lack of transparency,8,14
6010,6010,systemic biases at the organizational level can influence how teams are structured and who controls the decision making processes throughout the ai lifecycle,8,3
6011,6011,these biases can also influence downstream decisions by end users decision makers and policy makers and may lead to negative impacts,8,6
6012,6012,human ai interaction results vary,8,4
6013,6013,under certain conditions  for example in perceptual based judgment tasks  the ai part of the human ai interaction can amplify human biases leading to more biased decisions than the ai or human alone,8,4
6014,6014,when these variations are judiciously taken into account in organizing human ai teams however they can result in complementarity and improved overall performance,8,4
6015,6015,presenting ai system information to humans is complex,8,14
6016,6016,humans perceive and derive meaning from ai system output and explanations in different ways reflecting different individual preferences traits and skills,8,14
6017,6017,the govern function provides organizations with the opportunity to clarify and define the roles and responsibilities for the humans in the human ai team configurations and those who are overseeing the ai system performance,8,16
6018,6018,the govern function also creates mechanisms for organizations to make their decision making processes more explicit to help counter systemic biases,8,26
6019,6019,the map function suggests opportunities to define and document processes for operator and practitioner proficiency with ai system performance and trustworthiness concepts and to define relevant technical standards and certifications,8,18
6020,6020,implementing map function categories and subcategories may help organizations improve their internal competency for analyzing context identifying procedural and system limitations exploring and examining impacts of ai based systems in the real world and evaluating decision making processes throughout the ai lifecycle,8,4
6021,6021,the govern and map functions describe the importance of interdisciplinarity and demographically diverse teams and utilizing feedback from potentially impacted individuals and communities,8,17
6022,6022,ai actors called out in the ai rmf who perform human factors tasks and activities can assist technical teams by anchoring in design and development practices to user intentions and representatives of the broader ai community and societal values,8,4
6023,6023,these actors further help to incorporate context specific norms and values in system design and evaluate end user experiences  in conjunction with ai systems,8,4
6024,6024,ai risk management approaches for human ai configurations will be augmented by ongoing research and evaluation,8,3
6025,6025,for example the degree to which humans are empowered and incentivized to challenge ai system output requires further studies,8,4
6026,6026,data about the frequency and rationale with which humans overrule ai system output in deployed systems may be useful to collect and analyze,8,4
6027,6027,nist described several key attributes of the ai rmf when work on the framework first began,8,4
6028,6028,these attributes have remained intact and were used to guide the ai rmfs development,8,4
6029,6029,they are provided here as a reference,8,24
6030,6030,the ai rmf strives to be risk based resource efficient pro innovation and voluntary,8,3
6031,6031,be consensus driven and developed and regularly updated through an open transparent process,8,15
6032,6032,all stakeholders should have the opportunity to contribute to the airmfs development,8,27
6033,6033,use clear and plain language that is understandable by a broad audience includingsenior executives government officials non governmental organization leadership and those who are not ai professionals  while still of sufficient technical depth to be useful to practitioners,8,14
6034,6034,the ai rmf should allow for communication of ai risks across an organization between organizations with customers and to the public at large,8,3
6035,6035,provide common language and understanding to manage ai risks,8,3
6036,6036,the ai rmf should offer taxonomy terminology definitions metrics and characterizations for ai risk,8,3
6037,6037,be easily usable and fit well with other aspects of risk management,8,3
6038,6038,use of the framework should be intuitive and readily adaptable as part of an organizations broader risk management strategy and processes,8,3
6039,6039,it should be consistent or aligned with other approaches to managing ai risks,8,3
6040,6040,be useful to a wide range of perspectives sectors and technology domains,8,0
6041,6041,the ai rmf should be universally applicable to any ai technology and to context specific use cases,8,4
6042,6042,be outcome focused and non prescriptive,8,21
6043,6043,the framework should provide a catalog of outcomes and approaches rather than prescribe one size fits all requirements,8,27
6044,6044,takeadvantageofandfostergreaterawarenessofexistingstandardsguidelinesbest practices methodologies and tools for managing ai risks  as well as illustrate the need for additional improved resources,8,3
6045,6045,be lawand regulation agnostic,8,8
6046,6046,the framework should support organizations abilities to operate under applicable domestic and international legal or regulatory regimes,8,26
6047,6047,be a living document,8,22
6048,6048,the ai rmf should be readily updated as technology understanding and approaches to ai trustworthiness and uses of ai change and as stakeholders learn from implementing ai risk management generally and this framework in particular,8,3
6049,6049,ai has the potential to make a substantial impact for individuals communities and society,9,4
6050,6050,to make sure the impact of your ai project is positive and does not unintentionally harm those affected by it you and your team should make considerations of ai ethics and safety a high priority,9,16
6051,6051,this section introduces ai ethics and provides a high level overview of the ethical building blocks needed for the responsible delivery of an ai project,9,16
6052,6052,the following guidance is designed to complement and supplement the data ethics framework,9,16
6053,6053,the data ethics framework is a tool that should be used in any project,9,16
6054,6054,this guidance is for everyone involved in the design production and deployment of an ai project such as data scientists data engineers domain experts delivery managers departmental leads,9,27
6055,6055,ethical considerations will arise at every stage of your ai project,9,16
6056,6056,you should use the expertise and active cooperation of all your team members to address them,9,1
6057,6057,ai ethics is a set of values principles and techniques that employ widely accepted standards to guide moral conduct in the development and use of ai systems,9,16
6058,6058,the field of ai ethics emerged from the need to address the individual and societal harms ai systems might cause,9,16
6059,6059,these harms rarely arise as a result of a deliberate choice most ai developers do not want to build biased or discriminatory applications or applications which invade users privacy,9,25
6060,6060,the main ways ai systems can cause involuntary harm aremisuse systems are used for purposes other than those for which they were designed and intended,9,4
6061,6061,questionable design creators have not thoroughly considered technical issues related to algorithmic bias and safety risks,9,25
6062,6062,unintended negative consequences creators have not thoroughly considered the potential negative impacts their systems may have on the individuals and communities they affect,9,17
6063,6063,the field of ai ethics mitigates these harms by providing project teams with the values principles and techniques needed to produce ethical fair and safe ai applications,9,16
6064,6064,an ai model which filters out spam emails for example will present fewer ethical challenges than one which identifies vulnerable children,9,0
6065,6065,you and your team should formulate governance procedures and protocols for each project using ai following a careful evaluation of social and ethical impacts,9,16
6066,6066,establish ethical building blocks for your ai project,9,16
6067,6067,you should establish ethical building blocks for the responsible delivery of your ai project,9,16
6068,6068,this involves building a culture of responsible innovation as well as a governance architecture to bring the values and principles of ethical fair and safe ai to life,9,16
6069,6069,building a culture of responsible innovation,9,16
6070,6070,to build and maintain a culture of responsibility you and your team should prioritise  goals as you design develop and deploy your ai project,9,16
6071,6071,in particular you should make sure your ai project is ethically permissible consider the impacts it may have on the wellbeing of affected stakeholders and communities,9,16
6072,6072,fair and non discriminatory consider its potential to have discriminatory effects on individuals and social groups mitigate biases which may influence your models outcome and be aware of fairness issues throughout the design and implementation lifecycle,9,6
6073,6073,worthy of public trust guarantee as much as possible the safety accuracy reliability security and robustness of its product,9,18
6074,6074,justifiable prioritise the transparency of how you design and implement your model and the justification and interpretability of its decisions and behaviours,9,20
6075,6075,prioritising these goals will help build a culture of responsible innovation,9,16
6076,6076,to make sure they are fully incorporated into your project you should establish a governance architecture consisting of a framework of ethical values,9,16
6077,6077,set of actionable principles,9,26
6078,6078,process based governance framework,9,26
6079,6079,you should understand the framework of ethical values which support underwrite and motivate the responsible design and use of ai,9,16
6080,6080,the alan turing institute calls these the sum values,9,26
6081,6081,respect the dignity of individuals,9,16
6082,6082,connect with each other sincerely openly and inclusively,9,27
6083,6083,care for the wellbeing of all,9,16
6084,6084,protect the priorities of social values justice and public interest,9,16
6085,6085,these values provide you with an accessible framework to enable you and your team members to explore and discuss the ethical aspects of ai,9,16
6086,6086,establish well defined criteria which allow you and your team to evaluate the ethical permissibility of your ai project,9,16
6087,6087,while the sum values can help you consider the ethical permissibility of your ai project they are not specifically catered to the particularities of designing developing and implementing an ai system,9,16
6088,6088,ai systems increasingly perform tasks previously done by humans,9,4
6089,6089,for example ai systems can screen cvs as part of a recruitment process,9,4
6090,6090,however unlike human recruiters you cannot hold an ai system directly responsible or accountable for denying applicants a job,9,9
6091,6091,this lack of accountability of the ai system itself creates a need for a set of actionable principles tailored to the design and use of ai systems,9,16
6092,6092,the alan turing institute calls these the fast track principles,9,26
6093,6093,fairnessaccountabilitysustainabilitytransparencyensure your project is fair and prevent bias or discrimination,9,6
6094,6094,safeguard public trust in your projects capacity to deliver safe and reliable ai,9,16
6095,6095,fairness,9,6
6096,6096,if your ai system processes social or demographic data you should design it to meet a minimum level of discriminatory non harm,9,6
6097,6097,to do this you shoulduse only fair and equitable datasets data fairness,9,6
6098,6098,include reasonable features processes and analytical structures in your model architecture design fairness,9,20
6099,6099,prevent the system from having any discriminatory impact outcome fairness,9,6
6100,6100,implement the system in an unbiased way implementation fairness,9,6
6101,6101,accountability,9,16
6102,6102,you should design your ai system to be fully answerable and auditable,9,0
6103,6103,to do this you shouldestablish a continuous chain of responsibility for all roles involved in the design and implementation lifecycle of the project,9,16
6104,6104,implement activity monitoring to allow for oversight and review throughout the entire project,9,25
6105,6105,sustainability,9,16
6106,6106,the technical sustainability of these systems ultimately depends on their safety including their accuracy reliability security and robustness,9,15
6107,6107,you should make sure designers and users remain aware ofthe transformative effects ai systems can have on individuals and society,9,4
6108,6108,your ai systems real world impact,9,4
6109,6109,transparency,9,14
6110,6110,designers and implementers of ai systems should be able toexplain to affected stakeholders how and why a model performed the way it did in a specific context,9,20
6111,6111,justify the ethical permissibility the discriminatory non harm and the public trustworthiness of its outcome and of the processes behind its design and use,9,16
6112,6112,build a process based governance framework,9,26
6113,6113,the final method to make sure you use ai ethically fairly and safely is building a process based governance framework,9,16
6114,6114,the alan turing institute calls it a pbg framework,9,26
6115,6115,its primary purpose is to integrate the sum values and the fast track principles across the implementation of ai within a service,9,4
6116,6116,building a good pbg framework for your ai project will provide your team with an overview of the relevant team members and roles involved in each governance action,9,16
6117,6117,the relevant stages of the workflow in which intervention and targeted consideration are necessary to meet governance goals,9,17
6118,6118,explicit timeframes for any evaluations follow up actions re assessments and continuous monitoring,9,18
6119,6119,clear and well defined protocols for logging activity and for implementing mechanisms to support end to end auditability,9,11
6120,6120,ai has the potential to change the way we live and work,10,4
6121,6121,embedding ai across all sectors has the potential to create thousands of jobs and drive economic growth,10,0
6122,6122,by one estimate ais contribution to the united kingdom could be as large as  of gdp by,10,2
6123,6123,a number of public sector organisations are already successfully using ai for tasks ranging from fraud detection to answering customer queries,10,0
6124,6124,the potential uses for ai in the public sector are significant but have to be balanced with ethical fairness and safety considerations,10,16
6125,6125,organisation leads who want to understand the best ways to use ai,10,4
6126,6126,delivery leads who want to evaluate if ai can meet user needs,10,21
6127,6127,recognising ais potential the governments industrial strategy white paper placed artificial intelligence and data as one of  grand challenges supported by the m ai sector deal,10,2
6128,6128,the government has set up  new bodies to support the use of ai build the right infrastructure and facilitate public and private sector adoption of these technologies,10,2
6129,6129,these  new bodies are the ai council which will be an expert committee providing high level leadership on implementing the ai sector deal,10,19
6130,6130,office for ai which works with industry academia and the third sector to coordinate and oversee the implementation of the uks ai strategy,10,2
6131,6131,centre for data ethics and innovation which identifies the measures needed to make sure the development of ai is safe ethical and innovative,10,16
6132,6132,the government has also set up  funds to support the development and uptake of ai the govtech catalyst to help public sector bodies take advantage of emerging technologiesregulators,10,2
6133,6133,pioneer fund to help regulators promote cutting edge regulatory practices when developing emerging technologies,10,8
6134,6134,at its core ai is a research field spanning philosophy logic statistics computer science mathematics neuroscience linguistics cognitive psychology and economics,10,4
6135,6135,ai can be defined as the use of digital technology to create systems capable of performing tasks commonly thought to require intelligence,10,4
6136,6136,ai is constantly evolving but generally it involves machines using statistics to find patterns in large amounts of data,10,4
6137,6137,is the ability to perform repetitive tasks with data without the need for constant human guidance,10,9
6138,6138,this guidance mostly discusses machine learning,10,13
6139,6139,machine learning is a subset of ai and refers to the development of digital systems that improve their performance on a given task over time through experience,10,13
6140,6140,machine learning is the most widely used form of ai and has contributed to innovations like self driving cars speech recognition and machine translation,10,13
6141,6141,recent advances in machine learning are the result of improvements to algorithms,10,13
6142,6142,increases in funding,10,17
6143,6143,huge growth in the amount of data created and stored by digital systems,10,24
6144,6144,increased access to computational power and the expansion of cloud computing,10,22
6145,6145,machine learning can be supervised learning which allows an ai model to learn from labelled training data for example training an ai model to help tag content on gov uk,10,13
6146,6146,unsupervised learning which is training an ai algorithm to use unlabelled and unclassified information,10,13
6147,6147,reinforcement learning which allows an ai model to learn as it performs a task,10,13
6148,6148,ai can benefit the public sector in a number of ways,10,17
6149,6149,for example it can provide more accurate information forecasts and predictions leading to better outcomes for example more accurate medical diagnoses,10,13
6150,6150,produce a positive social impact by using ai to provide solutions for some of the worlds most challenging social problems,10,4
6151,6151,simulate complex systems to experiment with different policy options and spot unintended consequences before committing to a measure,10,21
6152,6152,improve public services for example personalising public services to adapt to individual circumstances,10,27
6153,6153,automate simple manual tasks which frees staff up to do more interesting work,10,22
6154,6154,ai is not a general purpose solution which can solve every problem,10,4
6155,6155,current applications of ai focus on performing narrowly defined tasks,10,4
6156,6156,ai generally cannot be imaginative,10,4
6157,6157,perform well without a large quantity of relevant high quality data,10,24
6158,6158,infer additional context if the information is not present in the data,10,9
6159,6159,even if ai can help you meet some user needs simpler solutions may be more effective and less expensive,10,4
6160,6160,for example optical character recognition technology can extract information from scans of passports,10,12
6161,6161,however a digital form requiring manual input might be more accurate quicker to build and cheaper,10,25
6162,6162,youll need to investigate alternative mature technology solutions thoroughly to check if this is the case,10,25
6163,6163,with an ai project you should consider a number of factors including ai ethics and safety,10,16
6164,6164,these factors span safety ethical legal and administrative concerns and include data quality the success of your ai project depends on the quality of your data,10,3
6165,6165,fairness are the models trained and tested on relevant accurate and generalisable datasets and is the ai system deployed by users trained to implement them responsibly and without bias,10,6
6166,6166,accountability consider who is responsible for each element of the models output and how the designers and implementers of ai systems will be held accountable,10,16
6167,6167,privacy complying with appropriate data policies for example the general data protection regulations gdpr and the data protection act explainability and transparency so the affected stakeholders can know how the ai model reached its decision,10,11
6168,6168,costs consider how much it will cost to build run and maintain an ai infrastructure train and educate staff and if the work to install ai may outweigh any potential savings,10,4
6169,6169,youll need to make sure your ai system is compliant with general data protection regulation gdpr and the data protection act  dpa  including the points which relate to automated decision making,10,21
6170,6170,we recommend discussing this with legal advisors,10,10
6171,6171,automated decisions in this context are decisions made without human intervention which have legal or similarly significant effects on data subjects,10,21
6172,6172,for example an online decision to award a loan or a recruitment aptitude test which uses pre programmed algorithms,10,21
6173,6173,if you want to use automated processes to make decisions with legal or similarly significant effects on individuals you must follow the safeguards laid out in the gdpr and dpa,10,21
6174,6174,this includes making sure you provide users with specific and easily accessible information about the automated decision making process,10,21
6175,6175,a simple way to obtain human intervention to review and potentially change the decision,10,21
6176,6176,remember to make sure your use of automated decision making does not conflict with any other laws or regulations,10,21
6177,6177,you should consider both the final decision and any automated decisions which significantly affected the decision making process,10,21
6178,6178,ethical considerations in the use of machine learning for research and statistics,11,13
6179,6179,machine learning algorithms work by learning from training data and applying that learning to new unseen data,11,13
6180,6180,by using machine learning analysts are able to identify trends and patterns in very large datasets,11,13
6181,6181,the information collected from machine learning can be descriptive it uses data to explain a phenomenon,11,13
6182,6182,predictive it predicts what will happen based on trends and patterns from the data given,11,13
6183,6183,prescriptive it can make decision making suggestions,11,21
6184,6184,this means that machine learning methods have an incredibly wide range of application,11,13
6185,6185,supervised machine learning in this instance algorithms are fed labelled data data which is annotated so that the machine knows its target for training,11,13
6186,6186,supervised learning leads to a prediction or classification of a known quantity ie an outcome variable using patterns that the machine finds in the data to predict an outcome,11,13
6187,6187,for example if a data scientist wanted to teach a system to identify cats in different images they would feed the system with images of cats labelled as cats,11,13
6188,6188,unsupervised machine learningunlike supervised machine learning in which the machine is fed input variables and an output variable unsupervised machine learning only uses input data,11,13
6189,6189,this means that the model learns without supervision to discover its own patterns and information from the data,11,13
6190,6190,this type of machine learning assists us in finding unknown patterns in data,11,13
6191,6191,the data fed into the model is typically unlabelled,11,13
6192,6192,using the same example as in the supervised machine learning in this instance a data scientist would feed the system with images of cats but they would not be labelled and it would be the responsibility of the system to analyse the data and predict which images are of cats,11,13
6193,6193,reinforcement machine learning trains models to make decision sequences by utilising a process of trial and error,11,13
6194,6194,the programmer will reward the machine when it does what the programmer wants and penalise it when it does not though the programmer will not give the models help in making these decisions,11,20
6195,6195,the model then will try to maximise its reward causing it to change its decisions strategise,11,20
6196,6196,examples of machine learning projects in research and statistics include using satellite data to estimate land coverage and land usage,11,13
6197,6197,understanding perceptions of public emergencies using social media data to consider impacts and possible mitigations,11,17
6198,6198,using text data to classify businesses according to their economic activity,11,24
6199,6199,the use of machine learning data in research and statistics provides substantial potential benefits,11,13
6200,6200,particularly beneficial is the ability to analyse large data sets and extract information quickly once a model is deployed,11,0
6201,6201,the automation of tasks may be less resource intensive and the ability for models to autonomously adapt to improve the quality and validity of outcomes is often invaluable to data scientists researchers and analysts,11,15
6202,6202,as such machine learning is now used in multiple types of research,11,13
6203,6203,when analysts embark on a research project using any method it is always important that any possible ethical issues relating to the collection access use and storage of data are considered,11,24
6204,6204,this helps reduce potential harm to all individuals involved in the research and helps maintain public acceptability around the production of research and statistics and enables researchers to efficiently access and harness data that supports the production of statistics for the public good,11,17
6205,6205,these ethical issues are particularly important when using more contemporary methods such as machine learning as it poses not only traditional data ethics considerations such as transparency and privacy concerns but also new ones,11,16
6206,6206,when algorithms are used it may be more challenging to guard against mistakes or bias which may result from human interaction with the model for example coding design decisions or data input and that could affect the systems outputs,11,15
6207,6207,moreover research findings may be biased or erroneous should models be used outside of their intended purpose or if machine outputs are not thoroughly reviewed and checked for validity before use,11,18
6208,6208,of course despite these ethical considerations there are huge benefits to using machine learning methods,11,13
6209,6209,taking a considered approach to ethics in every project helps to maintain public trust in the use of data for research and statistics more generally enabling researchers to harness the power of data to support public good research,11,16
6210,6210,no matter what stage your machine learning project may be at it is always sensible to discuss possible ethical issues that could arise with other researchers,11,16
6211,6211,this applies if you are thinking about starting a new project using machine learning in the process of designing your study or even if you have started to create or deploy a machine learning system,11,13
6212,6212,of course it is always beneficial to start thinking about ethical challenges at the earliest possible stage of the research,11,16
6213,6213,by doing this you are implementing good data ethics by design,11,16
6214,6214,the uk statistics authority provides researchers with an ethics self assessment tool which is used to empower researchers to identify and review any ethical challenges apparent in a research project,11,16
6215,6215,this guidance supplements the ethics self assessment tool and also provides a high level checklist that you can use to ensure that any research or statistical project that uses machine learning techniques is ethically responsible,11,16
6216,6216,to help analysts navigate potential ethical issues the uk statistics authority has developed six ethical principles to consider throughout the life cycle of a research project,11,16
6217,6217,these principles focus on ensuring the public good of research and statistics maintaining confidentiality of data understanding the potential risks and limitations in new research methods and technologies compliance with legal requirements considering public acceptability of the project and transparency in the collection use and sharing of data,11,11
6218,6218,this guidance is underpinned by these general principles but focuses specifically on ethical considerations relating to machine learning which require us to take particular care,11,13
6219,6219,these include the importance of minimising and mitigating social bias and subsequent discrimination within machine learning research and clearly communicating these biases and the limitations of our research,11,6
6220,6220,the need to consider the transparency and explainability of machine learning research and the implications this has for reproducibility,11,14
6221,6221,the importance of maintaining accountability within all aspects of machine learning processes ensuring that models are used only for their intended purposes and that different stakeholders are aware of their responsibilities,11,16
6222,6222,the need to consider the confidentiality and privacy risks arising from the data used both in relation to training data which is fed into the machine and outputs resulting from the machine learnings findings,11,13
6223,6223,also worth reflection here is the office for statistical regulations code of practice for statistics,11,24
6224,6224,the osr code of practice sets the standards that producers of official statistics should commit to,11,18
6225,6225,specifically the code of practice is framed around three main pillars,11,27
6226,6226,these are value quality and trustworthiness,11,18
6227,6227,these support and map clearly onto the uk statistics authoritys ethical principles and are important statistical standards,11,18
6228,6228,we can benefit greatly from considering these principles in relation to machine learning and the ethical issues addressed below,11,13
6229,6229,there are several ways in which bias can be reflected within machine learning which could influence how algorithms are created used and interpreted by analysts,11,6
6230,6230,again whilst there may be some overlapping relevance to operational or other uses of machine learning this guidance focuses on the implications of different ethical challenges for research and statistics,11,13
6231,6231,in many circumstances bias in machine learning is a result of cognitive human error  problems are introduced by those who design or train the systems,11,6
6232,6232,for example the training data may be incomplete or unrepresentative of the correct population,11,13
6233,6233,bias though does not always exist as a result of human error for example it may be introduced into data sets or models as a consequence of previous societal norms,11,6
6234,6234,the potential biases associated with machine learning are not unique to machine learning methods for example sample bias is a typical ethical and quality consideration within traditional statistical methods and mitigations used to limit bias in traditional statistical methods can certainly help when thinking about machine learning,11,13
6235,6235,that being said issues of bias in machine learning may be presented in more complex ways particularly when using large datasets and can often be more prominent for machine learning application,11,6
6236,6236,detecting bias within machine learning studies can be very difficult and understanding and limiting the effects of bias once it is identified can be a complex process,11,6
6237,6237,there are a number of different biases which can be found within machine learning projects,11,6
6238,6238,four of the most common are outlined below,11,24
6239,6239,sample biasthis occurs when the data used to train a model is not representative of the correct population,11,6
6240,6240,algorithmic biasthis is the bias that occurs when the algorithm being used is faulty for example there may be technical issues with the algorithm stopping it from working or not being used in the way it was intended and therefore not appropriate for the new application,11,6
6241,6241,prejudicial biasthis refers to a situation in which the model reflects existing prejudices within the training data,11,6
6242,6242,a model will apply the same stereotyping that exists in society if it is fed prejudicial data and this will influence its results,11,6
6243,6243,similar to prejudicial bias is observer bias which relates to the conscious and unconscious prejudices or judgements which a researcher may bring to their data,11,6
6244,6244,this may affect both the representativeness of the training data and therefore the results from the system but also the interpretation of results or recommendations,11,6
6245,6245,exclusion biasif a data point or set of features such as location geography etc,11,6
6246,6246,is left out of the training data exclusion bias may occur,11,6
6247,6247,this normally happens when a researcher takes a data point out of the training data thinking it is not consequential but this can lead to inaccuracy in the system,11,13
6248,6248,a common solution to make sure training data doesnt perpetuate bias from sensitive attributes such as gender ethnicity socio economic status etc,11,6
6249,6249,is to remove these features from the training data,11,13
6250,6250,however this may not eradicate the bias completely and can in some instances worsen the problem as correlated attributes still existent in the data may still reflect the bias you are trying to erase,11,6
6251,6251,for example a name might be correlated to gender or a postcode may be correlated to socio economic status,11,6
6252,6252,if you do decide to discard a sensitive attribute or indeed any part of your data you should document this thoroughly  investigating and analysing the features relevance to the data before deletion and the relationship between this feature and your labels,11,11
6253,6253,it may help to ask a colleague for a second opinion,11,22
6254,6254,the data science campus present methods for cleaning faulty data to improve reliability and accuracy in their research on estimating vehicle and pedestrian activity from town and city traffic cameras which provides an interesting example of machine learning being used for statistical purposes,11,13
6255,6255,take time to reflect on possible biases that may find their way into the data you are using,11,6
6256,6256,any potential for bias no matter how big or small and the impacts that may arise from this should be clearly communicated with all stakeholders and documented throughout the project,11,17
6257,6257,reflexivity on the part of the researcher is key,11,16
6258,6258,it may be useful to discuss your project with colleagues who are not involved in the project as part of an independent review,11,25
6259,6259,ensure that training datasets are as representative of the correct population as possible,11,6
6260,6260,this may help counteract sample and prejudicial bias,11,6
6261,6261,you cannot expect your model to learn what a cat looks like if you only feed it images of dogs,11,13
6262,6262,it is important that all the cases that we might expect a model to be exposed to are examined and ensure that a well balanced data set is used,11,18
6263,6263,where possible the algorithms and data sets should be tested and validated to mitigate any possible biases and systems should be systematically monitored to ensure that biases do not occur as the systems continue to learn,11,6
6264,6264,machine learning models will sometimes perform differently in the real world than they did when being trained and so it is important that models are continuously modelled and audited so that any issues can be identified quickly,11,13
6265,6265,documenting the decisions that are made at all stages can be beneficial in determining where and how bias could have been introduced,11,6
6266,6266,one of the uk statistics authoritys ethical principles   transparency  relates to the obligation of researchers to ensure that the decisions they make about their data analysis and methods are openly and honestly documented and communicated in a way that allows others to evaluate them,11,14
6267,6267,explainability then can be used to define machine learning techniques that as humans we are adequately able to understand trust and manage,11,14
6268,6268,though not synonymous issues of transparency and explainability are inextricably linked with one another and so for the purpose of this guidance the two challenges will be addressed together,11,14
6269,6269,these concepts relate to the potentially opaque nature of machine learning algorithms and the difficulty in communicating how these systems are used and how they work particularly to a lay audience,11,14
6270,6270,so called black box algorithms where the researcher knows what data is fed into the algorithm and what comes out but might not be able to interpret it in actionable human terms are central to considering this issue,11,13
6271,6271,there may be good reasons for using a black box approach  for example a black box algorithm may provide better quality or more accurate data than a more transparent alternative  but it is important to note however that there are explainable machine learning algorithms which should be considered to optimise transparency where it is appropriate to do so,11,13
6272,6272,this will depend on a number of factors including the intended use of the model the maintainability of the model and its outputs and whether using the more transparent alternative risks sacrificing the quality or accuracy of another more opaque model,11,20
6273,6273,of course there are far greater complexities when it comes to algorithmic explainability than allow us to make binary distinctions between black box algorithms and explainable algorithms and there a number of model agnostic methods which may help researchers in some instances and with certain caveats interpret machine learning models,11,14
6274,6274,explainability then is not black and white and should be seen more as a sliding scale,11,14
6275,6275,a lack of transparency in machine learning projects has further implications for the reproducibility of results,11,14
6276,6276,the ability to replicate results is crucial in ensuring and maintaining scientific trust,11,18
6277,6277,being transparent about the way in which data is used and how our research is carried out allows others to verify the results of a project via replication and the accuracy or inaccuracy of a model enabling researchers to determine whether the model is sufficient and whether it is suitable for further use,11,14
6278,6278,transparency is particularly important to consider when using technologies such as machine learning especially with the aim of reproducibility in mind,11,14
6279,6279,for instance the researchers deploying systems may not be those who made that system to begin with which makes validating the quality of the system and accurately predicting the decision making rationale of the person who made the system much harder,11,15
6280,6280,regular and systematic documentation of all coding processes and decisions made are therefore the building blocks of transparency,11,14
6281,6281,there are many different stakeholders that need to be considered when thinking about transparency and communicating your research from members of the public to internal and external analysts,11,14
6282,6282,in this guidance we refer to all those interacting with machine learning as stakeholders however it is important for researchers to consider who these different stakeholders are and tailor communication with each group for best impact,11,13
6283,6283,if in doubt assume that the group that you are interacting with has no experience of machine learning  it is better to simplify your language to ensure that stakeholders understand your message,11,14
6284,6284,it is also worth considering how machine learning projects can benefit us and their limitations,11,13
6285,6285,for example whilst machine learning algorithms can help us make predictions most are not causal,11,13
6286,6286,this means that conclusions can rarely be drawn that are beyond correlation therefore it is often not possible to make claims of cause and effect,11,14
6287,6287,it is important for all stakeholders to understand that they are interacting with machine learning and the conclusions and recommendations that come from a machine learning model,11,13
6288,6288,if stakeholders from participants to analysts are not given clear information to help them understand the research then this risks promoting harmful practices,11,14
6289,6289,explainability and transparency are important considerations for researchers to build public confidence and promote ethical practice,11,14
6290,6290,everyone interacting with the project should be able and encouraged to ask questions at any point within the research process and this should be communicated in a user interface or similar document,11,24
6291,6291,these questions should be answered in a transparent and timely manner,11,14
6292,6292,if the choice is taken that an opaque machine learning algorithm is to be used over a more explainable one researchers may need to consider how this can be communicated to a non specialist audience,11,14
6293,6293,in these cases consideration should be given as to how information can be communicated transparently and in ways that are easy to understand to relevant non expert communities as necessary,11,14
6294,6294,researchers using machine learning techniques should communicate in plain language why the opaque algorithm was chosen and what its advantages are that make it the best choice despite the lack of explainability as well as what training data has been used any bias or limitations of the data and whether the training data has been validated through other studies,11,14
6295,6295,considerations of the quality of the training data may also provide a useful framework for communicating some of these issues for which existing guidance may be helpful,11,27
6296,6296,researchers might also find it useful to produce a clear statement of why their conclusions are believed to be valid what is meant by being valid for their particular use case and any limitations on the validity,11,14
6297,6297,all human decision making processes should be audited and easily reviewable,11,21
6298,6298,it may be useful to systematically review each decision making process at different stages of the research clearly documenting the rationale and impact of these decisions,11,21
6299,6299,creating an audit trail of human decision making throughout a machine learning process supports analysts in explaining to non expert audiences how and why machine learning has been used and any potential ethical implications of this use,11,13
6300,6300,it may be useful to discuss your project with independent colleagues as part of challenge sessions both to maximise collective understanding and to provide an additional opportunity to check the model is doing what is expected,11,20
6301,6301,an explanation of the outputs and any associated recommendations arising from the project should be easily accessible and as understandable as possible supporting non expert understanding of the impact of the research findings,11,14
6302,6302,this should be delivered in a timely manner,11,22
6303,6303,it is always helpful to put yourself in the shoes of other stakeholders when you design any project and this can be particularly helpful when thinking about how to communicate complex ideas such as machine learning algorithms and systems to non expert audiences,11,16
6304,6304,the exercise below may help analysts think about these concepts and communicate with users effectively,11,14
6305,6305,communicating with non experts when using machine learning techniques to ensure transparency,11,14
6306,6306,what information might your audience find helpful,11,14
6307,6307,take a minute to consider what you would like to know about a project if you were approached by someone who wanted to tell you about their machine learning research,11,2
6308,6308,consider this from the perspective of the audience you are trying to communicate with,11,14
6309,6309,what information might your audience not find helpful,11,14
6310,6310,take a minute to consider what information you may not find helpful if you were approached by someone who wanted to tell you about their machine learning research from the perspective of the audience you are trying to communicate with,11,14
6311,6311,helpfulwhat is machine learning,11,13
6312,6312,why did you choose to use machine learning over other methods,11,13
6313,6313,what is the aim of the research,11,0
6314,6314,why is the study important and what will you do with the results,11,14
6315,6315,were there any limitations to your research or the machine learning methods that you used,11,13
6316,6316,how did you access the data and how will it be used,11,24
6317,6317,why did you choose to use this dataset,11,13
6318,6318,will the outcome of the research affect groups or individuals either positively or negatively,11,6
6319,6319,is my personal information safe,11,9
6320,6320,how is my data being stored and can it be reused for other purposes,11,24
6321,6321,will anyone be identifiable from the data or the research outcomes,11,9
6322,6322,not helpfulwhat will the typical lay person learn from being presented with an algorithm,11,13
6323,6323,are they likely to understand it or is there a more understandable way of presenting this to a lay audience,11,14
6324,6324,too much information,11,22
6325,6325,it may put users off if there is too much information or if the information presented is hard to read,11,14
6326,6326,how can you present the information in a way that is concise and easy to read,11,14
6327,6327,helpfulwhat is the key message in regard to policy,11,22
6328,6328,what was the key policy challenge that the research seeks to address,11,17
6329,6329,why is this research important for policy,11,17
6330,6330,what are its main implications,11,0
6331,6331,are the policy implications of the research short term or long term,11,22
6332,6332,what are the key findings from the research,11,0
6333,6333,how have you arrived at these findings,11,14
6334,6334,what methods were used,11,24
6335,6335,what will happen with the data collected and how will it be used going forward,11,9
6336,6336,are there any limitationsassumptions to the research,11,25
6337,6337,what are the key policy recommendations,11,22
6338,6338,it may also be beneficial to consider the views of key stakeholders particularly in relation to the public,11,0
6339,6339,for example if a policy decision was made on the basis of your project would the public be comfortable with this,11,21
6340,6340,not helpfulmuch like the public audience policy and decision makers are unlikely to gain from being presented with the inner workings of your algorithm or the granular specifics of your methods,11,21
6341,6341,instead a brief high level summary of how you have come to your findings may be more useful,11,14
6342,6342,policy and decision makers are likely to be limited on time and so it is critical to keep communication with them succinct,11,22
6343,6343,how can you present the information in a way that is concise and easy to read or visualise,11,14
6344,6344,how can you present your research findings in a way that is relevant to policy and which clearly highlights its aims its importance its limitations and its implications,11,14
6345,6345,helpfulwhat training data was used to teach the system,11,13
6346,6346,how did you obtain and quality assure the data that you used to teach the system,11,24
6347,6347,how did you train the system which methods did you use throughout the process,11,13
6348,6348,were there any limitations or biases in the training data that may have affected the results,11,6
6349,6349,how have these been mitigated,11,25
6350,6350,what patterns or recommendations have emerged from the data,11,24
6351,6351,how did the machine learning model come to this conclusion,11,13
6352,6352,what are the assumptions related to this recommendation,11,21
6353,6353,how much better is this approach than others that could have been used,11,14
6354,6354,are there any improvements that could be made to the current approach,11,25
6355,6355,how was the model evaluatedcompared against other models,11,20
6356,6356,are there plans in place to continue the work with updated data,11,22
6357,6357,have you shared your code for others to use and adapt,11,25
6358,6358,not helpfulthis group may have a more technical understanding of machine learning or statistical processes and so more detailed information regarding how the system was trained and how it reached the results it did could be useful,11,13
6359,6359,think about what you have been asked to do and why and what the benefits of this are,11,21
6360,6360,you can then tailor your communication with this in mind,11,14
6361,6361,these questions have been designed as a starting point for conversations with different key groups and should provide a good starting point when thinking about how to communicate with different audiences,11,24
6362,6362,publishing your algorithms for people to see is really useful and goes some way to ensuring transparency,11,7
6363,6363,however many people may not be able to understand or interpret an algorithm,11,14
6364,6364,making your algorithm accessible is not enough to make your research transparent,11,25
6365,6365,perhaps you could consider linking your audiences to the published algorithm should they want to see it but consider the questions in the left hand column to better explain what it means to your audience,11,2
6366,6366,you will need to communicate your research to different audiences and they will likely have varying levels of understanding,11,24
6367,6367,it is important to tailor your communications with each group to ensure transparency however by considering the questions above in relation to a non expert user you may find it easier to communicate this information to all stakeholders,11,14
6368,6368,by placing ourselves in the shoes of a person with no knowledge of machine learning or by practicing explaining our work to people with no knowledge of it not only are we able to better communicate with this group of users but we can alsobegin to think more transparently about our work,11,14
6369,6369,better communicate with a lay audience,11,14
6370,6370,ensure explainability,11,14
6371,6371,better understand our own research,11,14
6372,6372,improve the impact of our work on different audiences,11,14
6373,6373,though there are many complex moral and legal issues surrounding accountability every person who is involved in the creation of a machine learning system has a degree of responsibility for considering the systems ramifications good or bad and researchers will need to be aware of their organisations own different approaches and standards,11,16
6374,6374,it is not the responsibility of the machine or system to be considerate of these ideals,11,26
6375,6375,this is the responsibility of those who establish these systems and they should be accepting of scrutiny and appropriate accountability,11,16
6376,6376,accountability is the willingness to accept responsibility for a process and therefore be answerable to any query or issue that may arise as a result,11,16
6377,6377,organisations should agree on appropriate mechanisms to ensure accountability in line with accepted standards,11,16
6378,6378,the office for statistical regulation have produced guidance on good practice in designing developing and using models which may be useful in helping to think about these mechanismsensuring the appropriate use of machine learning and machine learning models,11,13
6379,6379,when designing a research project it is imperative that the benefits and challenges of different methods are weighed up and that the researchers are able to justify the methods that they have chosen to use,11,0
6380,6380,this should be clearly documented,11,14
6381,6381,in the case of machine learning it is also the collective responsibility of those who develop and train models and those who deploy pre existing models to ensure that the use case of a model is clear and that models are not used beyond their intended use,11,16
6382,6382,training a machine learning model can take anywhere between hours and months depending on the problem you are trying to solve and the data being used,11,13
6383,6383,using a pre existing model with the same intended use as a basis for your project may save time and money,11,20
6384,6384,in addition training machine learning models also has an environmental impact which could be lessened by using pre existing models,11,13
6385,6385,however in order to ensure transparency and accountability it is important to consider how the model being used was developed and for what purpose,11,16
6386,6386,utilising models beyond their intended use without care and caution may impact upon the accuracy and validity of your results,11,20
6387,6387,using pre trained models may be particularly problematic if the model lacks transparency as it may be harder to identify the processes used to train the model and existing biases,11,20
6388,6388,whilst developers cannot stop others from using their models to remain accountable anyone making and training models should try to be explicit in communicating the intended use of a model so that models are not created that are then used by others in the wrong way,11,20
6389,6389,if you are considering using a pre trained model it is important to understand what the model was built to be used for and ensure before it is implemented that it is suitable for your own project,11,20
6390,6390,if you play any role no matter how big or how small in the design or development of a machine learning project it is your responsibility to understand what your role is and the policies relevant to your role,11,16
6391,6391,you should be accepting of scrutiny  this could improve your research help you with your decision making process and ensure a level of public trust,11,18
6392,6392,ensure accountability by design by having governance processes to ensure human oversight at the appropriate organisational level throughout design and implementation,11,16
6393,6393,it is vital that a model is audited at regular intervals and that this is well documented,11,18
6394,6394,sufficient time should be built into the project plan to allow for this,11,27
6395,6395,this will enable assurance that the machine learning technique fulfils the intended purpose without any unwanted consequences,11,13
6396,6396,regular audit allows scope drift and risk to be identified mitigated against and documented as required by the relevant stakeholders,11,18
6397,6397,a continuous chain of human responsibility should be established through the full lifecycle of the machine learning model,11,13
6398,6398,all human involvement and oversight should be transparent documenting all roles and actions taken to ensure human responsibility can be identified,11,16
6399,6399,machine learning  particularly when using predictive analytics  often involves linking data from multiple sources,11,13
6400,6400,new data sources such as social media data and biometric data are increasingly being used by analysts to identify social trends and using these sources raises ethical questions surrounding confidentiality,11,9
6401,6401,put simply confidentiality refers to the measures taken to ensure that data and data subjects are protected from being identified via the separation or modification of personal information provided by participants from the data,11,11
6402,6402,there are many ways in which we can protect confidentiality including techniques such as the anonymisation of datasets de identification and data management and security processes such as limiting access to data to specific agreed purposes,11,11
6403,6403,the techniques used will depend on several factors including the type of data being used and for what purpose,11,24
6404,6404,in choosing which methods may be most appropriate for your project it may be helpful for researchers to consider the  safes framework which has been adopted by the  office for national statistics,11,24
6405,6405,this helps researchers maximise the use of their data whilst ensuring that the data is kept secure at all times,11,11
6406,6406,potential mitigationsonce the dataset has been explored and the data necessary for the research has been determined additional data should be deleted,11,9
6407,6407,however this should be done with care and only after careful consideration as deletion of some data may make the model worse or less transparent for the user,11,20
6408,6408,if anonymising data this should be done at the earliest opportunity and consideration should be taken to ensure that the most appropriate methods of anonymisation are used eg natural language processingbased anonymisation differential privacy data masking aggregations,11,11
6409,6409,ensuring the exclusion of unnecessary data will help prevent the system from identifying unhelpful correlations,11,9
6410,6410,consideration should be given to the level of control data subjects should have over their own data balancing the risks to individual privacy against the statistical value of the dataset,11,9
6411,6411,data subject rights can be specified in materials that are publicly available to data subjects such as privacy notices or participant information and organisations should have appropriate mechanisms to delete or withdraw data as required,11,9
6412,6412,has the project been considered in relation to the uk statistics authoritys general ethical principles,11,16
6413,6413,public good have the benefits of using machine learning for a project been clearly documented,11,13
6414,6414,for further guidance on communicating public good see our public good guidance,11,14
6415,6415,methods and quality is machine learning the most suitable method to use,11,13
6416,6416,have the limitations of machine learning datatechnologies been considered,11,13
6417,6417,transparency has transparency in the collection use retention and sharing of the data being used and produced been considered,11,11
6418,6418,legal compliance has relevant regulation been considered in relation to the dataset used both in the uk and if necessary overseas,11,8
6419,6419,public views and engagement have potential public views regarding particular uses of machine learning data across different contexts been considered,11,2
6420,6420,confidentiality and data security have appropriate mechanisms to maintain confidentiality of datasets been considered,11,11
6421,6421,have you considered the potential for bias which could arise from any of your data,11,6
6422,6422,have you scrutinised your training data for potential biases and considered the potential for your own conscious or sub conscious biases to be reflected in the data,11,6
6423,6423,if potential bias is identified ensure that this is documented to enable informed interpretation of results,11,6
6424,6424,what are the potential implications of this bias and how can this be minimised,11,6
6425,6425,who are the key stakeholders who need to be considered when communicating your project and what are likely to be their main questions and concerns,11,27
6426,6426,assuming that you had no knowledge of the project what would you like to know about how your data is being used to provide outputs that inform decision makers,11,24
6427,6427,are you able to explain what data is being used to train the algorithm and what you expect to get from the data afterwards,11,13
6428,6428,have you enabled an open and transparent system to allow stakeholders to ask questions throughout the research process,11,25
6429,6429,would another researcher be able to reproduce your results with the information available to them,11,9
6430,6430,has human accountability been built into the project from the design phase,11,16
6431,6431,are there structures in place to enable accountability,11,16
6432,6432,has a chain of human responsibility been established with each stage of the projects lifecycle being documented to show the human oversight,11,16
6433,6433,has time been put aside throughout the lifecycle to account for an audit of the machine learning model,11,13
6434,6434,if you have created a model yourself has the intended use of the model been clearly communicated to ensure that it is not misused,11,20
6435,6435,if you are using a pre existing model have you ensured that you are using it in the way it was intended,11,20
6436,6436,has data minimisation been appropriately considered,11,11
6437,6437,only the data that is required should be stored and used and any unnecessary data should be deleted once it has been determined that it is appropriate to do so,11,11
6438,6438,have you considered whether it is appropriate to anonymise your data and if so what the most appropriate methods of anonymisation will be,11,9
6439,6439,have you ensured that your data is being safely stored,11,22
6440,6440,might your data system or results be re used outside of their original context and purpose in the future to the disadvantage of individuals groups or communities,11,9
6441,6441,what can you do to try and protect against this possibility,11,25
6442,6442,the blueprint for an ai bill of rights is a set of five principles and associated practices to help guide the design use and deployment of automated systems to protect the rights of the american public in the age of artificial intelligence,12,2
6443,6443,developed through extensive consultation with the american public these principles are a blueprint for building and deploying automated systems that are aligned with democratic values and protect civil rights civil liberties and privacy,12,15
6444,6444,the blueprint for an ai bill of rights includes a foreword the five principles notes on applying the blueprint for an ai bill of rights and from principles to practice that gives concrete steps that can be taken by many kinds of organizationsfrom governments at all levels to companies of all sizesto uphold these values,12,2
6445,6445,experts from across the private sector governments and international consortia have published principles and frameworks to guide the responsible use of automated systems this framework provides a national values statement and toolkit that is sector agnostic to inform building these protections into policy practice or the technological design process,12,15
6446,6446,where existing law or policysuch as sector specific privacy laws and oversight requirementsdo not already provide guidance the blueprint for an ai bill of rights should be used to inform policy decisions,12,2
6447,6447,listening to the american publicthe white house office of science and technology policy has led a year long process to seek and distill input from people across the countryfrom impacted communities and industry stakeholders to technology developers and other experts across fields and sectors as well as policymakers throughout the federal governmenton the issue of algorithmic and data driven harms and potential remedies,12,2
6448,6448,through panel discussions public listening sessions meetings a formal request for information and input to a publicly accessible and widely publicized email address people throughout the united states public servants across federal agencies and members of the international community spoke up about both the promises and potential harms of these technologies and played a central role in shaping the blueprint for an ai bill of rights,12,2
6449,6449,the core messages gleaned from these discussions include that ai has transformative potential to improve americans lives and that preventing the harms of these technologies is both necessary and achievable,12,4
6450,6450,the appendix includes a full list of public engagements,12,17
6451,6451,the blueprint for an ai bill of rights is an exercise in envisioning a future where the american public is protected from the potential harms and can fully enjoy the benefits of automated systems,12,2
6452,6452,it describes principles that can help ensure these protections,12,11
6453,6453,some of these protections are already required by the us constitution or implemented under existing us laws,12,11
6454,6454,for example government surveillance and data search and seizure are subject to legal requirements and judicial oversight,12,8
6455,6455,there are constitutional requirements for human review of criminal investigative matters and statutory requirements for judicial review,12,12
6456,6456,civil rights laws protect the american people against discrimination,12,6
6457,6457,there are regulatory safety requirements for medical devices as well as sector  population  or technology specific privacy and security protections,12,1
6458,6458,ensuring some of the additional protections proposed in this framework would require new laws to be enacted or new policies and practices to be adopted,12,17
6459,6459,in some cases exceptions to the principles described in the blueprint for an ai bill of rights may be necessary to comply with existing law conform to the practicalities of a specific use case or balance competing public interests,12,2
6460,6460,in particular law enforcement and other regulatory contexts may require government actors to protect civil rights civil liberties and privacy in a manner consistent with but using alternate mechanisms to the specific principles discussed in this framework,12,12
6461,6461,the blueprint for an ai bill of rights is meant to assist governments and the private sector in moving principles into practice,12,2
6462,6462,the expectations given in the technical companion are meant to serve as a blueprint for the development of additional technical standards and practices that should be tailored for particular sectors and contexts,12,27
6463,6463,while existing laws informed the development of the blueprint for an ai bill of rights this framework does not detail those laws beyond providing them as examples where appropriate of existing protective measures,12,2
6464,6464,this framework instead shares a broad forward leaning vision of recommended principles for automated system development and use to inform private and public involvement with these systems where they have the potential to meaningfully impact rights opportunities or access,12,15
6465,6465,additionally this framework does not analyze or take a position on legislative and regulatory proposals in municipal state and federal government or those in other countries,12,26
6466,6466,we have seen modest progress in recent years with some state and local governments responding to these problems with legislation and some courts extending longstanding statutory protections to new and emerging technologies,12,22
6467,6467,there are companies working to incorporate additional protections in their design and use of automated systems and researchers developing innovative guardrails,12,15
6468,6468,advocates researchers and government organizations have proposed principles for the ethical use of ai and other automated systems,12,16
6469,6469,these include the organization for economic co operation and developments oecds  recommendation on artificial intelligence which includes principles for responsible stewardship of trustworthy ai and which the united states adopted and executive order  on promoting the use of trustworthy artificial intelligence in the federal government which sets out principles that govern the federal governments use of ai,12,2
6470,6470,the blueprint for an ai bill of rights is fully consistent with these principles and with the direction in executive order  on advancing racial equity and support for underserved communities through the federal government,12,2
6471,6471,these principles find kinship in the fair information practice principles fipps derived from the  report of an advisory committee to the us department of health education and welfare records computers and the rights of citizens,12,26
6472,6472,i while there is no single universal articulation of the fipps these core principles for managing information about individuals have been incorporated into data privacy laws and policies across the globe,12,11
6473,6473,ii the blueprint for an ai bill of rights embraces elements of the fipps that are particularly relevant to automated systems without articulating a specific set of fipps or scoping applicability or the interests served to a single particular domain like privacy civil rights and civil liberties ethics or risk management,12,2
6474,6474,the technical companion builds on this prior work to provide practical next steps to move these principles into practice and promote common approaches that allow technological innovation to flourish while protecting people from harm,12,27
6475,6475,algorithmic discrimination algorithmic discrimination occurs when automated systems contribute to unjustified different treatment or impacts disfavoring people based on their race color ethnicity sex including pregnancy childbirth and related medical conditions gender identity intersex status and sexual orientation religion age national origin disability veteran status genetic information or any other classification protected by law,12,6
6476,6476,depending on the specific circumstances such algorithmic discrimination may violate legal protections,12,6
6477,6477,throughout this framework the term algorithmic discrimination takes this meaning and not a technical understanding of discrimination as distinguishing between items,12,6
6478,6478,automated system an automated system is any system software or process that uses computation as whole or part of a system to determine outcomes make or aid decisions inform policy implementation collect data or observations or otherwise interact with individuals andor communities,12,15
6479,6479,automated systems include but are not limited to systems derived from machine learning statistics or other data processing or artificial intelligence techniques and exclude passive computing infrastructure,12,15
6480,6480,passive computing infrastructure is any intermediary technology that does not influence or determine the outcome of decision make or aid in decisions inform policy implementation or collect data or observations including web hosting domain registration networking caching data storage or cybersecurity,12,26
6481,6481,throughout this framework automated systems that are considered in scope are only those that have the potential to meaningfully impact individuals or communities rights opportunities or access,12,15
6482,6482,communities communities include neighborhoods social network connections both online and offline families construed broadly people connected by affinity identity or shared traits and formal organizational ties,12,7
6483,6483,this includes tribes clans bands rancherias villages and other indigenous communities,12,24
6484,6484,ai and other data driven automated systems most directly collect data on make inferences about and may cause harm to individuals,12,13
6485,6485,but the overall magnitude of their impacts may be most readily visible at the level of communities,12,17
6486,6486,accordingly the concept of community is integral to the scope of the blueprint for an ai bill of rights,12,2
6487,6487,united states law and policy have long employed approaches for protecting the rights of individuals but existing frameworks have sometimes struggled to provide protections when effects manifest most clearly at a community level,12,12
6488,6488,for these reasons the blueprint for an ai bill of rights asserts that the harms of automated systems should be evaluated protected against and redressed at both the individual and community levels,12,15
6489,6489,equity equity means the consistent and systematic fair just and impartial treatment of all individuals,12,6
6490,6490,systemic fair and just treatment must take into account the status of individuals who belong to underserved communities that have been denied such treatment such as black latino and indigenous and native american persons asian americans and pacific islanders and other persons of color members of religious minorities women girls and non binary people lesbian gay bisexual transgender queer and intersex lgbtqi persons older adults persons with disabilities persons who live in rural areas and persons otherwise adversely affected by persistent poverty or inequality,12,6
6491,6491,rights opportunities or access rights opportunities or access is used to indicate the scoping of this framework,12,26
6492,6492,it describes the set of civil rights civil liberties and privacy including freedom of speech voting and protections from discrimination excessive punishment unlawful surveillance and violations of privacy and other freedoms in both public and private sector contexts equal opportunities including equitable access to education housing credit employment and other programs or access to critical resources or services such as healthcare financial services safety social services non deceptive information about goods and services and government benefits,12,12
6493,6493,sensitive data data and metadata are sensitive if they pertain to an individual in a sensitive domain defined below are generated by technologies used in a sensitive domain can be used to infer data from a sensitive domain or sensitive data about an individual such as disability related data genomic data biometric data behavioral data geolocation data data related to interaction with the criminal justice system relationship history and legal status such as custody and divorce information and home work or school environmental data or have the reasonable potential to be used in ways that are likely to expose individuals to meaningful harm such as a loss of privacy or financial harm due to identity theft,12,11
6494,6494,data and metadata generated by or about those who are not yet legal adults is also sensitive even if not related to a sensitive domain,12,12
6495,6495,such data includes but is not limited to numerical text image audio or video data,12,24
6496,6496,sensitive domains sensitive domains are those in which activities being conducted can cause material harms including significant adverse effects on human rights such as autonomy and dignity as well as civil liberties and civil rights,12,3
6497,6497,domains that have historically been singled out as deserving of enhanced data protections or where such enhanced protections are reasonably expected by the public include but are not limited to health family planning and care employment education criminal justice and personal finance,12,11
6498,6498,in the context of this framework such domains are considered sensitive whether or not the specifics of a system context would necessitate coverage under existing law and domains and data that are considered sensitive are understood to change over time based on societal norms and context,12,26
6499,6499,surveillance technology surveillance technology refers to products or services marketed for or that can be lawfully used to detect monitor intercept collect exploit preserve protect transmit andor retain data identifying information or communications concerning individuals or groups,12,12
6500,6500,this framework limits its focus to both government and commercial use of surveillance technologies when juxtaposed with real time or subsequent automated analysis and when such systems have a potential for meaningful impact on individuals or communities rights opportunities or access,12,12
6501,6501,underserved communities the term underserved communities refers to communities that have been systematically denied a full opportunity to participate in aspects of economic social and civic life as exemplified by the list in the preceding definition of equity the below examples are meant to illustrate the breadth of automated systems that insofar as they have the potential to meaningfully impact rights opportunities or access to critical resources or services should be covered by the blueprint for an ai bill of rights,12,26
6502,6502,these examples should not be construed to limit that scope which includes automated systems that may not yet exist but which fall under these criteria,12,15
6503,6503,examples of automated systems for which the blueprint for an ai bill of rights should be considered include those that have the potential to meaningfully impactcivil rights civil liberties or privacy including but not limited tospeech related systems such as automated content moderation toolssurveillance and criminal justice system algorithms such as risk assessments predictive policing automated license plate readers real time facial recognition systems especially those used in public places or during protected activities like peaceful protests social media monitoring and ankle monitoring devicesvoting related systems such as signature matching toolssystems with a potential privacy impact such as smart home systems and associated data systems that use or collect health related data systems that use or collect education related data criminal justice system data ad targeting systems and systems that perform big data analytics in order to build profiles or infer personal information about individuals andany system that has the meaningful potential to lead to algorithmic discrimination,12,15
6504,6504,equal opportunities including but not limited toeducation related systems such as algorithms that purport to detect student cheating or plagiarism admissions algorithms online or virtual reality student monitoring systems projections of student progress or outcomes algorithms that determine access to resources or programs and surveillance of classes whether online or in personhousing related systems such as tenant screening algorithms automated valuation systems that estimate the value of homes used in mortgage underwriting or home insurance and automated valuations from online aggregator websites andemployment related systems such as workplace algorithms that inform all aspects of the terms and conditions of employment including but not limited to pay or promotion hiring or termination algorithms virtual or augmented reality workplace training programs and electronic workplace surveillance and management systems,12,26
6505,6505,access to critical resources and services including but not limited tohealth and health insurance technologies such as medical ai systems and devices ai assisted diagnostic tools algorithms or predictive models used to support clinical decision making medical or insurance health risk assessments drug addiction risk assessments and associated access algorithms wearable technologies wellness apps insurance care allocation algorithms and health insurance cost and underwriting algorithmsfinancial system algorithms such as loan allocation algorithms financial system access determination algorithms credit scoring systems insurance algorithms including risk assessments automated interest rate determinations and financial algorithms that apply penalties eg that can garnish wages or withhold tax returnssystems that impact the safety of communities such as automated traffic control systems electrical grid controls smart city technologies and industrial emissions and environmental impact control algorithms andsystems related to access to benefits or services or assignment of penalties such as systems that support decision makers who adjudicate benefits such as collating or analyzing information or matching records systems which similarly assist in the adjudication of administrative or criminal penalties fraud detection algorithms services or benefits access control algorithms biometric systems used as access control and systems which make benefits or services related decisions on a fully or partially autonomous basis such as a determination to revoke benefits,12,17
6506,6506,blueprint for an ai bill of rights making automated systems work for the american people is a white paper published by the white house office of science and technology policy,12,2
6507,6507,it is intended to support the development of policies and practices that protect civil rights and promote democratic values in the building deployment and governance of automated systems,12,15
6508,6508,the blueprint for an ai bill of rights is non binding and does not constitute us government policy,12,2
6509,6509,it does not supersede modify or direct an interpretation of any existing statute regulation policy or international instrument,12,1
6510,6510,it does not constitute binding guidance for the public or federal agencies and therefore does not require compliance with the principles described herein,12,1
6511,6511,it also is not determinative of what the us governments position will be in any international negotiation,12,26
6512,6512,adoption of these principles may not meet the requirements of existing statutes regulations policies or international instruments or the requirements of the federal agencies that enforce them,12,1
6513,6513,these principles are not intended to and do not prohibit or limit any lawful activity of a government agency including law enforcement national security or intelligence activities,12,1
6514,6514,the appropriate application of the principles set forth in this white paper depends significantly on the context in which automated systems are being utilized,12,15
6515,6515,in some circumstances application of these principles in whole or in part may not be appropriate given the intended use of automated systems to achieve government agency missions,12,26
6516,6516,future sector specific guidance will likely be necessary and important for guiding the use of automated systems in certain settings such as ai systems used as part of school building security or automated health diagnostic systems,12,15
6517,6517,the blueprint for an ai bill of rights recognizes that law enforcement activities require a balancing of equities for example between the protection of sensitive law enforcement information and the principle of notice as such notice may not be appropriate or may need to be adjusted to protect sources methods and other law enforcement equities,12,12
6518,6518,even in contexts where these principles may not apply in whole or in part federal departments and agencies remain subject to judicial privacy and civil liberties oversight as well as existing policies and safeguards that govern automated systems including for example executive order  promoting the use of trustworthy artificial intelligence in the federal government december,12,11
6519,6519,this white paper recognizes that national security which includes certain law enforcement and homeland security activities and defense activities are of increased sensitivity and interest to our nations adversaries and are often subject to special requirements such as those governing classified information and other protected data,12,12
6520,6520,such activities require alternative compatible safeguards through existing policies that govern automated systems and ai such as the department of defense dod ai ethical principles and responsible ai implementation pathway and the intelligence community ic ai ethics principles and framework,12,16
6521,6521,the implementation of these policies to national security and defense activities can be informed by the blueprint for an ai bill of rights where feasible,12,2
6522,6522,the blueprint for an ai bill of rights is not intended to and does not create any legal right benefit or defense substantive or procedural enforceable at law or in equity by any party against the united states its departments agencies or entities its officers employees or agents or any other person nor does it constitute a waiver of sovereign immunity,12,2
6523,6523,among the great challenges posed to democracy today is the use of technology data and automated systems in ways that threaten the rights of the american public,12,15
6524,6524,too often these tools are used to limit our opportunities and prevent our access to critical resources or services,12,15
6525,6525,these problems are well documented,12,14
6526,6526,in america and around the world systems supposed to help with patient care have proven unsafe ineffective or biased,12,15
6527,6527,algorithms used in hiring and credit decisions have been found to reflect and reproduce existing unwanted inequities or embed new harmful bias and discrimination,12,6
6528,6528,unchecked social media data collection has been used to threaten peoples opportunities undermine their privacy or pervasively track their activityoften without their knowledge or consent,12,9
6529,6529,these outcomes are deeply harmfulbut they are not inevitable,12,3
6530,6530,automated systems have brought about extraordinary benefits from technology that helps farmers grow food more efficiently and computers that predict storm paths to algorithms that can identify diseases in patients,12,13
6531,6531,these tools now drive important decisions across sectors while data is helping to revolutionize global industries,12,0
6532,6532,fueled by the power of american innovation these tools hold the potential to redefine every part of our society and make life better for everyone,12,0
6533,6533,this important progress must not come at the price of civil rights or democratic values foundational american principles that president biden has affirmed as a cornerstone of his administration,12,22
6534,6534,on his first day in office the president ordered the full federal government to work to root out inequity embed fairness in decision making processes and affirmatively advance civil rights equal opportunity and racial justice in america,12,22
6535,6535,the president has spoken forcefully about the urgent challenges posed to democracy today and has regularly called on people of conscience to act to preserve civil rightsincluding the right to privacy which he has called the basis for so many more rights that we have come to take for granted that are ingrained in the fabric of this countryto advance president bidens vision the white house office of science and technology policy has identified five principles that should guide the design use and deployment of automated systems to protect the american public in the age of artificial intelligence,12,2
6536,6536,the blueprint for an ai bill of rights is a guide for a society that protects all people from these threatsand uses technologies in ways that reinforce our highest values,12,2
6537,6537,responding to the experiences of the american public and informed by insights from researchers technologists advocates journalists and policymakers this framework is accompanied by from principles to practicea handbook for anyone seeking to incorporate these protections into policy and practice including detailed steps toward actualizing these principles in the technological design process,12,15
6538,6538,these principles help provide guidance whenever automated systems can meaningfully impact the publics rights opportunities or access to critical needs,12,15
6539,6539,safe and effective systemsyou should be protected from unsafe or ineffective systems,12,5
6540,6540,automated systems should be developed with consultation from diverse communities stakeholders and domain experts to identify concerns risks and potential impacts of the system,12,15
6541,6541,systems should undergo pre deployment testing risk identification and mitigation and ongoing monitoring that demonstrate they are safe and effective based on their intended use mitigation of unsafe outcomes including those beyond the intended use and adherence to domain specific standards,12,15
6542,6542,outcomes of these protective measures should include the possibility of not deploying the system or removing a system from use,12,17
6543,6543,automated systems should not be designed with an intent or reasonably foreseeable possibility of endangering your safety or the safety of your community,12,15
6544,6544,they should be designed to proactively protect you from harms stemming from unintended yet foreseeable uses or impacts of automated systems,12,15
6545,6545,you should be protected from inappropriate or irrelevant data use in the design development and deployment of automated systems and from the compounded harm of its reuse,12,15
6546,6546,independent evaluation and reporting that confirms that the system is safe and effective including reporting of steps taken to mitigate potential harms should be performed and the results made public whenever possible,12,17
6547,6547,while technologies are being deployed to solve problems across a wide array of issues our reliance on technology can also lead to its use in situations where it has not yet been proven to workeither at all or within an acceptable range of error,12,15
6548,6548,in other cases technologies do not work as intended or as promised causing substantial and unjustified harm,12,25
6549,6549,automated systems sometimes rely on data from other systems including historical data allowing irrelevant information from past decisions to infect decision making in unrelated situations,12,15
6550,6550,in some cases technologies are purposefully designed to violate the safety of others such as technologies designed to facilitate stalking in other cases intended or unintended uses lead to unintended harms,12,12
6551,6551,many of the harms resulting from these technologies are preventable and actions are already being taken to protect the public,12,12
6552,6552,some companies have put in place safeguards that have prevented harm from occurring by ensuring that key development decisions are vetted by an ethics review others have identified and mitigated harms found through pre deployment testing and ongoing monitoring processes,12,3
6553,6553,governments at all levels have existing public consultation processes that may be applied when considering the use of new automated systems and existing product development and testing practices already protect the american public from many potential harms,12,15
6554,6554,still these kinds of practices are deployed too rarely and unevenly,12,25
6555,6555,expanded proactive protections could build on these existing practices increase confidence in the use of automated systems and protect the american public,12,15
6556,6556,innovators deserve clear rules of the road that allow new ideas to flourish and the american public deserves protections from unsafe outcomes,12,16
6557,6557,all can benefit from assurances that automated systems will be designed tested and consistently confirmed to work as intended and that they will be proactively protected from foreseeable unintended harmful outcomes,12,15
6558,6558,a proprietary model was developed to predict the likelihood of sepsis in hospitalized patients and was implemented at hundreds of hospitals around the country,12,20
6559,6559,an independent study showed that the model predictions underperformed relative to the designers claims while also causing alert fatigue by falsely alerting likelihood of sepsis,12,14
6560,6560,on social media black people who quote and criticize racist messages have had their own speech silenced when a platforms automated moderation system failed to distinguish this counter speech or other critique and journalism from the original hateful messages to which such speech responded,12,7
6561,6561,a device originally developed to help people track and find lost items has been used as a tool by stalkers to track victims locations in violation of their privacy and safety,12,12
6562,6562,the device manufacturer took steps after release to protect people from unwanted tracking by alerting people on their phones when a device is found to be moving with them over time and also by having the device make an occasional noise but not all phones are able to receive the notification and the devices remain a safety concern due to their misuse,12,12
6563,6563,an algorithm used to deploy police was found to repeatedly send police to neighborhoods they regularly visit even if those neighborhoods were not the ones with the highest crime rates,12,12
6564,6564,these incorrect crime predictions were the result of a feedback loop generated from the reuse of data from previous arrests and algorithm predictions,12,12
6565,6565,ai enabled nudification technology that creates images where people appear to be nudeincluding apps that enable non technical users to create or alter images of individuals without their consenthas proliferated at an alarming rate,12,12
6566,6566,such technology is becoming a common form of image based abuse that disproportionately impacts women,12,12
6567,6567,as these tools become more sophisticated they are producing altered images that are increasingly realistic and are difficult for both humans and ai to detect as inauthentic,12,12
6568,6568,regardless of authenticity the experience of harm to victims of non consensual intimate images can be devastatingly realaffecting their personal and professional lives and impacting their mental and physical health,12,12
6569,6569,a company installed ai powered cameras in its delivery vans in order to evaluate the road safety habits of its drivers but the system incorrectly penalized drivers when other cars cut them off or when other events beyond their control took place on the road,12,4
6570,6570,as a result drivers were incorrectly ineligible to receive a bonus,12,6
6571,6571,the expectations for automated systems are meant to serve as a blueprint for the development of additional technical standards and practices that are tailored for particular sectors and contexts,12,15
6572,6572,in order to ensure that an automated system is safe and effective it should include safeguards to protect the public from harm in a proactive and ongoing manner avoid use of data inappropriate for or irrelevant to the task at hand including reuse that could cause compounded harm and demonstrate the safety and effectiveness of the system,12,15
6573,6573,these expectations are explained below,12,22
6574,6574,protect the public from harm in a proactive and ongoing mannerconsultation,12,12
6575,6575,the public should be consulted in the design implementation deployment acquisition and maintenance phases of automated system development with emphasis on early stage consultation before a system is introduced or a large change implemented,12,15
6576,6576,this consultation should directly engage diverse impacted communities to consider concerns and risks that may be unique to those communities or disproportionately prevalent or severe for them,12,27
6577,6577,the extent of this engagement and the form of outreach to relevant stakeholders may differ depending on the specific automated system and development phase but should include subject matter sector specific and context specific experts as well as experts on potential impacts such as civil rights civil liberties and privacy experts,12,2
6578,6578,for private sector applications consultations before product launch may need to be confidential,12,2
6579,6579,government applications particularly law enforcement applications or applications that raise national security considerations may require confidential or limited engagement based on system sensitivities and preexisting oversight laws and structures,12,26
6580,6580,concerns raised in this consultation should be documented and the automated system developers were proposing to create use or deploy should be reconsidered based on this feedback,12,15
6581,6581,testing,12,18
6582,6582,systems should undergo extensive testing before deployment,12,15
6583,6583,this testing should follow domain specific best practices when available for ensuring the technology will work in its real world context,12,25
6584,6584,such testing should take into account both the specific technology used and the roles of any human operators or reviewers who impact system outcomes or effectiveness testing should include both automated systems testing and human led manual testing,12,15
6585,6585,testing conditions should mirror as closely as possible the conditions in which the system will be deployed and new testing may be required for each deployment to account for material differences in conditions from one deployment to another,12,20
6586,6586,following testing system performance should be compared with the in place potentially human driven status quo procedures with existing human performance considered as a performance baseline for the algorithm to meet pre deployment and as a lifecycle minimum performance standard,12,18
6587,6587,decision possibilities resulting from performance testing should include the possibility of not deploying the system,12,20
6588,6588,risk identification and mitigation,12,3
6589,6589,before deployment and in a proactive and ongoing manner potential risks of the automated system should be identified and mitigated,12,15
6590,6590,identified risks should focus on the potential for meaningful impact on peoples rights opportunities or access and include those to impacted communities that may not be direct users of the automated system risks resulting from purposeful misuse of the system and other concerns identified via the consultation process,12,15
6591,6591,assessment and where possible measurement of the impact of risks should be included and balanced such that high impact risks receive attention and mitigation proportionate with those impacts,12,17
6592,6592,automated systems with the intended purpose of violating the safety of others should not be developed or used systems with such safety violations as identified unintended consequences should not be used until the risk can be mitigated,12,15
6593,6593,ongoing risk mitigation may necessitate rollback or significant modification to a launched automated system,12,15
6594,6594,ongoing monitoring,12,22
6595,6595,automated systems should have ongoing monitoring procedures including recalibration procedures in place to ensure that their performance does not fall below an acceptable level over time based on changing real world conditions or deployment contexts post deployment modification or unexpected conditions,12,15
6596,6596,this ongoing monitoring should include continuous evaluation of performance metrics and harm assessments updates of any systems and retraining of any machine learning models as necessary as well as ensuring that fallback mechanisms are in place to allow reversion to a previously working system,12,18
6597,6597,monitoring should take into account the performance of both technical system components the algorithm as well as any hardware components data inputs etc,12,24
6598,6598,and human operators,12,4
6599,6599,it should include mechanisms for testing the actual accuracy of any predictions or recommendations generated by a system not just a human operators determination of their accuracy,12,18
6600,6600,ongoing monitoring procedures should include manual human led monitoring as a check in the event there are shortcomings in automated monitoring systems,12,15
6601,6601,these monitoring procedures should be in place for the lifespan of the deployed automated system,12,15
6602,6602,clear organizational oversight,12,16
6603,6603,entities responsible for the development or use of automated systems should lay out clear governance structures and procedures,12,15
6604,6604,this includes clearly stated governance procedures before deploying the system as well as responsibility of specific individuals or entities to oversee ongoing assessment and mitigation,12,16
6605,6605,organizational stakeholders including those with oversight of the business process or operation being automated as well as other organizational divisions that may be affected due to the use of the system should be involved in establishing governance procedures,12,15
6606,6606,responsibility should rest high enough in the organization that decisions about resources mitigation incident response and potential rollback can be made promptly with sufficient weight given to risk mitigation objectives against competing concerns,12,3
6607,6607,those holding this responsibility should be made aware of any use cases with the potential for meaningful impact on peoples rights opportunities or access as determined based on risk identification procedures,12,12
6608,6608,in some cases it may be appropriate for an independent ethics review to be conducted before deployment,12,25
6609,6609,avoid inappropriate low quality or irrelevant data use and the compounded harm of its reuserelevant and high quality data,12,9
6610,6610,data used as part of any automated systems creation evaluation or deployment should be relevant of high quality and tailored to the task at hand,12,15
6611,6611,relevancy should be established based on research backed demonstration of the causal influence of the data to the specific use case or justified more generally based on a reasonable expectation of usefulness in the domain andor for the system design or ongoing development,12,17
6612,6612,relevance of data should not be established solely by appealing to its historical connection to the outcome,12,9
6613,6613,high quality and tailored data should be representative of the task at hand and errors from data entry or other sources should be measured and limited,12,24
6614,6614,any data used as the target of a prediction process should receive particular attention to the quality and validity of the predicted outcome or label to ensure the goal of the automated system is appropriately identified and measured,12,13
6615,6615,additionally justification should be documented for each data attribute and source to explain why it is appropriate to use that data to inform the results of the automated system and why such use will not violate any applicable laws,12,24
6616,6616,in cases of high dimensional andor derived attributes such justifications can be provided as overall descriptions of the attribute generation process and appropriateness,12,14
6617,6617,derived data sources tracked and reviewed carefully,12,24
6618,6618,data that is derived from other data through the use of algorithms such as data derived or inferred from prior model outputs should be identified and tracked eg via a specialized type in a data schema,12,13
6619,6619,derived data should be viewed as potentially high risk inputs that may lead to feedback loops compounded harm or inaccurate results,12,3
6620,6620,such sources should be carefully validated against the risk of collateral consequences,12,14
6621,6621,data reuse limits in sensitive domains,12,11
6622,6622,data reuse and especially data reuse in a new context can result in the spreading and scaling of harms,12,3
6623,6623,data from some domains including criminal justice data and data indicating adverse outcomes in domains such as finance employment and housing is especially sensitive and in some cases its reuse is limited by law,12,24
6624,6624,accordingly such data should be subject to extra oversight to ensure safety and efficacy,12,9
6625,6625,data reuse of sensitive domain data in other contexts eg criminal data reuse for civil legal matters or private sector use should only occur where use of such data is legally authorized and after examination has benefits for those impacted by the system that outweigh identified risks and as appropriate reasonable measures have been implemented to mitigate the identified risks,12,11
6626,6626,such data should be clearly labeled to identify contexts for limited reuse based on sensitivity,12,24
6627,6627,where possible aggregated datasets may be useful for replacing individual level sensitive data,12,24
6628,6628,demonstrate the safety and effectiveness of the systemindependent evaluation,12,17
6629,6629,automated systems should be designed to allow for independent evaluation eg via application programming interfaces,12,15
6630,6630,independent evaluators such as researchers journalists ethics review boards inspectors general and third party auditors should be given access to the system and samples of associated data in a manner consistent with privacy security law or regulation including eg intellectual property law in order to perform such evaluations,12,16
6631,6631,mechanisms should be included to ensure that system access for evaluation is provided in a timely manner to the deployment ready version of the system trusted to provide genuine unfiltered access to the full system and truly independent such that evaluator access cannot be revoked without reasonable and verified justification,12,15
6632,6632,reportingentities responsible for the development or use of automated systems should provide regularly updated reports that include an overview of the system including how it is embedded in the organizations business processes or other activities system goals any human run procedures that form a part of the system and specific performance expectations a description of any data used to train machine learning models or for other purposes including how data sources were processed and interpreted a summary of what data might be missing incomplete or erroneous and data relevancy justifications the results of public consultation such as concerns raised and any decisions made due to these concerns risk identification and management assessments and any steps taken to mitigate potential harms the results of performance testing including but not limited to accuracy differential demographic impact resulting error rates overall and per demographic group and comparisons to previously deployed systems ongoing monitoring procedures and regular performance testing reports including monitoring frequency results and actions taken and the procedures for and results from independent evaluations,12,15
6633,6633,reporting should be provided in a plain language and machine readable manner,12,24
6634,6634,real life examples of how these principles can become reality through laws policies and practical technical and sociotechnical approaches to protecting rights opportunities and access,12,12
6635,6635,executive order  on promoting the use of trustworthy artificial intelligence in the federal government requires that certain federal agencies adhere to nine principles when designing developing acquiring or using ai for purposes other than national security or defense,12,2
6636,6636,these principleswhile taking into account the sensitive law enforcement and other contexts in which the federal government may use ai as opposed to private sector use of airequire that ai is  lawful and respectful of our nations values  purposeful and performance driven  accurate reliable and effective  safe secure and resilient  understandable f  responsible and traceable  regularly monitored  transparent and  accountable,12,12
6637,6637,the blueprint for an ai bill of rights is consistent with the executive order,12,2
6638,6638,affected agencies across the federal government have released ai use case inventories and are implementing plans to bring those ai systems into compliance with the executive order or retire them,12,2
6639,6639,the law and policy landscape for motor vehicles shows that strong safety regulationsand measures to address harms when they occurcan enhance innovation in the context of complex technologies,12,8
6640,6640,cars like automated digital systems comprise a complex collection of components,12,15
6641,6641,the national highway traffic safety administration through its rigorous standards and independent evaluation helps make sure vehicles on our roads are safe without limiting manufacturers ability to innovate,12,18
6642,6642,at the same time rules of the road are implemented locally to impose contextually appropriate requirements on drivers such as slowing down near schools or playgrounds,12,27
6643,6643,from large companies to start ups industry is providing innovative solutions that allow organizations to mitigate risks to the safety and efficacy of ai systems both before deployment and through monitoring over time,12,3
6644,6644,these innovative solutions include risk assessments auditing mechanisms assessment of organizational procedures dashboards to allow for ongoing monitoring documentation procedures specific to model assessments and many other strategies that aim to mitigate risks posed by the use of ai to companies reputation legal responsibilities and other product safety and effectiveness concerns,12,0
6645,6645,the office of management and budget omb has called for an expansion of opportunities for meaningful stakeholder engagement in the design of programs and services,12,17
6646,6646,omb also points to numerous examples of effective and proactive stakeholder engagement including the community based participatory research program developed by the national institutes of health and the participatory technology assessments developed by the national oceanic and atmospheric administration,12,17
6647,6647,the national institute of standards and technology nist is developing a risk management framework to better manage risks posed to individuals organizations and society by ai,12,3
6648,6648,the nist ai risk management framework as mandated by congress is intended for voluntary use to help incorporate trustworthiness considerations into the design development use and evaluation of ai products services and systems,12,3
6649,6649,the nist framework is being developed through a consensus driven open transparent and collaborative process that includes workshops and other opportunities to provide input,12,27
6650,6650,the nist framework aims to foster the development of innovative approaches to address characteristics of trustworthiness including accuracy explainability and interpretability reliability privacy robustness safety security resilience and mitigation of unintended andor harmful bias as well as of harmful uses,12,18
6651,6651,the nist framework will consider and encompass principles such as transparency accountability and fairness during pre design design and development deployment use and testing and evaluation of ai technologies and systems,12,20
6652,6652,it is expected to be released in the winter of,12,22
6653,6653,some us government agencies have developed specific frameworks for ethical use of ai systems,12,16
6654,6654,the department of energy doe has activated the ai advancement council that oversees coordination and advises on implementation of the doe ai strategy and addresses issues andor escalations on the ethical use and development of ai systems,12,2
6655,6655,the department of defense has adopted artificial intelligence ethical principles and tenets for responsible artificial intelligence specifically tailored to its national security and defense activities,12,16
6656,6656,similarly the us intelligence community ic has developed the principles of artificial intelligence ethics for the intelligence community to guide personnel on whether and how to develop and use ai in furtherance of the ics mission as well as an ai ethics framework to help implement these principles,12,16
6657,6657,the national science foundation nsf funds extensive research to help foster the development of automated systems that adhere to and advance their safety security and effectiveness,12,15
6658,6658,multiple nsf programs support research that directly addresses many of these principles the national ai research institutes support research on all aspects of safe trustworthy fair and explainable ai algorithms and systems the cyber physical systems program supports research on developing safe autonomous and cyber physical systems with ai components the secure and trustworthy cyberspace program supports research on cybersecurity and privacy enhancing technologies in automated systems the formal methods in the field program supports research on rigorous formal verification and analysis of automated systems and machine learning and the designing accountable software systems program supports research on rigorous and reproducible methodologies for developing software systems with legal and regulatory compliance in mind,12,11
6659,6659,some state legislatures have placed strong transparency and validity requirements on the use of pretrial risk assessments,12,18
6660,6660,the use of algorithmic pretrial risk assessments has been a cause of concern for civil rights groups,12,6
6661,6661,idaho code section   enacted in  requires that any pretrial risk assessment before use in the state first be shown to be free of bias against any class of individuals protected from discrimination by state or federal law that any locality using a pretrial risk assessment must first formally validate the claim of its being free of bias that all documents records and information used to build or validate the risk assessment shall be open to public inspection and that assertions of trade secrets cannot be used to quash discovery in a criminal matter by a party to a criminal casealgorithmic discrimination protectionsyou should not face discrimination by algorithms and systems should be used and designed in an equitable way,12,6
6662,6662,algorithmic discrimination occurs when automated systems contribute to unjustified different treatment or impacts disfavoring people based on their race color ethnicity sex including pregnancy childbirth and related medical conditions gender identity intersex status and sexual orientation religion age national origin disability veteran status genetic information or any other classification protected by law,12,6
6663,6663,depending on the specific circumstances such algorithmic discrimination may violate legal protections,12,6
6664,6664,designers developers and deployers of automated systems should take proactive and continuous measures to protect individuals and communities from algorithmic discrimination and to use and design systems in an equitable way,12,6
6665,6665,this protection should include proactive equity assessments as part of the system design use of representative data and protection against proxies for demographic features ensuring accessibility for people with disabilities in design and development pre deployment and ongoing disparity testing and mitigation and clear organizational oversight,12,6
6666,6666,independent evaluation and plain language reporting in the form of an algorithmic impact assessment including disparity testing results and mitigation information should be performed and made public whenever possible to confirm these protections,12,6
6667,6667,this section provides a brief summary of the problems which the principle seeks to address and protect against including illustrative examples,12,14
6668,6668,there is extensive evidence showing that automated systems can produce inequitable outcomes and amplify existing inequity,12,15
6669,6669,data that fails to account for existing systemic biases in american society can result in a range of consequences,12,6
6670,6670,for example facial recognition technology that can contribute to wrongful and discriminatory arrests hiring algorithms that inform discriminatory decisions and healthcare algorithms that discount the severity of certain diseases in black americans,12,6
6671,6671,instances of discriminatory practices built into and resulting from ai and other automated systems exist across many industries areas and contexts,12,6
6672,6672,while automated systems have the capacity to drive extraordinary advances and innovations algorithmic discrimination protections should be built into their design deployment and ongoing use,12,6
6673,6673,many companies non profits and federal government agencies are already taking steps to ensure the public is protected from algorithmic discrimination,12,6
6674,6674,some companies have instituted bias testing as part of their product quality assessment and launch procedures and in some cases this testing has led products to be changed or not launched preventing harm to the public,12,0
6675,6675,federal government agencies have been developing standards and guidance for the use of automated systems in order to help prevent bias,12,15
6676,6676,non profits and companies have developed best practices for audits and impact assessments to help identify potential algorithmic discrimination and provide transparency to the public in the mitigation of such biases,12,6
6677,6677,but there is much more work to do to protect the public from algorithmic discrimination and to use and design automated systems in an equitable way,12,6
6678,6678,the guardrails protecting the public from discrimination in their daily lives should include their digital lives and impactsbasic safeguards against abuse bias and discrimination to ensure that all people are treated fairly when automated systems are used,12,6
6679,6679,this includes all dimensions of their lives from hiring to loan approvals from medical treatment and payment to encounters with the criminal justice system,12,12
6680,6680,ensuring equity should also go beyond existing guardrails to consider the holistic impact that automated systems make on underserved communities and to institute proactive protections that support these communities,12,15
6681,6681,an automated system using nontraditional factors such as educational attainment and employment history as part of its loan underwriting and pricing model was found to be much more likely to charge an applicant who attended a historically black college or university hbcu higher loan prices for refinancing a student loan than an applicant who did not attend an hbcu,12,0
6682,6682,this was found to be true even when controlling for other credit related factors,12,0
6683,6683,a hiring tool that learned the features of a companys employees predominantly men rejected women applicants for spurious and discriminatory reasons resumes with the word womens such as womens chess club captain were penalized in the candidate ranking,12,6
6684,6684,a predictive model marketed as being able to predict whether students are likely to drop out of school was used by more than  universities across the country,12,20
6685,6685,the model was found to use race directly as a predictor and also shown to have large disparities by race black students were as many as four times as likely as their otherwise similar white peers to be deemed at high risk of dropping out,12,6
6686,6686,these risk scores are used by advisors to guide students towards or away from majors and some worry that they are being used to guide black students away from math and science subjects,12,3
6687,6687,a risk assessment tool designed to predict the risk of recidivism for individuals in federal custody showed evidence of disparity in prediction,12,6
6688,6688,the tool overpredicts the risk of recidivism for some groups of color on the general recidivism tools and underpredicts the risk of recidivism for some groups of color on some of the violent recidivism tools,12,22
6689,6689,the department of justice is working to reduce these disparities and has publicly released a report detailing its review of the tool,12,6
6690,6690,an automated sentiment analyzer a tool often used by technology platforms to determine whether a statement posted online expresses a positive or negative sentiment was found to be biased against jews and gay people,12,6
6691,6691,for example the analyzer marked the statement im a jew as representing a negative sentiment while im a christian was identified as expressing a positive sentiment,12,12
6692,6692,this could lead to the preemptive blocking of social media comments such as im gay a related company with this bias concern has made their data public to encourage researchers to help address the issue and has released reports identifying and measuring this problem as well as detailing attempts to address it,12,6
6693,6693,searches for black girls asian girls or latina girls return predominantly sexualized content rather than role models toys or activities,12,12
6694,6694,some search engines have been working to reduce the prevalence of these results but the problem remains,12,7
6695,6695,advertisement delivery systems that predict who is most likely to click on a job advertisement end up delivering ads in ways that reinforce racial and gender stereotypes such as overwhelmingly directing supermarket cashier ads to women and jobs with taxi companies to primarily black people,12,6
6696,6696,body scanners used by tsa at airport checkpoints require the operator to select a male or female scanning setting based on the passengers sex but the setting is chosen based on the operators perception of the passengers gender identity,12,12
6697,6697,these scanners are more likely to flag transgender travelers as requiring extra screening done by a person,12,12
6698,6698,transgender travelers have described degrading experiences associated with these extra screenings,12,12
6699,6699,tsa has recently announced plans to implement a gender neutral algorithm while simultaneously enhancing the security effectiveness capabilities of the existing technology,12,12
6700,6700,the national disabled law students association expressed concerns that individuals with disabilities were more likely to be flagged as potentially suspicious by remote proctoring ai systems because of their disability specific access needs such as needing longer breaks or using screen readers or dictation software,12,25
6701,6701,an algorithm designed to identify patients with high needs for healthcare systematically assigned lower scores indicating that they were not as high need to black patients than to those of white patients even when those patients had similar numbers of chronic conditions and other markers of health,12,6
6702,6702,in addition healthcare clinical algorithms that are used by physicians to guide clinical decisions may include sociodemographic variables that adjust or correct the algorithms output on the basis of a patients race or ethnicity which can lead to race based health inequities,12,6
6703,6703,the expectations for automated systems are meant to serve as a blueprint for the development of additional technical standards and practices that are tailored for particular sectors and contexts,12,15
6704,6704,any automated system should be tested to help ensure it is free from algorithmic discrimination before it can be sold or used,12,6
6705,6705,protection against algorithmic discrimination should include designing to ensure equity broadly construed,12,6
6706,6706,some algorithmic discrimination is already prohibited under existing anti discrimination law,12,6
6707,6707,the expectations set out below describe proactive technical and policy steps that can be taken to not only reinforce those legal protections but extend beyond them to ensure equity for underserved communitiesxix even in circumstances where a specific legal protection may not be clearly established,12,27
6708,6708,these protections should be instituted throughout the design development and deployment process and are described below roughly in the order in which they would be instituted,12,11
6709,6709,protect the public from algorithmic discrimination in a proactive and ongoing mannerproactive assessment of equity in design,12,6
6710,6710,those responsible for the development use or oversight of automated systems should conduct proactive equity assessments in the design phase of the technology research and development or during its acquisition to review potential input data associated historical context accessibility for people with disabilities and societal goals to identify potential discrimination and effects on equity resulting from the introduction of the technology,12,15
6711,6711,the assessed groups should be as inclusive as possible of the underserved communities mentioned in the equity definition  black latino and indigenous and native american persons asian americans and pacific islanders and other persons of color members of religious minorities women girls and non binary people lesbian gay bisexual transgender queer and intersex lgbtqi persons older adults persons with disabilities persons who live in rural areas and persons otherwise adversely affected by persistent poverty or inequality,12,6
6712,6712,assessment could include both qualitative and quantitative evaluations of the system,12,18
6713,6713,this equity assessment should also be considered a core part of the goals of the consultation conducted as part of the safety and efficacy review,12,18
6714,6714,representative and robust data,12,24
6715,6715,any data used as part of system development or assessment should be representative of local communities based on the planned deployment setting and should be reviewed for bias based on the historical and societal context of the data,12,24
6716,6716,such data should be sufficiently robust to identify and help to mitigate biases and potential harms,12,6
6717,6717,guarding against proxies,12,7
6718,6718,directly using demographic information in the design development or deployment of an automated system for purposes other than evaluating a system for discrimination or using a system to counter discrimination runs a high risk of leading to algorithmic discrimination and should be avoided,12,6
6719,6719,in many cases attributes that are highly correlated with demographic features known as proxies can contribute to algorithmic discrimination,12,6
6720,6720,in cases where use of the demographic features themselves would lead to illegal algorithmic discrimination reliance on such proxies in decision making such as that facilitated by an algorithm may also be prohibited by law,12,6
6721,6721,proactive testing should be performed to identify proxies by testing for correlation between demographic information and attributes in any data used as part of system design development or use,12,6
6722,6722,if a proxy is identified designers developers and deployers should remove the proxy if needed it may be possible to identify alternative attributes that can be used instead,12,25
6723,6723,at a minimum organizations should ensure a proxy feature is not given undue weight and should monitor the system closely for any resulting algorithmic discrimination,12,6
6724,6724,ensuring accessibility during design development and deployment,12,15
6725,6725,systems should be designed developed and deployed by organizations in ways that ensure accessibility to people with disabilities,12,15
6726,6726,this should include consideration of a wide variety of disabilities adherence to relevant accessibility standards and user experience research both before and after deployment to identify and address any accessibility barriers to the use or effectiveness of the automated system,12,27
6727,6727,disparity assessment,12,6
6728,6728,automated systems should be tested using a broad set of measures to assess whether the system components both in pre deployment testing and in context deployment produce disparities,12,15
6729,6729,the demographics of the assessed groups should be as inclusive as possible of race color ethnicity sex including pregnancy childbirth and related medical conditions gender identity intersex status and sexual orientation religion age national origin disability veteran status genetic information or any other classification protected by law,12,6
6730,6730,the broad set of measures assessed should include demographic performance measures overall and subgroup parity assessment and calibration,12,18
6731,6731,demographic data collected for disparity assessment should be separated from data used for the automated system and privacy protections should be instituted in some cases it may make sense to perform such assessment using a data sample,12,6
6732,6732,for every instance where the deployed automated system leads to different treatment or impacts disfavoring the identified groups the entity governing implementing or using the system should document the disparity and a justification for any continued use of the system,12,15
6733,6733,disparity mitigation,12,6
6734,6734,when a disparity assessment identifies a disparity against an assessed group it may be appropriate to take steps to mitigate or eliminate the disparity,12,6
6735,6735,in some cases mitigation or elimination of the disparity may be required by law,12,6
6736,6736,disparities that have the potential to lead to algorithmic discrimination cause meaningful harm or violate equityxx goals should be mitigated,12,6
6737,6737,when designing and evaluating an automated system steps should be taken to evaluate multiple models and select the one that has the least adverse impact modify data input choices or otherwise identify a system with fewer disparities,12,21
6738,6738,if adequate mitigation of the disparity is not possible then the use of the automated system should be reconsidered,12,6
6739,6739,one of the considerations in whether to use the system should be the validity of any target measure unobservable targets may result in the inappropriate use of proxies,12,17
6740,6740,meeting these standards may require instituting mitigation procedures and other protective measures to address algorithmic discrimination avoid meaningful harm and achieve equity goals,12,6
6741,6741,ongoing monitoring and mitigation,12,17
6742,6742,automated systems should be regularly monitored to assess algorithmic discrimination that might arise from unforeseen interactions of the system with inequities not accounted for during the pre deployment testing changes to the system after deployment or changes to the context of use or associated data,12,15
6743,6743,monitoring and disparity assessment should be performed by the entity deploying or using the automated system to examine whether the system has led to algorithmic discrimination when deployed,12,6
6744,6744,this assessment should be performed regularly and whenever a pattern of unusual results is occurring,12,18
6745,6745,it can be performed using a variety of approaches taking into account whether and how demographic information of impacted people is available for example via testing with a sample of users or via qualitative user experience research,12,17
6746,6746,riskier and higher impact systems should be monitored and assessed more frequently,12,17
6747,6747,outcomes of this assessment should include additional disparity mitigation if needed or fallback to earlier procedures in the case that equity standards are no longer met and cant be mitigated and prior mechanisms provide better adherence to equity standards,12,6
6748,6748,demonstrate that the system protects against algorithmic discriminationindependent evaluation,12,6
6749,6749,as described in the section on safe and effective systems entities should allow independent evaluation of potential algorithmic discrimination caused by automated systems they use or oversee,12,15
6750,6750,in the case of public sector uses these independent evaluations should be made public unless law enforcement or national security restrictions prevent doing so,12,12
6751,6751,care should be taken to balance individual privacy with evaluation data access needs in many cases policy based andor technological innovations and controls allow access to such data without compromising privacy,12,11
6752,6752,reporting,12,24
6753,6753,entities responsible for the development or use of automated systems should provide reporting of an appropriately designed algorithmic impact assessment with clear specification of who performs the assessment who evaluates the system and how corrective actions are taken if necessary in response to the assessment,12,15
6754,6754,this algorithmic impact assessment should include at least the results of any consultation design stage equity assessments potentially including qualitative analysis accessibility designs and testing disparity testing document any remaining disparities and detail any mitigation implementation and assessments,12,6
6755,6755,this algorithmic impact assessment should be made public whenever possible,12,17
6756,6756,reporting should be provided in a clear and machine readable manner using plain language to allow for more straightforward public accountability,12,24
6757,6757,real life examples of how these principles can become reality through laws policies and practical technical and sociotechnical approaches to protecting rights opportunities and access,12,12
6758,6758,the federal government is working to combat discrimination in mortgage lending,12,6
6759,6759,the department of justice has launched a nationwide initiative to combat redlining which includes reviewing how lenders who may be avoiding serving communities of color are conducting targeted marketing and advertising,12,6
6760,6760,this initiative will draw upon strong partnerships across federal agencies including the consumer financial protection bureau and prudential regulators,12,0
6761,6761,the action plan to advance property appraisal and valuation equity includes a commitment from the agencies that oversee mortgage lending to include a nondiscrimination standard in the proposed rules for automated valuation models,12,17
6762,6762,the equal employment opportunity commission and the department of justice have clearly laid out how employers use of ai and other automated systems can result in discrimination against job applicants and employees with disabilities,12,6
6763,6763,the documents explain how employers use of software that relies on algorithmic decision making may violate existing requirements under title i of the americans with disabilities act ada,12,6
6764,6764,this technical assistance also provides practical tips to employers on how to comply with the ada and to job applicants and employees who think that their rights may have been violated,12,27
6765,6765,disparity assessments identified harms to black patients healthcare access,12,6
6766,6766,a widely used healthcare algorithm relied on the cost of each patients past medical care to predict future medical needs recommending early interventions for the patients deemed most at risk,12,13
6767,6767,this process discriminated against black patients who generally have less access to medical care and therefore have generated less cost than white patients with similar illness and need,12,6
6768,6768,a landmark study documented this pattern and proposed practical ways that were shown to reduce this bias such as focusing specifically on active chronic health conditions or avoidable future costs related to emergency visits and hospitalization,12,22
6769,6769,large employers have developed best practices to scrutinize the data and models used for hiring,12,6
6770,6770,an industry initiative has developed algorithmic bias safeguards for the workforce a structured questionnaire that businesses can use proactively when procuring software to evaluate workers,12,6
6771,6771,it covers specific technical questions such as the training data used model training process biases identified and mitigation steps employed,12,13
6772,6772,standards organizations have developed guidelines to incorporate accessibility criteria into technology design processes,12,27
6773,6773,the most prevalent in the united states is the access boards section  regulations which are the technical standards for federal information communication technology software hardware and web,12,26
6774,6774,other standards include those issued by the international organization for standardization and the world wide web consortium  web content accessibility guidelines a globally recognized voluntary consensus standard for web content and other information and communications technology,12,27
6775,6775,nist has released special publication  towards a standard for identifying and managing bias in artificial intelligence,12,6
6776,6776,the special publication describes the stakes and challenges of bias in artificial intelligence and provides examples of how and why it can chip away at public trust identifies three categories of bias in ai  systemic statistical and human  and describes how and where they contribute to harms and describes three broad challenges for mitigating bias  datasets testing and evaluation and human factors  and introduces preliminary guidance for addressing them,12,6
6777,6777,throughout the special publication takes a socio technical perspective to identifying and managing ai bias,12,6
6778,6778,data privacyyou should be protected from abusive data practices via built in protections and you should have agency over how data about you is used,12,11
6779,6779,you should be protected from violations of privacy through design choices that ensure such protections are included by default including ensuring that data collection conforms to reasonable expectations and that only data strictly necessary for the specific context is collected,12,11
6780,6780,designers developers and deployers of automated systems should seek your permission and respect your decisions regarding collection use access transfer and deletion of your data in appropriate ways and to the greatest extent possible where not possible alternative privacy by design safeguards should be used,12,11
6781,6781,systems should not employ user experience and design decisions that obfuscate user choice or burden users with defaults that are privacy invasive,12,15
6782,6782,consent should only be used to justify collection of data in cases where it can be appropriately and meaningfully given,12,9
6783,6783,any consent requests should be brief be understandable in plain language and give you agency over data collection and the specific context of use current hard to understand notice and choice practices for broad uses of data should be changed,12,21
6784,6784,enhanced protections and restrictions for data and inferences related to sensitive domains including health work education criminal justice and finance and for data pertaining to youth should put you first,12,11
6785,6785,in sensitive domains your data and related inferences should only be used for necessary functions and you should be protected by ethical review and use prohibitions,12,11
6786,6786,you and your communities should be free from unchecked surveillance surveillance technologies should be subject to heightened oversight that includes at least pre deployment assessment of their potential harms and scope limits to protect privacy and civil liberties,12,12
6787,6787,continuous surveillance and monitoring should not be used in education work housing or in other contexts where the use of such surveillance technologies is likely to limit rights opportunities or access,12,12
6788,6788,whenever possible you should have access to reporting that confirms your data decisions have been respected and provides an assessment of the potential impact of surveillance technologies on your rights opportunities or access,12,12
6789,6789,this section provides a brief summary of the problems which the principle seeks to address and protect against including illustrative examples,12,14
6790,6790,data privacy is a foundational and cross cutting principle required for achieving all others in this framework,12,11
6791,6791,surveillance and data collection sharing use and reuse now sit at the foundation of business models across many industries with more and more companies tracking the behavior of the american public building individual profiles based on this data and using this granular level information as input into automated systems that further track profile and impact the american public,12,1
6792,6792,government agencies particularly law enforcement agencies also use and help develop a variety of technologies that enhance and expand surveillance capabilities which similarly collect data used as input into other automated systems that directly impact peoples lives,12,12
6793,6793,federal law has not grown to address the expanding scale of private data collection or of the ability of governments at all levels to access that data and leverage the means of private collection,12,26
6794,6794,meanwhile members of the american public are often unable to access their personal data or make critical decisions about its collection and use,12,9
6795,6795,data brokers frequently collect consumer data from numerous sources without consumers permission or knowledge,12,9
6796,6796,moreover there is a risk that inaccurate and faulty data can be used to make decisions about their lives such as whether they will qualify for a loan or get a job,12,9
6797,6797,use of surveillance technologies has increased in schools and workplaces and when coupled with consequential management and evaluation decisions it is leading to mental health harms such as lowered self confidence anxiety depression and a reduced ability to use analytical reasoning,12,12
6798,6798,documented patterns show that personal data is being aggregated by data brokers to profile communities in harmful ways,12,9
6799,6799,the impact of all this data harvesting is corrosive breeding distrust anxiety and other mental health problems chilling speech protest and worker organizing and threatening our democratic process,12,9
6800,6800,the american public should be protected from these growing risks,12,12
6801,6801,increasingly some companies are taking these concerns seriously and integrating mechanisms to protect consumer privacy into their products by design and by default including by minimizing the data they collect communicating collection and use clearly and improving security practices,12,11
6802,6802,federal government surveillance and other collection and use of data is governed by legal protections that help to protect civil liberties and provide for limits on data retention in some cases,12,11
6803,6803,many states have also enacted consumer data privacy protection regimes to address some of these harms,12,11
6804,6804,however these are not yet standard practices and the united states lacks a comprehensive statutory or regulatory framework governing the rights of the public when it comes to personal data,12,9
6805,6805,while a patchwork of laws exists to guide the collection and use of personal data in specific contexts including health employment education and credit it can be unclear how these laws apply in other contexts and in an increasingly automated society,12,9
6806,6806,additional protections would assure the american public that the automated systems they use are not monitoring their activities collecting information on their lives or otherwise surveilling them without context specific consent or legal authority,12,12
6807,6807,an insurer might collect data from a persons social media presence as part of deciding what life insurance rates they should be offered,12,9
6808,6808,a data broker harvested large amounts of personal data and then suffered a breach exposing hundreds of thousands of people to potential identity theft,12,0
6809,6809,a local public housing authority installed a facial recognition system at the entrance to housing complexes to assist law enforcement with identifying individuals viewed via camera when police reports are filed leading the community both those living in the housing complex and not to have videos of them sent to the local police department and made available for scanning by its facial recognition software,12,12
6810,6810,companies use surveillance software to track employee discussions about union activity and use the resulting data to surveil individual employees and surreptitiously intervene in discussions,12,1
6811,6811,the expectations for automated systems are meant to serve as a blueprint for the development of additional technical standards and practices that are tailored for particular sectors and contexts,12,15
6812,6812,traditional terms of servicethe block of text that the public is accustomed to clicking through when using a website or digital appare not an adequate mechanism for protecting privacy,12,7
6813,6813,the american public should be protected via built in privacy protections data minimization use and collection limitations and transparency in addition to being entitled to clear mechanisms to control access to and use of their dataincluding their metadatain a proactive informed and ongoing way,12,11
6814,6814,any automated system collecting using sharing or storing personal data should meet these expectations,12,15
6815,6815,protect privacy by design and by defaultprivacy by design and by default,12,11
6816,6816,automated systems should be designed and built with privacy protected by default,12,15
6817,6817,privacy risks should be assessed throughout the development life cycle including privacy risks from reidentification and appropriate technical and policy mitigation measures should be implemented,12,3
6818,6818,this includes potential harms to those who are not users of the automated system but who may be harmed by inferred data purposeful privacy violations or community surveillance or other community harms,12,15
6819,6819,data collection should be minimized and clearly communicated to the people whose data is collected,12,9
6820,6820,data should only be collected or used for the purposes of training or testing machine learning models if such collection and use is legal and consistent with the expectations of the people whose data is collected,12,9
6821,6821,user experience research should be conducted to confirm that people understand what data is being collected about them and how it will be used and that this collection matches their expectations and desires,12,24
6822,6822,data collection and use case scope limits,12,24
6823,6823,data collection should be limited in scope with specific narrow identified goals to avoid mission creep  anticipated data collection should be determined to be strictly necessary to the identified goals and should be minimized as much as possible,12,11
6824,6824,data collected based on these identified goals and for a specific context should not be used in a different context without assessing for new privacy risks and implementing appropriate mitigation measures which may include express consent,12,9
6825,6825,clear timelines for data retention should be established with data deleted as soon as possible in accordance with legal or policy based limitations,12,22
6826,6826,determined data retention timelines should be documented and justified,12,22
6827,6827,risk identification and mitigation,12,3
6828,6828,entities that collect use share or store sensitive data should attempt to proactively identify harms and seek to manage them so as to avoid mitigate and respond appropriately to identified risks,12,3
6829,6829,appropriate responses include determining not to process data when the privacy risks outweigh the benefits or implementing measures to mitigate acceptable risks,12,11
6830,6830,appropriate responses do not include sharing or transferring the privacy risks to users via notice or consent requests where users could not reasonably be expected to understand the risks without further support,12,3
6831,6831,privacy preserving security,12,11
6832,6832,entities creating using or governing automated systems should follow privacy and security best practices designed to ensure data and metadata do not leak beyond the specific consented use case,12,15
6833,6833,best practices could include using privacy enhancing cryptography or other types of privacy enhancing technologies or fine grained permissions and access control mechanisms along with conventional system security protocols,12,11
6834,6834,protect the public from unchecked surveillanceheightened oversight of surveillance,12,12
6835,6835,surveillance or monitoring systems should be subject to heightened oversight that includes at a minimum assessment of potential harms during design before deployment and in an ongoing manner to ensure that the american publics rights opportunities and access are protected,12,12
6836,6836,this assessment should be done before deployment and should give special attention to ensure there is not algorithmic discrimination especially based on community membership when deployed in a specific real world context,12,6
6837,6837,such assessment should then be reaffirmed in an ongoing manner as long as the system is in use,12,18
6838,6838,limited and proportionate surveillance,12,12
6839,6839,surveillance should be avoided unless it is strictly necessary to achieve a legitimate purpose and it is proportionate to the need,12,12
6840,6840,designers developers and deployers of surveillance systems should use the least invasive means of monitoring available and restrict monitoring to the minimum number of subjects possible,12,12
6841,6841,to the greatest extent possible consistent with law enforcement and national security needs individuals subject to monitoring should be provided with clear and specific notice before it occurs and be informed about how the data gathered through surveillance will be used,12,12
6842,6842,scope limits on surveillance to protect rights and democratic values,12,12
6843,6843,civil liberties and civil rights must not be limited by the threat of surveillance or harassment facilitated or aided by an automated system,12,15
6844,6844,surveillance systems should not be used to monitor the exercise of democratic rights such as voting privacy peaceful assembly speech or association in a way that limits the exercise of civil rights or civil liberties,12,12
6845,6845,information about or algorithmically determined assumptions related to identity should be carefully limited if used to target or guide surveillance systems in order to avoid algorithmic discrimination such identity related information includes group characteristics or affiliations geographic designations location based and association based inferences social networks and biometrics,12,12
6846,6846,continuous surveillance and monitoring systems should not be used in physical or digital workplaces regardless of employment status public educational institutions and public accommodations,12,12
6847,6847,continuous surveillance and monitoring systems should not be used in a way that has the effect of limiting access to critical resources or services or suppressing the exercise of rights even where the organization is not under a particular duty to protect those rights,12,12
6848,6848,provide the public with mechanisms for appropriate and meaningful consent access and control over their datause specific consent,12,11
6849,6849,consent practices should not allow for abusive surveillance practices,12,12
6850,6850,where data collectors or automated systems seek consent they should seek it for specific narrow use contexts for specific time durations and for use by specific entities,12,9
6851,6851,consent should not extend if any of these conditions change consent should be re acquired before using data if the use case changes a time limit elapses or data is transferred to another entity including being shared or sold,12,9
6852,6852,consent requested should be limited in scope and should not request consent beyond what is required,12,21
6853,6853,refusal to provide consent should be allowed without adverse effects to the greatest extent possible based on the needs of the use case,12,1
6854,6854,brief and direct consent requests,12,21
6855,6855,when seeking consent from users short plain language consent requests should be used so that users understand for what use contexts time span and entities they are providing data and metadata consent,12,21
6856,6856,user experience research should be performed to ensure these consent requests meet performance standards for readability and comprehension,12,27
6857,6857,this includes ensuring that consent requests are accessible to users with disabilities and are available in the languages and reading level appropriate for the audience,12,27
6858,6858,user experience design choices that intentionally obfuscate or manipulate user choice ie dark patterns should be not be used,12,14
6859,6859,data access and correction,12,11
6860,6860,people whose data is collected used shared or stored by automated systems should be able to access data and metadata about themselves know who has access to this data and be able to correct it if necessary,12,9
6861,6861,entities should receive consent before sharing data with other entities and should keep records of what data is shared and with whom,12,9
6862,6862,consent withdrawal and data deletion,12,9
6863,6863,entities should allow to the extent legally permissible withdrawal of data access consent resulting in the deletion of user data metadata and the timely removal of their data from any systems eg machine learning models derived from that data,12,9
6864,6864,automated system support,12,15
6865,6865,entities designing developing and deploying automated systems should establish and maintain the capabilities that will allow individuals to use their own automated systems to help them make consent access and control decisions in a complex data ecosystem,12,15
6866,6866,capabilities include machine readable data standardized data formats metadata or tags for expressing data processing permissions and preferences and data provenance and lineage context of use and access specific tags and training models for assessing privacy risk,12,11
6867,6867,demonstrate that data privacy and user control are protectedindependent evaluation,12,11
6868,6868,as described in the section on safe and effective systems entities should allow independent evaluation of the claims made regarding data policies,12,11
6869,6869,these independent evaluations should be made public whenever possible,12,18
6870,6870,care will need to be taken to balance individual privacy with evaluation data access needs,12,11
6871,6871,reporting,12,24
6872,6872,when members of the public wish to know what data about them is being used in a system the entity responsible for the development of the system should respond quickly with a report on the data it has collected or stored about them,12,24
6873,6873,such a report should be machine readable understandable by most users and include to the greatest extent allowable under law any data and metadata about them or collected from them when and how their data and metadata were collected the specific ways that data or metadata are being used who has access to their data and metadata and what time limitations apply to these data,12,24
6874,6874,in cases where a user login is not available identity verification may need to be performed before providing such a report to ensure user privacy,12,12
6875,6875,additionally summary reporting should be proactively made public with general information about how peoples data and metadata is used accessed and stored,12,24
6876,6876,summary reporting should include the results of any surveillance pre deployment assessment including disparity assessment in the real world deployment context the specific identified goals of any data collection and the assessment done to ensure only the minimum required data is collected,12,24
6877,6877,it should also include documentation about the scope limit assessments including data retention timelines and associated justification and an assessment of the impact of surveillance or data collection on rights opportunities and access,12,24
6878,6878,where possible this assessment of the impact of surveillance should be done by an independent party,12,12
6879,6879,reporting should be provided in a clear and machine readable manner,12,24
6880,6880,some domains including health employment education criminal justice and personal finance have long been singled out as sensitive domains deserving of enhanced data protections,12,11
6881,6881,this is due to the intimate nature of these domains as well as the inability of individuals to opt out of these domains in any meaningful way and the historical discrimination that has often accompanied data knowledge,12,6
6882,6882,domains understood by the public to be sensitive also change over time including because of technological developments,12,14
6883,6883,tracking and monitoring technologies personal tracking devices and our extensive data footprints are used and misused more than ever before as such the protections afforded by current legal guidelines may be inadequate,12,12
6884,6884,the american public deserves assurances that data related to such sensitive domains is protected and used appropriately and only in narrowly defined contexts with clear benefits to the individual andor society,12,12
6885,6885,to this end automated systems that collect use share or store data related to these sensitive domains should meet additional expectations,12,15
6886,6886,data and metadata are sensitive if they pertain to an individual in a sensitive domain defined below are generated by technologies used in a sensitive domain can be used to infer data from a sensitive domain or sensitive data about an individual such as disability related data genomic data biometric data behavioral data geolocation data data related to interaction with the criminal justice system relationship history and legal status such as custody and divorce information and home work or school environmental data or have the reasonable potential to be used in ways that are likely to expose individuals to meaningful harm such as a loss of privacy or financial harm due to identity theft,12,11
6887,6887,data and metadata generated by or about those who are not yet legal adults is also sensitive even if not related to a sensitive domain,12,12
6888,6888,such data includes but is not limited to numerical text image audio or video data,12,24
6889,6889,sensitive domains are those in which activities being conducted can cause material harms including significant adverse effects on human rights such as autonomy and dignity as well as civil liberties and civil rights,12,12
6890,6890,domains that have historically been singled out as deserving of enhanced data protections or where such enhanced protections are reasonably expected by the public include but are not limited to health family planning and care employment education criminal justice and personal finance,12,11
6891,6891,in the context of this framework such domains are considered sensitive whether or not the specifics of a system context would necessitate coverage under existing law and domains and data that are considered sensitive are understood to change over time based on societal norms and context,12,26
6892,6892,continuous positive airway pressure machines gather data for medical purposes such as diagnosing sleep apnea and send usage data to a patients insurance company which may subsequently deny coverage for the device based on usage data,12,0
6893,6893,patients were not aware that the data would be used in this way or monitored by anyone other than their doctor,12,9
6894,6894,a department store company used predictive analytics applied to collected consumer data to determine that a teenage girl was pregnant and sent maternity clothing ads and other baby related advertisements to her house revealing to her father that she was pregnant,12,13
6895,6895,school audio surveillance systems monitor student conversations to detect potential stress indicators as a warning of potential violence,12,12
6896,6896,online proctoring systems claim to detect if a student is cheating on an exam using biometric markers,12,12
6897,6897,these systems have the potential to limit student freedom to express a range of emotions at school and may inappropriately flag students with disabilities who need accommodations or use screen readers or dictation software as cheating,12,15
6898,6898,location data acquired from a data broker can be used to identify people who visit abortion clinics,12,24
6899,6899,companies collect student data such as demographic information free or reduced lunch status whether theyve used drugs or whether theyve expressed interest in lgbtqi groups and then use that data to forecast student successparents and education experts have expressed concern about collection of such sensitive data without express parental consent the lack of transparency in how such data is being used and the potential for resulting discriminatory impacts,12,9
6900,6900,many employers transfer employee data to third party job verification services,12,9
6901,6901,this information is then used by potential future employers banks or landlords,12,0
6902,6902,in one case a former employee alleged that a company supplied false data about her job title which resulted in a job offer being revoked,12,0
6903,6903,what should be expected of automated systems,12,15
6904,6904,in addition to the privacy expectations above for general non sensitive data any system collecting using sharing or storing sensitive data should meet the expectations below,12,11
6905,6905,depending on the technological use case and based on an ethical assessment consent for sensitive data may need to be acquired from a guardian andor child,12,9
6906,6906,provide enhanced protections for data related to sensitive domainsnecessary functions only,12,11
6907,6907,sensitive data should only be used for functions strictly necessary for that domain or for functions that are required for administrative reasons eg school attendance records unless consent is acquired if appropriate and the additional expectations in this section are met,12,11
6908,6908,consent for non necessary functions should be optional ie should not be required incentivized or coerced in order to receive opportunities or access to services,12,1
6909,6909,in cases where data is provided to an entity eg health insurance company in order to facilitate payment for such a need that data should only be used for that purpose,12,9
6910,6910,ethical review and use prohibitions,12,1
6911,6911,any use of sensitive data or decision process based in part on sensitive data that might limit rights opportunities or access whether the decision is automated or not should go through a thorough ethical review and monitoring both in advance and by periodic review eg via an independent ethics committee or similarly robust process,12,21
6912,6912,in some cases this ethical review may determine that data should not be used or shared for specific uses even with consent,12,9
6913,6913,some novel uses of automated systems in this context where the algorithm is dynamically developing and where the science behind the use case is not well established may also count as human subject experimentation and require special review under organizational compliance bodies applying medical scientific and academic human subject experimentation ethics rules and governance procedures,12,15
6914,6914,data quality,12,18
6915,6915,in sensitive domains entities should be especially careful to maintain the quality of data to avoid adverse consequences arising from decision making based on flawed or inaccurate data,12,11
6916,6916,such care is necessary in a fragmented complex data ecosystem and for datasets that have limited access such as for fraud prevention and law enforcement,12,11
6917,6917,it should be not left solely to individuals to carry the burden of reviewing and correcting data,12,9
6918,6918,entities should conduct regular independent audits and take prompt corrective measures to maintain accurate timely and complete data,12,18
6919,6919,limit access to sensitive data and derived data,12,11
6920,6920,sensitive data and derived data should not be sold shared or made public as part of data brokerage or other agreements,12,11
6921,6921,sensitive data includes data that can be used to infer sensitive information even systems that are not directly marketed as sensitive domain technologies are expected to keep sensitive data private,12,11
6922,6922,access to such data should be limited based on necessity and based on a principle of local control such that those individuals closest to the data subject have more access while those who are less proximate do not eg a teacher has access to their students daily progress data while a superintendent does not,12,9
6923,6923,reporting,12,24
6924,6924,in addition to the reporting on data privacy as listed above for non sensitive data entities developing technologies related to a sensitive domain and those collecting using storing or sharing sensitive data should whenever appropriate regularly provide public reports describing any data security lapses or breaches that resulted in sensitive data leaks the number type and outcomes of ethical pre reviews undertaken a description of any data sold shared or made public and how that data was assessed to determine it did not present a sensitive data risk and ongoing risk identification and management procedures and any mitigation added based on these procedures,12,11
6925,6925,reporting should be provided in a clear and machine readable manner,12,24
6926,6926,real life examples of how these principles can become reality through laws policies and practical technical and sociotechnical approaches to protecting rights opportunities and access,12,12
6927,6927,the privacy act of  requires privacy protections for personal information in federal records systems including limits on data retention and also provides individuals a general right to access and correct their data,12,11
6928,6928,among other things the privacy act limits the storage of individual information in federal systems of records illustrating the principle of limiting the scope of data retention,12,11
6929,6929,under the privacy act federal agencies may only retain data about an individual that is relevant and necessary to accomplish an agencys statutory purpose or to comply with an executive order of the president,12,9
6930,6930,the law allows for individuals to be able to access any of their individual information stored in a federal system of records if not included under one of the systems of records exempted pursuant to the privacy act,12,9
6931,6931,in these cases federal agencies must provide a method for an individual to determine if their personal information is stored in a particular system of records and must provide procedures for an individual to contest the contents of a record about them,12,12
6932,6932,further the privacy act allows for a cause of action for an individual to seek legal relief if a federal agency does not comply with the privacy acts requirements,12,12
6933,6933,among other things a court may order a federal agency to amend or correct an individuals information in its records or award monetary damages if an inaccurate irrelevant untimely or incomplete record results in an adverse determination about an individuals qualifications character rights  opportunities or benefitsnists privacy framework provides a comprehensive detailed and actionable approach for organizations to manage privacy risks,12,10
6934,6934,the nist framework gives organizations ways to identify and communicate their privacy risks and goals to support ethical decision making in system product and service design or deployment as well as the measures they are taking to demonstrate compliance with applicable laws or regulations,12,11
6935,6935,it has been voluntarily adopted by organizations across many different sectors around the world,12,27
6936,6936,a school boards attempt to surveil public school studentsundertaken without adequate community inputsparked a state wide biometrics moratorium,12,12
6937,6937,reacting to a plan in the city of lockport new york the states legislature banned the use of facial recognition systems and other biometric identifying technology in schools until july,12,12
6938,6938,the law additionally requires that a report on the privacy civil rights and civil liberties implications of the use of such technologies be issued before biometric identification technologies can be used in new york schools,12,12
6939,6939,federal law requires employers and any consultants they may retain to report the costs of surveilling employees in the context of a labor dispute providing a transparency mechanism to help protect worker organizing,12,10
6940,6940,employers engaging in workplace surveillance where an object thereof directly or indirectly is to obtain information concerning the activities of employees or a labor organization in connection with a labor dispute must report expenditures relating to this surveillance to the department of labor office of labor management standards and consultants who employers retain for these purposes must also file reports regarding their activities,12,1
6941,6941,privacy choices on smartphones show that when technologies are well designed privacy and data agency can be meaningful and not overwhelming,12,11
6942,6942,these choicessuch as contextual timely alerts about location trackingare brief direct and use specific,12,1
6943,6943,many of the expectations listed here for privacy by design and use specific consent mirror those distributed to developers as best practices when developing for smart phone devices such as being transparent about how user data will be used asking for app permissions during their use so that the use context will be clear to users and ensuring that the app will still work if users deny or later revoke some permissions,12,11
6944,6944,notice and explanationyou should know that an automated system is being used and understand how and why it contributes to outcomes that impact you,12,15
6945,6945,designers developers and deployers of automated systems should provide generally accessible plain language documentation including clear descriptions of the overall system functioning and the role automation plays notice that such systems are in use the individual or organization responsible for the system and explanations of outcomes that are clear timely and accessible,12,15
6946,6946,such notice should be kept up to date and people impacted by the system should be notified of significant use case or key functionality changes,12,15
6947,6947,you should know how and why an outcome impacting you was determined by an automated system including when the automated system is not the sole input determining the outcome,12,21
6948,6948,automated systems should provide explanations that are technically valid meaningful and useful to you and to any operators or others who need to understand the system and calibrated to the level of risk based on the context,12,15
6949,6949,reporting that includes summary information about these automated systems in plain language and assessments of the clarity and quality of the notice and explanations should be made public whenever possible,12,15
6950,6950,this section provides a brief summary of the problems which the principle seeks to address and protect against including illustrative examples,12,14
6951,6951,automated systems now determine opportunities from employment to credit and directly shape the american publics experiences from the courtroom to online classrooms in ways that profoundly impact peoples lives,12,15
6952,6952,but this expansive impact is not always visible,12,22
6953,6953,an applicant might not know whether a person rejected their resume or a hiring algorithm moved them to the bottom of the list,12,6
6954,6954,a defendant in the courtroom might not know if a judge denying their bail is informed by an automated system that labeled them high risk from correcting errors to contesting decisions people are often denied the knowledge they need to address the impact of automated systems on their lives,12,15
6955,6955,notice and explanations also serve an important safety and efficacy purpose allowing experts to verify the reasonableness of a recommendation before enacting it,12,14
6956,6956,in order to guard against potential harms the american public needs to know if an automated system is being used,12,15
6957,6957,clear brief and understandable notice is a prerequisite for achieving the other protections in this framework,12,27
6958,6958,likewise the public is often unable to ascertain how or why an automated system has made a decision or contributed to a particular outcome,12,15
6959,6959,the decision making processes of automated systems tend to be opaque complex and therefore unaccountable whether by design or by omission,12,15
6960,6960,these factors can make explanations both more challenging and more important and should not be used as a pretext to avoid explaining important decisions to the people impacted by those choices,12,14
6961,6961,in the context of automated systems clear and valid explanations should be recognized as a baseline requirement,12,14
6962,6962,providing notice has long been a standard practice and in many cases is a legal requirement when for example making a video recording of someone outside of a law enforcement or national security context,12,12
6963,6963,in some cases such as credit lenders are required to provide notice and explanation to consumers,12,0
6964,6964,techniques used to automate the process of explaining such systems are under active research and improvement and such explanations can take many forms,12,14
6965,6965,innovative companies and researchers are rising to the challenge and creating and deploying explanatory systems that can help the public better understand decisions that impact them,12,15
6966,6966,while notice and explanation requirements are already in place in some sectors or situations the american public deserve to know consistently and across sectors if an automated system is being used in a way that impacts their rights opportunities or access,12,15
6967,6967,this knowledge should provide confidence in how the public is being treated and trust in the validity and reasonable use of automated systems,12,15
6968,6968,a lawyer representing an older client with disabilities who had been cut off from medicaid funded home health care assistance couldnt determine why especially since the decision went against historical access practices,12,10
6969,6969,in a court hearing the lawyer learned from a witness that the state in which the older client lived had recently adopted a new algorithm to determine eligibility,12,10
6970,6970,the lack of a timely explanation made it harder to understand and contest the decision,12,14
6971,6971,a formal child welfare investigation is opened against a parent based on an algorithm and without the parent ever being notified that data was being collected and used as part of an algorithmic child maltreatment risk assessment,12,21
6972,6972,the lack of notice or an explanation makes it harder for those performing child maltreatment assessments to validate the risk assessment and denies parents knowledge that could help them contest a decision,12,14
6973,6973,a predictive policing system claimed to identify individuals at greatest risk to commit or become the victim of gun violence based on automated analysis of social ties to gang members criminal histories previous experiences of gun violence and other factors and led to individuals being placed on a watch list with no explanation or public transparency regarding how the system came to its conclusions,12,12
6974,6974,both police and the public deserve to understand why and how such a system is making these determinations,12,12
6975,6975,a system awarding benefits changed its criteria invisibly,12,26
6976,6976,individuals were denied benefits due to data entry errors and other system flaws,12,0
6977,6977,these flaws were only revealed when an explanation of the system was demanded and produced,12,14
6978,6978,the lack of an explanation made it harder for errors to be corrected in a timely manner,12,14
6979,6979,an automated system should provide demonstrably clear timely understandable and accessible notice of use and explanations as to how and why a decision was made or an action was taken by the system,12,15
6980,6980,these expectations are explained below,12,22
6981,6981,provide clear timely understandable and accessible notice of use and explanations,12,14
6982,6982,generally accessible plain language documentation,12,24
6983,6983,the entity responsible for using the automated system should ensure that documentation describing the overall system including any human components is public and easy to find,12,15
6984,6984,the documentation should describe in plain language how the system works and how any automated component is used to determine an action or decision,12,15
6985,6985,it should also include expectations about reporting described throughout this framework such as the algorithmic impact assessments described as part of algorithmic discrimination protections,12,6
6986,6986,accountable,12,16
6987,6987,notices should clearly identify the entity responsible for designing each component of the system and the entity using it,12,15
6988,6988,timely and up to date,12,22
6989,6989,users should receive notice of the use of automated systems in advance of using or while being impacted by the technology,12,15
6990,6990,an explanation should be available with the decision itself or soon thereafter,12,21
6991,6991,notice should be kept up to date and people impacted by the system should be notified of use case or key functionality changes,12,15
6992,6992,brief and clear,12,22
6993,6993,notices and explanations should be assessed such as by research on users experiences including user testing to ensure that the people using or impacted by the automated system are able to easily find notices and explanations read them quickly and understand and act on them,12,15
6994,6994,this includes ensuring that notices and explanations are accessible to users with disabilities and are available in the languages and reading level appropriate for the audience,12,27
6995,6995,notices and explanations may need to be available in multiple forms eg on paper on a physical sign or online in order to meet these expectations and to be accessible to the american public,12,14
6996,6996,provide explanations as to how and why a decision was made or an action was taken by an automated systemtailored to the purpose,12,15
6997,6997,explanations should be tailored to the specific purpose for which the user is expected to use the explanation and should clearly state that purpose,12,14
6998,6998,an informational explanation might differ from an explanation provided to allow for the possibility of recourse an appeal or one provided in the context of a dispute or contestation process,12,14
6999,6999,for the purposes of this framework explanation should be construed broadly,12,14
7000,7000,an explanation need not be a plain language statement about causality but could consist of any mechanism that allows the recipient to build the necessary understanding and intuitions to achieve the stated purpose,12,14
7001,7001,tailoring should be assessed eg via user experience research,12,27
7002,7002,tailored to the target of the explanation,12,14
7003,7003,explanations should be targeted to specific audiences and clearly state that audience,12,14
7004,7004,an explanation provided to the subject of a decision might differ from one provided to an advocate or to a domain expert or decision maker,12,14
7005,7005,tailoring should be assessed eg via user experience research,12,27
7006,7006,tailored to the level of risk,12,3
7007,7007,an assessment should be done to determine the level of risk of the automated system,12,15
7008,7008,in settings where the consequences are high as determined by a risk assessment or extensive oversight is expected eg in criminal justice or some public sector settings explanatory mechanisms should be built into the system design so that the systems full behavior can be explained in advance ie only fully transparent models should be used rather than as an after the decision interpretation,12,17
7009,7009,in other settings the extent of explanation provided should be tailored to the risk level,12,14
7010,7010,valid,12,22
7011,7011,the explanation provided by a system should accurately reflect the factors and the influences that led to a particular decision and should be meaningful for the particular customization based on purpose target and level of risk,12,14
7012,7012,while approximation and simplification may be necessary for the system to succeed based on the explanatory purpose and target of the explanation or to account for the risk of fraud or other concerns related to revealing decision making information such simplifications should be done in a scientifically supportable way,12,14
7013,7013,where appropriate based on the explanatory system error ranges for the explanation should be calculated and included in the explanation with the choice of presentation of such information balanced with usability and overall interface complexity concerns,12,14
7014,7014,demonstrate protections for notice and explanationreporting,12,14
7015,7015,summary reporting should document the determinations made based on the above considerations including the responsible entities for accountability purposes the goal and use cases for the system identified users and impacted populations the assessment of notice clarity and timeliness the assessment of the explanations validity and accessibility the assessment of the level of risk and the account and assessment of how explanations are tailored including to the purpose the recipient of the explanation and the level of risk,12,14
7016,7016,individualized profile information should be made readily available to the greatest extent possible that includes explanations for any system impacts or inferences,12,14
7017,7017,reporting should be provided in a clear plain language and machine readable manner,12,24
7018,7018,real life examples of how these principles can become reality through laws policies and practical technical and sociotechnical approaches to protecting rights opportunities and access,12,12
7019,7019,people in illinois are given written notice by the private sector if their biometric information is used,12,12
7020,7020,the biometric information privacy act enacted by the state contains a number of provisions concerning the use of individual biometric data and identifiers,12,12
7021,7021,included among them is a provision that no private entity may collect capture purchase receive through trade or otherwise obtain such information about an individual unless written notice is provided to that individual or their legally appointed representative,12,1
7022,7022,major technology companies are piloting new ways to communicate with the public about their automated technologies,12,15
7023,7023,for example a collection of non profit organizations and companies have worked together to develop a framework that defines operational approaches to transparency for machine learning systems,12,26
7024,7024,this framework and others like it inform the public about the use of these tools going beyond simple notice to include reporting elements such as safety evaluations disparity assessments and explanations of how the systems work,12,15
7025,7025,lenders are required by federal law to notify consumers about certain decisions made about them,12,0
7026,7026,both the fair credit reporting act and the equal credit opportunity act require in certain circumstances that consumers who are denied credit receive adverse action notices,12,0
7027,7027,anyone who relies on the information in a credit report to deny a consumer credit must under the fair credit reporting act provide an adverse action notice to the consumer which includes notice of the reasons a creditor took adverse action on the application or on an existing credit account in addition under the risk based pricing rule lenders must either inform borrowers of their credit score or else tell consumers when they are getting worse terms because of information in their credit report the cfpb has also asserted that the law gives every applicant the right to a specific explanation if their application for credit was denied and that right is not diminished simply because a company uses a complex algorithm that it doesnt understand such explanations illustrate a shared value that certain decisions need to be explained,12,0
7028,7028,a california law requires that warehouse employees are provided with notice and explanation about quotas potentially facilitated by automated systems that apply to them,12,15
7029,7029,warehousing employers in california that use quota systems often facilitated by algorithmic monitoring systems are required to provide employees with a written description of each quota that applies to the employee including quantified number of tasks to be performed or materials to be produced or handled within the defined time period and any potential adverse employment action that could result from failure to meet the quotaacross the federal government agencies are conducting and supporting research on explainable ai systems,12,6
7030,7030,the nist is conducting fundamental research on the explainability of ai systems,12,14
7031,7031,a multidisciplinary team of researchers aims to develop measurement methods and best practices to support the implementation of core tenets of explainable ai,12,14
7032,7032,the defense advanced research projects agency has a program on explainable artificial intelligence that aims to create a suite of machine learning techniques that produce more explainable models while maintaining a high level of learning performance prediction accuracy and enable human users to understand appropriately trust and effectively manage the emerging generation of artificially intelligent partners,12,14
7033,7033,the national science foundations program on fairness in artificial intelligence also includes a specific interest in research foundations for explainable ai,12,6
7034,7034,human alternatives consideration and fallbackyou should be able to opt out where appropriate and have access to a person who can quickly consider and remedy problems you encounter,12,21
7035,7035,you should be able to opt out from automated systems in favor of a human alternative where appropriate,12,21
7036,7036,appropriateness should be determined based on reasonable expectations in a given context and with a focus on ensuring broad accessibility and protecting the public from especially harmful impacts,12,21
7037,7037,in some cases a human or other alternative may be required by law,12,12
7038,7038,you should have access to timely human consideration and remedy by a fallback and escalation process if an automated system fails it produces an error or you would like to appeal or contest its impacts on you,12,15
7039,7039,human consideration and fallback should be accessible equitable effective maintained accompanied by appropriate operator training and should not impose an unreasonable burden on the public,12,27
7040,7040,automated systems with an intended use within sensitive domains including but not limited to criminal justice employment education and health should additionally be tailored to the purpose provide meaningful access for oversight include training for any people interacting with the system and incorporate human consideration for adverse or high risk decisions,12,15
7041,7041,reporting that includes a description of these human governance processes and assessment of their timeliness accessibility outcomes and effectiveness should be made public whenever possible,12,15
7042,7042,this section provides a brief summary of the problems which the principle seeks to address and protect against including illustrative examples,12,14
7043,7043,there are many reasons people may prefer not to use an automated system the system can be flawed and can lead to unintended outcomes it may reinforce bias or be inaccessible it may simply be inconvenient or unavailable or it may replace a paper or manual process to which people had grown accustomed,12,15
7044,7044,yet members of the public are often presented with no alternative or are forced to endure a cumbersome process to reach a human decision maker once they decide they no longer want to deal exclusively with the automated system or be impacted by its results,12,15
7045,7045,as a result of this lack of human reconsideration many receive delayed access or lose access to rights opportunities benefits and critical services,12,22
7046,7046,the american public deserves the assurance that when rights opportunities or access are meaningfully at stake and there is a reasonable expectation of an alternative to an automated system they can conveniently opt out of an automated system and will not be disadvantaged for that choice,12,15
7047,7047,in some cases such a human or other alternative may be required by law for example it could be required as reasonable accommodations for people with disabilities,12,27
7048,7048,in addition to being able to opt out and use a human alternative the american public deserves a human fallback system in the event that an automated system fails or causes harm,12,15
7049,7049,no matter how rigorously an automated system is tested there will always be situations for which the system fails,12,15
7050,7050,the american public deserves protection via human review against these outlying or unexpected scenarios,12,12
7051,7051,in the case of time critical systems the public should not have to waitimmediate human consideration and fallback should be available,12,15
7052,7052,in many time critical systems such a remedy is already immediately available such as a building manager who can open a door in the case an automated card access system fails,12,15
7053,7053,in the criminal justice system employment education healthcare and other sensitive domains automated systems are used for many purposes from pre trial risk assessments and parole decisions to technologies that help doctors diagnose disease,12,15
7054,7054,absent appropriate safeguards these technologies can lead to unfair inaccurate or dangerous outcomes,12,15
7055,7055,these sensitive domains require extra protections,12,11
7056,7056,it is critically important that there is extensive human oversight in such settings,12,15
7057,7057,these critical protections have been adopted in some scenarios,12,11
7058,7058,where automated systems have been introduced to provide the public access to government benefits existing human paper and phone based processes are generally still in place providing an important alternative to ensure access,12,15
7059,7059,companies that have introduced automated call centers often retain the option of dialing zero to reach an operator,12,22
7060,7060,when automated identity controls are in place to board an airplane or enter the country there is a person supervising the systems who can be turned to for help or to appeal a misidentification,12,12
7061,7061,the american people deserve the reassurance that such procedures are in place to protect their rights opportunities and access,12,12
7062,7062,people make mistakes and a human alternative or fallback mechanism will not always have the right answer but they serve as an important check on the power and validity of automated systems,12,15
7063,7063,an automated signature matching system is used as part of the voting process in many parts of the country to determine whether the signature on a mail in ballot matches the signature on file,12,26
7064,7064,these signature matching systems are less likely to work correctly for some voters including voters with mental or physical disabilities voters with shorter or hyphenated names and voters who have changed their name,12,6
7065,7065,a human curing process which helps voters to confirm their signatures and correct other voting mistakes is important to ensure all votes are counted and it is already standard practice in much of the country for both an election official and the voter to have the opportunity to review and correct any such issues,12,22
7066,7066,an unemployment benefits system in colorado required as a condition of accessing benefits that applicants have a smartphone in order to verify their identity,12,12
7067,7067,no alternative human option was readily available which denied many people access to benefits,12,22
7068,7068,a fraud detection system for unemployment insurance distribution incorrectly flagged entries as fraudulent leading to people with slight discrepancies or complexities in their files having their wages withheld and tax returns seized without any chance to explain themselves or receive a review by a person,12,0
7069,7069,via patient was wrongly denied access to pain medication when the hospitals software confused her medication history with that of her dogs,12,22
7070,7070,even after she tracked down an explanation for the problem doctors were afraid to override the system and she was forced to go without pain relief due to the systems error,12,22
7071,7071,a large corporation automated performance evaluation and other hr functions leading to workers being fired by an automated system without the possibility of human review appeal or other form of recourse,12,15
7072,7072,an automated system should provide demonstrably effective mechanisms to opt out in favor of a human alternative where appropriate as well as timely human consideration and remedy by a fallback system with additional human oversight and safeguards for systems used in sensitive domains and with training and assessment for any human based portions of the system to ensure effectiveness,12,15
7073,7073,provide a mechanism to conveniently opt out from automated systems in favor of a human alternative where appropriatebrief clear accessible notice and instructions,12,21
7074,7074,those impacted by an automated system should be given a brief clear notice that they are entitled to opt out along with clear instructions for how to opt out,12,21
7075,7075,instructions should be provided in an accessible form and should be easily findable by those impacted by the automated system,12,15
7076,7076,the brevity clarity and accessibility of the notice and instructions should be assessed eg via user experience research,12,14
7077,7077,human alternatives provided when appropriate,12,27
7078,7078,in many scenarios there is a reasonable expectation of human involvement in attaining rights opportunities or access,12,12
7079,7079,when automated systems make up part of the attainment process alternative timely human driven processes should be provided,12,15
7080,7080,the use of a human alternative should be triggered by an opt out process,12,21
7081,7081,timely and not burdensome human alternative,12,22
7082,7082,opting out should be timely and not unreasonably burdensome in both the process of requesting to opt out and the human driven alternative provided,12,21
7083,7083,provide timely human consideration and remedy by a fallback and escalation system if an automated system fails produces error or you would like to appeal or contest its impacts on youproportionate,12,15
7084,7084,the availability of human consideration and fallback along with associated training and safeguards against human bias should be proportionate to the potential of the automated system to meaningfully impact rights opportunities or access,12,15
7085,7085,automated systems that have greater control over outcomes provide input to high stakes decisions relate to sensitive domains or otherwise have a greater potential to meaningfully impact rights opportunities or access should have greater availability eg staffing and oversight of human consideration and fallback mechanisms,12,15
7086,7086,accessible,12,14
7087,7087,mechanisms for human consideration and fallback whether in person on paper by phone or otherwise provided should be easy to find and use,12,21
7088,7088,these mechanisms should be tested to ensure that users who have trouble with the automated system are able to use human consideration and fallback with the understanding that it may be these users who are most likely to need the human assistance,12,15
7089,7089,similarly it should be tested to ensure that users with disabilities are able to find and use human consideration and fallback and also request reasonable accommodations or modifications,12,27
7090,7090,convenient,12,22
7091,7091,mechanisms for human consideration and fallback should not be unreasonably burdensome as compared to the automated systems equivalent,12,15
7092,7092,equitable,12,6
7093,7093,consideration should be given to ensuring outcomes of the fallback and escalation system are equitable when compared to those of the automated system and such that the fallback and escalation system provides equitable access to underserved communities,12,15
7094,7094,ixtimely,12,22
7095,7095,human consideration and fallback are only useful if they are conducted and concluded in a timely manner,12,21
7096,7096,the determination of what is timely should be made relative to the specific automated system and the review system should be staffed and regularly assessed to ensure it is providing timely consideration and fallback,12,15
7097,7097,in time critical systems this mechanism should be immediately available or where possible available before the harm occurs,12,22
7098,7098,time critical systems include but are not limited to voting related systems automated building access and other access systems systems that form a critical component of healthcare and systems that have the ability to withhold wages or otherwise cause immediate financial penalties,12,15
7099,7099,effective,12,22
7100,7100,the organizational structure surrounding processes for consideration and fallback should be designed so that if the human decision maker charged with reassessing a decision determines that it should be overruled the new decision will be effectively enacted,12,21
7101,7101,this includes ensuring that the new decision is entered into the automated system throughout its components any previous repercussions from the old decision are also overturned and safeguards are put in place to help ensure that future decisions do not result in the same errors,12,15
7102,7102,maintained,12,22
7103,7103,the human consideration and fallback process and any associated automated processes should be maintained and supported as long as the relevant automated system continues to be in use,12,15
7104,7104,institute training assessment and oversight to combat automation bias and ensure any human based components of a system are effective,12,15
7105,7105,training and assessment,12,27
7106,7106,anyone administering interacting with or interpreting the outputs of an automated system should receive training in that system including how to properly interpret outputs of a system in light of its intended purpose and in how to mitigate the effects of automation bias,12,15
7107,7107,the training should reoccur regularly to ensure it is up to date with the system and to ensure the system is used appropriately,12,13
7108,7108,assessment should be ongoing to ensure that the use of the system with human involvement provides for appropriate results ie that the involvement of people does not invalidate the systems assessment as safe and effective or lead to algorithmic discrimination,12,15
7109,7109,oversight,12,16
7110,7110,human based systems have the potential for bias including automation bias as well as other concerns that may limit their effectiveness,12,15
7111,7111,the results of assessments of the efficacy and potential bias of such human based systems should be overseen by governance structures that have the potential to update the operation of the human based system in order to mitigate these effects,12,15
7112,7112,implement additional human oversight and safeguards for automated systems related to sensitive domainsautomated systems used within sensitive domains including criminal justice employment education and health should meet the expectations laid out throughout this framework especially avoiding capricious inappropriate and discriminatory impacts of these technologies,12,15
7113,7113,additionally automated systems used within sensitive domains should meet these expectationsnarrowly scoped data and inferences,12,15
7114,7114,human oversight should ensure that automated systems in sensitive domains are  narrowly scoped to address a defined goal justifying each included data item or attribute as relevant to the specific use case,12,15
7115,7115,data included should be carefully limited to avoid algorithmic discrimination resulting from eg use of community characteristics social network analysis or group based inferences,12,6
7116,7116,tailored to the situation,12,27
7117,7117,human oversight should ensure that automated systems in sensitive domains are tailored to the specific use case and real world deployment scenario and evaluation testing should show that the system is safe and effective for that specific situation,12,15
7118,7118,validation testing performed based on one location or use case should not be assumed to transfer to another,12,18
7119,7119,human consideration before any high risk decision,12,21
7120,7120,automated systems where they are used in sensitive domains may play a role in directly providing information or otherwise providing positive outcomes to impacted people,12,15
7121,7121,however automated systems should not be allowed to directly intervene in high risk situations such as sentencing decisions or medical care without human consideration,12,15
7122,7122,meaningful access to examine the system,12,15
7123,7123,designers developers and deployers of automated systems should consider limited waivers of confidentiality including those related to trade secrets where necessary in order to provide meaningful oversight of systems used in sensitive domains incorporating measures to protect intellectual property and trade secrets from unwarranted disclosure as appropriate,12,11
7124,7124,this includes potentially private and protected meaningful access to source code documentation and related data during any associated legal discovery subject to effective confidentiality or court orders,12,11
7125,7125,such meaningful access should include but is not limited to adhering to the principle on notice and explanation using the highest level of risk so the system is designed with built in explanations such systems should use fully transparent models where the model itself can be understood by people needing to directly examine it,12,14
7126,7126,demonstrate access to human alternatives consideration and fallbackreporting,12,21
7127,7127,reporting should include an assessment of timeliness and the extent of additional burden for human alternatives aggregate statistics about who chooses the human alternative along with the results of the assessment about brevity clarity and accessibility of notice and opt out instructions,12,21
7128,7128,reporting on the accessibility timeliness and effectiveness of human consideration and fallback should be made public at regular intervals for as long as the system is in use,12,15
7129,7129,this should include aggregated information about the number and type of requests for consideration fallback employed and any repeated requests the timeliness of the handling of these requests including mean wait times for different types of requests as well as maximum wait times and information about the procedures used to address requests for consideration along with the results of the evaluation of their accessibility,12,24
7130,7130,for systems used in sensitive domains reporting should include information about training and governance procedures for these technologies,12,15
7131,7131,reporting should also include documentation of goals and assessment of meeting those goals consideration of data included and documentation of the governance of reasonable access to the technology,12,24
7132,7132,reporting should be provided in a clear and machine readable manner,12,24
7133,7133,real life examples of how these principles can become reality through laws policies and practical technical and sociotechnical approaches to protecting rights opportunities and access,12,12
7134,7134,healthcare navigators help people find their way through online signup forms to choose and obtain healthcare,12,27
7135,7135,a navigator is an individual or organization thats trained and able to help consumers small businesses and their employees as they look for health coverage options through the marketplace a government web site including completing eligibility and enrollment forms for the  plan year the biden harris administration increased funding so that grantee organizations could train and certify more than  navigators to help uninsured consumers find affordable and comprehensive health coveragethe customer service industry has successfully integrated automated services such as chatbots and ai driven call response systems with escalation to a human support team,12,27
7136,7136,many businesses now use partially automated customer service platforms that help answer customer questions and compile common problems for human agents to review,12,15
7137,7137,these integrated human ai systems allow companies to provide faster customer care while maintaining human agents to answer calls or otherwise respond to complicated requests,12,4
7138,7138,using both ai and human agents is viewed as key to successful customer service,12,4
7139,7139,ballot curing laws in at least  states require a fallback system that allows voters to correct their ballot and have it counted in the case that a voter signature matching algorithm incorrectly flags their ballot as invalid or there is another issue with their ballot and review by an election official does not rectify the problem,12,18
7140,7140,some federal courts have found that such cure procedures are constitutionally required,12,10
7141,7141,ballot curing processes vary among states and include direct phone calls emails or mail contact by election officials,12,1
7142,7142,xv voters are asked to provide alternative information or a new signature to verify the validity of their ballot,12,18
7143,7143,applying the blueprint for an ai bill of rightswhile many of the concerns addressed in this framework derive from the use of ai the technical capabilities and specific definitions of such systems change with the speed of innovation and the potential harms of their use occur even with less technologically sophisticated tools,12,2
7144,7144,thus this framework uses a two part test to determine what systems are in scope,12,26
7145,7145,this framework applies to  automated systems that  have the potential to meaningfully impact the american publics rights opportunities or access to critical resources or services,12,15
7146,7146,these rights opportunities and access to critical resources of services should be enjoyed equally and be fully protected regardless of the changing role that automated systems may play in our lives,12,15
7147,7147,this framework describes protections that should be applied with respect to all automated systems that have the potential to meaningfully impact individuals or communities exercise ofrights opportunities or accesscivil rights civil liberties and privacy including freedom of speech voting and protections from discrimination excessive punishment unlawful surveillance and violations of privacy and other freedoms in both public and private sector contextsequal opportunities including equitable access to education housing credit employment and other programs oraccess to critical resources or services such as healthcare financial services safety social services non deceptive information about goods and services and government benefits,12,15
7148,7148,a list of examples of automated systems for which these principles should be considered is provided in the appendix,12,15
7149,7149,the technical companion which follows offers supportive guidance for any person or entity that creates deploys or oversees automated systems,12,15
7150,7150,considered together the five principles and associated practices of the blueprint for an ai bill of rights form an overlapping set of backstops against potential harms,12,2
7151,7151,this purposefully overlapping framework when taken as a whole forms a blueprint to help protect the public from harm,12,12
7152,7152,the measures taken to realize the vision set forward in this framework should be proportionate with the extent and nature of the harm or risk of harm to peoples rights opportunities and access,12,12
