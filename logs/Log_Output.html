
                                            <h1>Chatbot log</h1>
                                            <p><strong>Date:</strong> 24 Apr 2023</p>
                                            <p><strong>Total tokens used (approx.):</strong> 14962</p>
                                            <p><strong>Total session cost (USD) (approx.):</strong> 0.02994</p>

                                           
                    <p style="text-align: left;"><strong>---------------------------------------------------------------------------------------------</strong></p>
                    <h2 style="text-align: left;"><strong>Conversation ID - ###</strong></h2>
                    <p style="text-align: left;"><strong>Original query:&nbsp;</strong>What does the future of AI regulation look like?</p>
                    <p style="text-align: left;"><strong>Formatted query powered by LLM:&nbsp;</strong>"What can we expect in terms of regulations for artificial intelligence in the future?"</p>
                    <p style="text-align: left;">&nbsp;</p>
                    <p style="text-align: left;"><strong>User input:</strong> "What can we expect in terms of regulations for artificial intelligence in the future?"</p>
                    <p style="text-align: left;"><strong>Chatbot response:&nbsp;</strong>It is difficult to provide a definitive answer about future regulations for AI as different countries are taking different approaches. However, it is clear that regulations for AI will depend on the specific policy initiatives and measures of each country to achieve regulatory compliance for AI to protect the public interest and support the healthy development of AI technologies. The UK government has established guidelines for ethical, fairness, and safety considerations in using AI but there is no mention of any specific future regulations related to AI in the context. The bank of England's discussion paper emphasizes the need for additional clarification of existing legal requirements and guidance in respect of AI in the UK financial services sector, as well as support for safe and responsible adoption of AI, but does not provide any definitive answer about future regulations for AI in this sector.</p>
                    <p style="text-align: left;"><strong>Total query and completion tokens used:&nbsp;</strong>7113</p>
                    <p style="text-align: left;"><strong>Total embedding tokens used:&nbsp;</strong>16</p>
                    <p style="text-align: left;"><strong>Total cost:&nbsp;</strong>0.0142324</p>
                    <p style="text-align: left;">&nbsp;</p>
                    <p style="text-align: left;"><strong>Internal conversation:</strong></p>
                    <p style="text-align: left;"><strong>LLM input:&nbsp;</strong>
    
                Question: What does the future of AI regulation look like? 
                

                We have the opportunity to rephrase the above question so that the resulting refined question is more 
                easily understandable by a large language model. 
                Based on this could you try to rephrase and refine the above question and return the new question? 
                If this is not possible return the original question.
                

                Return the new question in the following format:-
                "<the new question>"
    
    
                </p>
                    <p style="text-align: left;"><strong>LLM output:&nbsp;</strong>"What can we expect in terms of regulations for artificial intelligence in the future?"</p>
                    <p style="text-align: left;">&nbsp;</p>
                    <p style="text-align: left;"><strong>LLM input:&nbsp;</strong>Context information is below. 
---------------------
Document Name: ceimia

 a single regulatory document, with several government organisations publishing relevant documents. Overall, the main aims that can be inferred from various AI regulatory policies are to preserve national security and stability, protect the public interest and the interests of citizens qua consumers, and to stimulate the healthy development of AI technologies. The Ministry of Science and Technology has acted as the overarching coordinative body for governing AI, introducing voluntary principles and guidance on integrating ethics into the whole AI lifecycle. The Cyberspace Administration of China (CAC) has complemented this overarching soft law approach by releasing hard law measures. Like the initiatives proposed in the EU and Canada, these policies introduce specific prohibitions and legal requirements for AI (see Section 4.3.) However, unlike AI regulation in the EU and Canada which has sought to address AI technologies in general, the approach taken by the CAC has been more targeted at specific types of AI, such as recommender systems (2021) and generative algorithms (2022). Likewise, while the EU and Canada’s initiatives are primary legislation, the CAC’s regulatory initiatives are secondary legislation based on powers from primary data protection statutes, such as the Personal Information Protection Law (2021).

Alongside these AI-specific measures, the State Council – China’s chief administrative authority – has published Guiding Opinions on Strengthening Ethical Governance of Science and Technology (2022). While this document considers science and technology in general, it is indicative of further regulatory measures being introduced and review bodies established that apply to AI research and development. Accordingly, future regulatory policy in China will likely come from a combination of initiatives covering science and technology research in general, as well as measures more specifically focused on the development and use of AI technologies. In this section, we focus on risk as an umbrella concept that broadly captures a jurisdiction’s approach to dealing with future uncertainties related to the design, development, and deployment of AI systems. The approach to risk is a theme through which the differences and similarities between the jurisdictions’ approaches become more clear. AI harms, for example, vary by context, where they might be already addressed by particular sectoral laws. At the same time, several harms can readily be traced to a pattern of similar problems, and typically get characterised as risks or in terms of their impact. In this section, we analyse how risk is framed or defined in each jurisdiction’s approach to AI regulatory policy and how, if at all, a jurisdiction builds a framework for risk management.

The EU’s approach to risk frames regulation around different risk classifications, rather than a specific definition. In the draft AI Act, this is achieved by defining different thresholds for risk through an approach that features mostly horizontal, but also some vertical, components. In terms of horizontal components, it differentiates between unacceptable risk, high risk, limited risk, minimal, or no risk AI systems. Unacceptable risk systems are prohibited. They include systems for social scoring, the use of biometric identification in public spaces and subliminal techniques, as defined in Title II. High risk systems are permitted, subject to compliance with certain mandatory requirements and an ex-ante conformity assessment outlined in Title III. They include, among others, systems that predict a person’s risk of committing a crime or that automate hiring decisions, such as sorting resumes or CVs, as defined in Annex III. Limited risk systems are permitted subject to transparency obligations outlined in Title IV. They include systems for biometric categorization, emotion recognition and deep fake systems. Minimal or no risk systems include all other systems not covered by the draft AI Act safeguards and regulations.

The draft AI Act features a specific focus on high-risk systems. The high-risk classification of the draft depends on the function performed by the AI system as well as on the specific purpose and modalities for which that system is used. These would be assessed by outlining
---------------------
Given the context information and not prior knowledge, answer the question: "What can we expect in terms of regulations for artificial intelligence in the future?"
</p>
                    <p style="text-align: left;"><strong>LLM output:&nbsp;</strong>Future regulatory policy for artificial intelligence in China is likely to come from a combination of initiatives covering science and technology research in general, as well as measures specifically focused on the development and use of AI technologies. In the EU, regulation is framed around different risk classifications, with different thresholds for risk and specific mandatory requirements and conformity assessments for high-risk AI systems. Overall, the aim of AI regulatory policies is to preserve national security and stability, protect the public interest and the interests of citizens qua consumers, and to stimulate the healthy development of AI technologies.</p>
    
                    
                    <p style="text-align: left;">&nbsp;</p>
                    <p style="text-align: left;"><strong>LLM input:&nbsp;</strong>Human: "What can we expect in terms of regulations for artificial intelligence in the future?"
AI: Future regulatory policy for artificial intelligence in China is likely to come from a combination of initiatives covering science and technology research in general, as well as measures specifically focused on the development and use of AI technologies. In the EU, regulation is framed around different risk classifications, with different thresholds for risk and specific mandatory requirements and conformity assessments for high-risk AI systems. Overall, the aim of AI regulatory policies is to preserve national security and stability, protect the public interest and the interests of citizens qua consumers, and to stimulate the healthy development of AI technologies.
Human: We have the opportunity to refine the above answer (only if needed) with some more context below.
------------
Document Name: ceimia

 regulators to identify needed standards and conformity assessment tools in support of Canadian artificial intelligence interests and priorities. As a key component of the Directive on Automated Decision Making, there are varying degrees of compliance based on the impact of each system.

Evaluated through an Algorithmic Impact Assessment Tool (AIA), a component of the Directive, system deployers are required to assess their system using the AIA to determine the impact level of automated decision-making systems (ADMS) and follow the appropriate compliance requirements as outlined in Annex C of the Directive. There are four different levels of impact, from lower to higher. Each comes with its own requirements, varying with regards to the stringency of review, notice, human-in-the-loop, explanation, training documentation, contingency planning and approval for the system to operate.

Following a similar pattern, the proposed legislation, AIDA, will require the persons responsible for an artificial intelligence system to conduct an assessment on whether it is a high-impact system. This assessment ought to be conducted in accordance with further regulations which are yet to be defined. In the case of AIDA, as currently drafted, the Minister of Industry may, by order, require that the person responsible for the AI system: (a) conduct an audit with respect to the possible contravention; or (b) engage the services of an independent auditor to conduct the audit, if they have reasonable grounds to believe that the required sections of AIDA (Sections 6-14) have been violated, e.g. requirements on data anonymization, record keeping or on the assessment of high-impact systems). With regards to the Directive on Automated Decision-making, the Government of Canada retains the right to authorise external parties to review and audit proprietary software components used for automated decision-making systems, in accordance with the information required by the algorithmic impact assessment. China has taken a keen interest in developing technical standards for AI. In 2020, the Standardization Administration of China - the country’s main standards-setting body - issued a call for the development of a full range of standards for AI. In October 2021, the central government published a National Strategy for Technical Standards, which specifically included AI as an area to strengthen standardisation research. Traditionally, China has followed a largely state-led approach to the development of technical standards, with this strategy incentivising more industry participation in standards making. Several technical standards committees focused on aspects of AI have been established. Some of these endeavours have resulted in the development of standards; for instance, a standard for autonomous driving test scenarios initiated by China was formally accepted by the International Organisation for Standardisation (ISO). The Personal Information Protection Law requires an ex ante data protection impact assessment if personal information is handled or used for automated decision making. On top of this, regulatory provisions require that the service providers of recommender systems with certain properties must provide relevant regulators with information on the systems and an algorithm self-assessment report. However, the exact information required in these reports is currently unclear. The Ethical Norms for the New Generation Artificial Intelligence specifies that those researching and developing AI systems should gradually realise auditability. However, the manner in which this should be achieved is not elaborated. China’s public registry for recommender algorithms could also be seen as a type of audit, with the CAC able to review required documents, such as the aforementioned Algorithmic Self-Assessment. Finally, through Cybersecurity Reviews conducted by the CAC, it is possible that AI systems will be audited to ensure compliance with data protection regulations. The draft AI Act requires high-risk systems to be in compliance with harmonised standards as defined in Regulation (EU) No 1025/2012 of the European Parliament and of the Council, and, where the standardisation process is blocked or delayed, the Commission should be able to establish, via implementing acts, common specifications for certain requirements in the AI Act
------------
Given the new context, refine the original answer to better answer the question. If the context isn't useful, output the original answer again.</p>
                    <p style="text-align: left;"><strong>LLM output:&nbsp;</strong>Regulatory policies for artificial intelligence (AI) aim to protect national security and stimulate healthy development. Countries are taking different approaches to regulating AI. Canada's Directive on Automated Decision Making requires system deployers to assess the impact level of their automated decision-making systems (ADMS) and follow appropriate compliance requirements. The proposed legislation, AIDA, will require a high-impact AI system assessment. China is developing technical standards for AI and has established technical standards committees focused on AI. The Personal Information Protection Law requires an ex-ante data protection impact assessment if personal information is handled in automated decision-making. The Ethical Norms for the New Generation Artificial Intelligence specifies that AI systems should gradually realize auditability. The draft AI Act requires high-risk systems to comply with harmonized standards. In summary, regulations for AI will depend on the specific policy initiatives and measures of each country to achieve regulatory compliance for AI to protect the public interest and support the healthy development of AI technologies.</p>
    
                    
                    <p style="text-align: left;">&nbsp;</p>
                    <p style="text-align: left;"><strong>LLM input:&nbsp;</strong>Context information is below. 
---------------------
Document Name: UK Gov guide to using artificial intelligence in the public sector


Current applications of AI focus on performing narrowly defined tasks. AI generally cannot be imaginative.

perform well without a large quantity of relevant, high quality data.

infer additional context if the information is not present in the data.

Even if AI can help you meet some user needs, simpler solutions may be more effective and less expensive. For example, optical character recognition technology can extract information from scans of passports. However, a digital form requiring manual input might be more accurate, quicker to build, and cheaper. You’ll need to investigate alternative mature technology solutions thoroughly to check if this is the case.

With an AI project you should consider a number of factors, including AI ethics and safety. These factors span safety, ethical, legal and administrative concerns and include data quality - the success of your AI project depends on the quality of your data.

fairness - are the models trained and tested on relevant, accurate, and generalisable datasets and is the AI system deployed by users trained to implement them responsibly and without bias.

accountability - consider who is responsible for each element of the model’s output and how the designers and implementers of AI systems will be held accountable.

privacy - complying with appropriate data policies, for example the General Data Protection Regulations (GDPR) and the Data Protection Act 2018.

explainability and transparency - so the affected stakeholders can know how the AI model reached its decision.

costs - consider how much it will cost to build, run and maintain an AI infrastructure, train and educate staff and if the work to install AI may outweigh any potential savings.

You’ll need to make sure your AI system is compliant with General Data Protection Regulation (GDPR) and the Data Protection Act 2018 (DPA 2018), including the points which relate to automated decision making. We recommend discussing this with legal advisors.

Automated decisions in this context are decisions made without human intervention, which have legal or similarly significant effects on ‘data subjects’. For example, an online decision to award a loan, or a recruitment aptitude test which uses pre-programmed algorithms.

If you want to use automated processes to make decisions with legal or similarly significant effects on individuals you must follow the safeguards laid out in the GDPR and DPA 2018. This includes making sure you provide users with specific and easily accessible information about the automated decision-making process.

a simple way to obtain human intervention to review, and potentially change the decision. Remember to make sure your use of automated decision-making does not conflict with any other laws or regulations.

You should consider both the final decision and any automated decisions which significantly affected the decision-making process.
---------------------
Given the context information and not prior knowledge, answer the question: "What can we expect in terms of regulations for artificial intelligence in the future?"
</p>
                    <p style="text-align: left;"><strong>LLM output:&nbsp;</strong>The context information does not provide any clear indication of what to expect in terms of regulations for artificial intelligence in the future. The information focuses on current considerations and factors to be considered for AI projects, including ethics, safety, data quality, fairness, accountability, privacy, explainability and transparency, and costs. It also provides guidelines regarding automated decision-making processes in relation to legal or significant effects on individuals. However, it does not mention any specific future regulations related to AI.</p>
    
                    
                    <p style="text-align: left;">&nbsp;</p>
                    <p style="text-align: left;"><strong>LLM input:&nbsp;</strong>Human: "What can we expect in terms of regulations for artificial intelligence in the future?"
AI: The context information does not provide any clear indication of what to expect in terms of regulations for artificial intelligence in the future. The information focuses on current considerations and factors to be considered for AI projects, including ethics, safety, data quality, fairness, accountability, privacy, explainability and transparency, and costs. It also provides guidelines regarding automated decision-making processes in relation to legal or significant effects on individuals. However, it does not mention any specific future regulations related to AI.
Human: We have the opportunity to refine the above answer (only if needed) with some more context below.
------------
Document Name: UK Gov guide to using artificial intelligence in the public sector

AI has the potential to change the way we live and work. Embedding AI across all sectors has the potential to create thousands of jobs and drive economic growth. By one estimate, AI’s contribution to the United Kingdom could be as large as 5% of GDP by 2030.

A number of public sector organisations are already successfully using AI for tasks ranging from fraud detection to answering customer queries.

The potential uses for AI in the public sector are significant, but have to be balanced with ethical, fairness and safety considerations.

organisation leads who want to understand the best ways to use AI.

delivery leads who want to evaluate if AI can meet user needs. Recognising AI’s potential, the government’s Industrial Strategy White Paper placed Artificial Intelligence and Data as one of 4 Grand Challenges, supported by the £950m AI Sector Deal.

The government has set up 3 new bodies to support the use of AI, build the right infrastructure and facilitate public and private sector adoption of these technologies. These 3 new bodies are the AI Council which will be an expert committee providing high-level leadership on implementing the AI Sector Deal.

Office for AI which works with industry, academia and the third sector to coordinate and oversee the implementation of the UK’s AI strategy. Centre for Data Ethics and Innovation which identifies the measures needed to make sure the development of AI is safe, ethical and innovative. The government has also set up 2 funds to support the development and uptake of AI, the GovTech Catalyst to help public sector bodies take advantage of emerging technologies

Regulators’. Pioneer Fund to help regulators promote cutting-edge regulatory practices when developing emerging technologies. At its core, AI is a research field spanning philosophy, logic, statistics, computer science, mathematics, neuroscience, linguistics, cognitive psychology and economics.

AI can be defined as the use of digital technology to create systems capable of performing tasks commonly thought to require intelligence.

AI is constantly evolving, but generally it involves machines using statistics to find patterns in large amounts of data.

is the ability to perform repetitive tasks with data without the need for constant human guidance. This guidance mostly discusses machine learning. Machine learning is a subset of AI, and refers to the development of digital systems that improve their performance on a given task over time through experience.

Machine learning is the most widely-used form of AI, and has contributed to innovations like self-driving cars, speech recognition and machine translation. Recent advances in machine learning are the result of improvements to algorithms.

increases in funding.

huge growth in the amount of data created and stored by digital systems.

increased access to computational power and the expansion of cloud computing. Machine learning can be supervised learning which allows an AI model to learn from labelled training data, for example training an AI model to help tag content on GOV uk.

unsupervised learning which is training an AI algorithm to use unlabelled and unclassified information.

reinforcement learning which allows an AI model to learn as it performs a task.

AI can benefit the public sector in a number of ways. For example, it can provide more accurate information, forecasts and predictions leading to better outcomes - for example more accurate medical diagnoses.

produce a positive social impact by using AI to provide solutions for some of the world’s most challenging social problems.

simulate complex systems to experiment with different policy options and spot unintended consequences before committing to a measure.

improve public services - for example personalising public services to adapt to individual circumstances.

automate simple, manual tasks which frees staff up to do more interesting work.

AI is not a general purpose solution which can solve every problem.

Current applications of AI focus on performing narrowly defined tasks. AI generally cannot be imaginative.


------------
Given the new context, refine the original answer to better answer the question. If the context isn't useful, output the original answer again.</p>
                    <p style="text-align: left;"><strong>LLM output:&nbsp;</strong>The UK government has recognized the potential benefits of AI across all sectors, including the public sector, and has established guidelines for ethical, fairness, and safety considerations. While there are significant opportunities for AI in the public sector, it has to be balanced with these considerations. The government has set up three new bodies to support the use of AI and facilitate public and private sector adoption of these technologies, and has also set up two funds to support the development and uptake of AI. However, there is no mention of any specific future regulations related to AI in the context.</p>
    
                    
                    <p style="text-align: left;">&nbsp;</p>
                    <p style="text-align: left;"><strong>LLM input:&nbsp;</strong>Context information is below. 
---------------------
Document Name: bank of england discussion paper

, and financial stability. One of the most significant questions is whether AI can be managed through clarifications of the existing regulatory framework, or whether a new approach is needed. How to regulate AI to ensure it delivers in the best interests of consumers, firms, and markets is the subject of a wide-ranging debate, both here in the UK and in other jurisdictions around the world.

This Discussion Paper (DP) sits within the context of this wider debate and focuses on the regulation of AI in UK financial services. The Bank, PRA, and the FCA seek to encourage a broad-based and structured discussion with stakeholders on the challenges associated with the use and regulation of AI. We are keen to explore how best to address these issues in a way that is aligned with our statutory objectives, provides clarity, is actionable, and makes a practical difference for consumers, firms, and markets.

Beginning with our current regulatory framework, we have considered how key existing sectoral legal requirements and guidance in UK financial services apply to AI. This evaluation will allow us to consider which ones are most relevant, explore whether they are sufficient, and identify gaps. The DP considers how such legal requirements and guidance apply to the use of AI in UK financial services to support consumer protection, competition, the safety and soundness of individual firms, market integrity, and financial stability. How can policy mitigate AI risks while facilitating beneficial innovation? Is there a role for technical and, indeed, for global standards? If so, what?

Given the extent of overlaps within the existing sectoral rules, policies and principles in UK financial services that apply to AI, the supervisory authorities’ approach is largely limited to clarifying how the existing regulatory framework applies to AI and addressing any identified gaps in the regulatory framework. In particular, the supervisory authorities are interested in the additional challenges and risks that AI brings to firms’ decision-making and governance processes, and how those may be addressed through the Senior Managers and Certification Regime (SM&CR) and other existing regulatory tools.

Given the wide-ranging implications of AI, we are keen to hear from a broad range of stakeholders. This includes firms regulated by the Bank, PRA and/or FCA, as well as non-regulated financial services firms, professional services firms (such as accounting and auditing firms), law firms, third parties (such as technology companies), trade associations and industry bodies, standard setting organisations, academics, and civil society organisations.

We note the importance of building, maintaining, and reinforcing the trust of all stakeholders, including consumers in AI. Engagement between the public and private sectors will facilitate the creation of a regulatory framework that enables innovation and mitigates potential risks.

We hope that this DP contributes to this process and look forward to hearing from you.

Victoria Saporta

Executive Director,

Prudential Policy Directorate,

Bank of England

Sheldon Mills

Executive Director,

Consumers and Competition,

Financial Conduct Authority

cid:image003.png@01D8DA36.85FC6730

Jessica Rusu

Chief Data, Information and Intelligence Officer (CDIIO),

Financial Conduct Authority

Executive summary

Artificial intelligence (AI) and machine learning (ML) are rapidly developing technologies that have the potential to transform financial services. The promise of this technology is to make financial services and markets more efficient, accessible, and tailored to consumer needs. This may bring important benefits to consumers, financial services firms, financial markets, and the wider economy.

However, AI can pose novel challenges, as well as create new regulatory risks, or amplify existing ones. The Bank of England (the Bank), the Prudential Regulation Authority (PRA) and the Financial Conduct Authority (FCA) therefore have a close interest in the safe and responsible adoption
---------------------
Given the context information and not prior knowledge, answer the question: "What can we expect in terms of regulations for artificial intelligence in the future?"
</p>
                    <p style="text-align: left;"><strong>LLM output:&nbsp;</strong>The discussion paper does not provide a definitive answer to what we can expect in terms of regulations for artificial intelligence in the future. However, it emphasizes the need for a broad-based and structured discussion with stakeholders on the challenges associated with the use and regulation of AI in UK financial services, and seeks to explore how best to address these issues in a way that is aligned with statutory objectives, provides clarity, is actionable, and makes a practical difference for consumers, firms, and markets. The paper also considers how key existing sectoral legal requirements and guidance in UK financial services apply to AI and evaluates whether they are sufficient or identify gaps that need to be addressed. The supervisory authorities are interested in clarifying how the existing regulatory framework applies to AI and addressing any identified gaps in the regulatory framework, while mitigating AI risks and facilitating beneficial innovation. The paper notes the importance of building, maintaining, and reinforcing the trust of all stakeholders, including consumers, in AI and engaging between the public and private sectors to create a regulatory framework that enables innovation and mitigates potential risks.</p>
    
                    
                    <p style="text-align: left;">&nbsp;</p>
                    <p style="text-align: left;"><strong>LLM input:&nbsp;</strong>Human: "What can we expect in terms of regulations for artificial intelligence in the future?"
AI: The discussion paper does not provide a definitive answer to what we can expect in terms of regulations for artificial intelligence in the future. However, it emphasizes the need for a broad-based and structured discussion with stakeholders on the challenges associated with the use and regulation of AI in UK financial services, and seeks to explore how best to address these issues in a way that is aligned with statutory objectives, provides clarity, is actionable, and makes a practical difference for consumers, firms, and markets. The paper also considers how key existing sectoral legal requirements and guidance in UK financial services apply to AI and evaluates whether they are sufficient or identify gaps that need to be addressed. The supervisory authorities are interested in clarifying how the existing regulatory framework applies to AI and addressing any identified gaps in the regulatory framework, while mitigating AI risks and facilitating beneficial innovation. The paper notes the importance of building, maintaining, and reinforcing the trust of all stakeholders, including consumers, in AI and engaging between the public and private sectors to create a regulatory framework that enables innovation and mitigates potential risks.
Human: We have the opportunity to refine the above answer (only if needed) with some more context below.
------------
Document Name: bank of england discussion paper

 a specific area – the regulation of AI within UK financial services.

4.5 At the same time, previous work by the supervisory authorities (including the AIPPF and 2022 ML survey) have found that one of the challenges to the adoption of AI in UK financial services is the lack of clarity surrounding the current rules, regulations, and principles, in particular, how these apply to AI and what that means for firms at a practical level.

4.6 To help address this challenge, this chapter discusses some parts of the current regulatory framework that are considered by the supervisory authorities to be most relevant to the regulation of AI. The supervisory authorities are keen to gather feedback from stakeholders as to whether additional clarification of existing legal requirements and guidance in respect of AI may be helpful, if the current regulatory framework could benefit from extension to better encompass AI, and how the supervisory authorities may best support the safe and responsible adoption of AI in UK financial services.

Box 4: Proportionality and the supervisory authorities’ approach to regulation

One of the regulatory principles under the Financial Services and Markets Act 2000 (FSMA) that the PRA and the FCA must have regard to in discharging their general functions is ‘that a burden or restriction which is imposed on a person, or on the carrying on of an activity, should be proportionate to the benefits, considered in general terms, which are expected to result from the imposition of that burden or restriction’.footnote[6] This principle of proportionality informs the supervisory authorities’ thinking and approach to AI, including any potential future regulatory interventions.

Other financial services regulators and authorities have explicitly noted the importance of proportionality in relation to the regulation of AI. For example, the International Organization of Securities Commissions (IOSCO) guidance on the use of AI by market intermediaries and asset managers states the IOSCO members and firms should consider proportionality when implementing measures. It notes that firms and regulators should, in judging proportionality, consider the activity that is being undertaken, the complexity of the activity, risk profiles, the degree of autonomy of the AI applications, and the potential impact that the technology has on client outcomes and market integrity. Similarly, De Nederlandsche Bank (the central bank of the Netherlands) states that the applicability of their AI principles should be considered in light of the scale, complexity, and materiality of an organisation’s AI applications.

Consumer protection – FCA

4.7 The FCA’s approach to consumer protection is based on a combination of the FCA’s Principles for Businesses (the ‘Principles’), other high-level rules, and detailed rules, and guidance. These include Principles and rules contained in the FCA Handbook. The Principles are general statements of the fundamental obligations of firms and other persons to whom they apply, who are liable to disciplinary sanctions if they breach one or more of the Principles.

4.8 The FCA has recently introduced new rules to raise standards for firms dealing with retail customers: the Policy Statement 22/9 'A new Consumer Duty' (the ‘Consumer Duty’). These rules come into force for new and existing products and services that are open to sale or renewal on 31 July 2023. The rules come into force for ‘closed’ products and services on 31 July 2024.

4.9 The Consumer Duty includes a new Consumer Principle, which sets a higher standard than the existing Principles 6 and 7 in terms of how firms need to treat retail customers. The Consumer Duty requires firms to play a greater and more positive role in delivering good outcomes for consumers, including (where a firm can determine or materially influence outcomes) those who are not direct customers of the firm. The Consumer Principle requires firms to play a greater and more positive role in delivering good outcomes for retail customers, including those
------------
Given the new context, refine the original answer to better answer the question. If the context isn't useful, output the original answer again.</p>
                    <p style="text-align: left;"><strong>LLM output:&nbsp;</strong>The bank of England's discussion paper emphasizes the lack of clarity surrounding the current rules and regulations, as well as the principles governing the use of AI in UK financial services. The supervisory authorities are seeking feedback from stakeholders on whether additional clarification of the existing legal requirements and guidance in respect of AI may be helpful. They also consider how they can best support the safe and responsible adoption of AI. The principles of proportionality, informed by the thinking and approach to AI, influence regulatory interventions. The FCA's Principles, high-level rules, and detailed rules, and guidance, along with the recently introduced Consumer Duty, provide a framework for consumer protection, requiring firms to play a more positive role in delivering good outcomes for retail customers. However, the discussion paper does not provide any definitive answer about future regulations for AI in the UK financial services sector.</p>
    
                    
                    <p style="text-align: left;">&nbsp;</p>
                    <p style="text-align: left;"><strong>LLM input:&nbsp;</strong>Context information is below. 
---------------------
Regulatory policies for artificial intelligence (AI) aim to protect national security and stimulate healthy development. Countries are taking different approaches to regulating AI. Canada's Directive on Automated Decision Making requires system deployers to assess the impact level of their automated decision-making systems (ADMS) and follow appropriate compliance requirements. The proposed legislation, AIDA, will require a high-impact AI system assessment. China is developing technical standards for AI and has established technical standards committees focused on AI. The Personal Information Protection Law requires an ex-ante data protection impact assessment if personal information is handled in automated decision-making. The Ethical Norms for the New Generation Artificial Intelligence specifies that AI systems should gradually realize auditability. The draft AI Act requires high-risk systems to comply with harmonized standards. In summary, regulations for AI will depend on the specific policy initiatives and measures of each country to achieve regulatory compliance for AI to protect the public interest and support the healthy development of AI technologies.

The UK government has recognized the potential benefits of AI across all sectors, including the public sector, and has established guidelines for ethical, fairness, and safety considerations. While there are significant opportunities for AI in the public sector, it has to be balanced with these considerations. The government has set up three new bodies to support the use of AI and facilitate public and private sector adoption of these technologies, and has also set up two funds to support the development and uptake of AI. However, there is no mention of any specific future regulations related to AI in the context.

The bank of England's discussion paper emphasizes the lack of clarity surrounding the current rules and regulations, as well as the principles governing the use of AI in UK financial services. The supervisory authorities are seeking feedback from stakeholders on whether additional clarification of the existing legal requirements and guidance in respect of AI may be helpful. They also consider how they can best support the safe and responsible adoption of AI. The principles of proportionality, informed by the thinking and approach to AI, influence regulatory interventions. The FCA's Principles, high-level rules, and detailed rules, and guidance, along with the recently introduced Consumer Duty, provide a framework for consumer protection, requiring firms to play a more positive role in delivering good outcomes for retail customers. However, the discussion paper does not provide any definitive answer about future regulations for AI in the UK financial services sector.
---------------------
Given the context information and not prior knowledge, answer the question: "What can we expect in terms of regulations for artificial intelligence in the future?"
</p>
                    <p style="text-align: left;"><strong>LLM output:&nbsp;</strong>It is difficult to provide a definitive answer about future regulations for AI as different countries are taking different approaches. However, it is clear that regulations for AI will depend on the specific policy initiatives and measures of each country to achieve regulatory compliance for AI to protect the public interest and support the healthy development of AI technologies. The UK government has established guidelines for ethical, fairness, and safety considerations in using AI but there is no mention of any specific future regulations related to AI in the context. The bank of England's discussion paper emphasizes the need for additional clarification of existing legal requirements and guidance in respect of AI in the UK financial services sector, as well as support for safe and responsible adoption of AI, but does not provide any definitive answer about future regulations for AI in this sector.</p>
    
                    
                    <p style="text-align: left;">&nbsp;</p>
                    <p style="text-align: left;"><strong>---------------------------------------------------------------------------------------------</strong></p>
                    
                    <p style="text-align: left;"><strong>---------------------------------------------------------------------------------------------</strong></p>
                    <h2 style="text-align: left;"><strong>Conversation ID - ###</strong></h2>
                    <p style="text-align: left;"><strong>Original query:&nbsp;</strong>What does the future of AI regulation look like?</p>
                    <p style="text-align: left;"><strong>Formatted query powered by LLM:&nbsp;</strong>"What can we expect in terms of regulations for artificial intelligence in the future?"</p>
                    <p style="text-align: left;">&nbsp;</p>
                    <p style="text-align: left;"><strong>User input:</strong> "What can we expect in terms of regulations for artificial intelligence in the future?"</p>
                    <p style="text-align: left;"><strong>Chatbot response:&nbsp;</strong>It is important for policymakers and other key stakeholders to leverage contextual understanding to promote regulatory cooperation, coordination, and alignment in order to avoid a fragmented regulatory environment for artificial intelligence that could create barriers for interoperability and trade. There may be risks associated with introducing distinct or mutually-exclusive regulatory requirements, including hindering the development and deployment of AI technologies, reducing competitiveness in the global market, and limiting the potential benefits that AI can bring to society. Specific regulatory concerns highlighted include ethical and safety considerations in the public sector and risks relating to data, models, and governance in UK financial markets. It is important for the regulatory environment to be proportionate to manage and mitigate potential risks and harms that AI may have on consumers, firms, and the stability and integrity of the UK financial system and markets. The context information does not provide a clear answer about specific future regulations for artificial intelligence.</p>
                    <p style="text-align: left;"><strong>Total query and completion tokens used:&nbsp;</strong>7849</p>
                    <p style="text-align: left;"><strong>Total embedding tokens used:&nbsp;</strong>16</p>
                    <p style="text-align: left;"><strong>Total cost:&nbsp;</strong>0.0157044</p>
                    <p style="text-align: left;">&nbsp;</p>
                    <p style="text-align: left;"><strong>Internal conversation:</strong></p>
                    <p style="text-align: left;"><strong>LLM input:&nbsp;</strong>
    
                Question: What does the future of AI regulation look like? 
                

                We have the opportunity to rephrase the above question so that the resulting refined question is more 
                easily understandable by a large language model. 
                Based on this could you try to rephrase and refine the above question and return the new question? 
                If this is not possible return the original question.
                

                Return the new question in the following format:-
                "<the new question>"
    
    
                </p>
                    <p style="text-align: left;"><strong>LLM output:&nbsp;</strong>"What can we expect in terms of regulations for artificial intelligence in the future?"</p>
                    <p style="text-align: left;">&nbsp;</p>
                    <p style="text-align: left;"><strong>LLM input:&nbsp;</strong>Context information is below. 
---------------------
Document Name: ceimia

 a single regulatory document, with several government organisations publishing relevant documents. Overall, the main aims that can be inferred from various AI regulatory policies are to preserve national security and stability, protect the public interest and the interests of citizens qua consumers, and to stimulate the healthy development of AI technologies. The Ministry of Science and Technology has acted as the overarching coordinative body for governing AI, introducing voluntary principles and guidance on integrating ethics into the whole AI lifecycle. The Cyberspace Administration of China (CAC) has complemented this overarching soft law approach by releasing hard law measures. Like the initiatives proposed in the EU and Canada, these policies introduce specific prohibitions and legal requirements for AI (see Section 4.3.) However, unlike AI regulation in the EU and Canada which has sought to address AI technologies in general, the approach taken by the CAC has been more targeted at specific types of AI, such as recommender systems (2021) and generative algorithms (2022). Likewise, while the EU and Canada’s initiatives are primary legislation, the CAC’s regulatory initiatives are secondary legislation based on powers from primary data protection statutes, such as the Personal Information Protection Law (2021).

Alongside these AI-specific measures, the State Council – China’s chief administrative authority – has published Guiding Opinions on Strengthening Ethical Governance of Science and Technology (2022). While this document considers science and technology in general, it is indicative of further regulatory measures being introduced and review bodies established that apply to AI research and development. Accordingly, future regulatory policy in China will likely come from a combination of initiatives covering science and technology research in general, as well as measures more specifically focused on the development and use of AI technologies. In this section, we focus on risk as an umbrella concept that broadly captures a jurisdiction’s approach to dealing with future uncertainties related to the design, development, and deployment of AI systems. The approach to risk is a theme through which the differences and similarities between the jurisdictions’ approaches become more clear. AI harms, for example, vary by context, where they might be already addressed by particular sectoral laws. At the same time, several harms can readily be traced to a pattern of similar problems, and typically get characterised as risks or in terms of their impact. In this section, we analyse how risk is framed or defined in each jurisdiction’s approach to AI regulatory policy and how, if at all, a jurisdiction builds a framework for risk management.

The EU’s approach to risk frames regulation around different risk classifications, rather than a specific definition. In the draft AI Act, this is achieved by defining different thresholds for risk through an approach that features mostly horizontal, but also some vertical, components. In terms of horizontal components, it differentiates between unacceptable risk, high risk, limited risk, minimal, or no risk AI systems. Unacceptable risk systems are prohibited. They include systems for social scoring, the use of biometric identification in public spaces and subliminal techniques, as defined in Title II. High risk systems are permitted, subject to compliance with certain mandatory requirements and an ex-ante conformity assessment outlined in Title III. They include, among others, systems that predict a person’s risk of committing a crime or that automate hiring decisions, such as sorting resumes or CVs, as defined in Annex III. Limited risk systems are permitted subject to transparency obligations outlined in Title IV. They include systems for biometric categorization, emotion recognition and deep fake systems. Minimal or no risk systems include all other systems not covered by the draft AI Act safeguards and regulations.

The draft AI Act features a specific focus on high-risk systems. The high-risk classification of the draft depends on the function performed by the AI system as well as on the specific purpose and modalities for which that system is used. These would be assessed by outlining
---------------------
Given the context information and not prior knowledge, answer the question: What are some of the potential risks associated with a fragmented regulatory environment for artificial intelligence?
</p>
                    <p style="text-align: left;"><strong>LLM output:&nbsp;</strong>There is no information provided about potential risks associated with a fragmented regulatory environment for artificial intelligence.</p>
    
                    
                    <p style="text-align: left;">&nbsp;</p>
                    <p style="text-align: left;"><strong>LLM input:&nbsp;</strong>Human: What are some of the potential risks associated with a fragmented regulatory environment for artificial intelligence?
AI: There is no information provided about potential risks associated with a fragmented regulatory environment for artificial intelligence.
Human: We have the opportunity to refine the above answer (only if needed) with some more context below.
------------
Document Name: ceimia

 of comparable thresholds to mitigate AI risks and their potential impact for the EU and Canada. Yet, several key differences can be seen, even between the seemingly similar approaches outlined above. For instance, Canada’s focus on “impact” rather than “risk” is closer to the UK and USA’s emphasis on actual impact rather than hypothetical risk that AI technologies can have on individuals and groups. Additionally, although the USA and China have both taken more decentralised approaches to AI regulatory policy than the EU or Canada, China’s Cyberspace Administration has introduced hard law initiatives for specific AI technologies, while the USA’s federal approach has largely relied on voluntary measures or guidance on applying existing sectoral legislation.

While differences in AI regulatory policies are understandable and expected, given the different aspirations and governance institutions of each jurisdiction, some types of divergence could bring about negative outcomes. In particular, a fragmented regulatory environment that lacks a high degree of mutual recognition could create barriers for interoperability and trade. Although it is too early to assess the likelihood of this type of landscape emerging for AI regulatory policy, it is a plausible outcome if distinct or mutually-exclusive regulatory requirements are introduced.29 Given this, as the efforts to introduce AI regulatory policies progress, it is vital that stakeholders understand the similarities and differences between governments’ approaches, so that they are able to reasonably assess the possibility of fragmentation and promote deeper cooperation. This report, and our subsequent publications in this project, will aid stakeholders in having this contextualised understanding of AI regulatory policy.30 As AI regulatory policy continues to mature, it is crucial that policymakers and other key stakeholders leverage this contextual understanding to promote regulatory cooperation, coordination, and where appropriate alignment. We filtered AI governance documents for relevance based on the following criteria:

Inclusion criteria: hard or soft law initiatives that are designed to govern AI technologies31, which have been drafted or published by a national-level

government institution32, including:

Hard law introduced by national-level government institutions, inclusive

of both primary and secondary legislation (e.g. the EU AI Act, China’s Provisions on the Administration of Deep Synthesis Internet Information Services);

Soft law by national-level government institutions (e.g. UK’s Algorithmic Transparency Recording Standard or USA’ NIST AI Risk Management Framework).

Exclusion criteria: governance initiatives that do not include a hard or soft regulatory element, that are drafted by sub-national or non-governmental bodies, or that do not specifically focus on AI in any part of the document. This will include:

Strategies that don’t specify AI governance approach33 (e.g. UK National AI Strategy);

State or municipal government initiatives (e.g. New York’s AI Audit Law);

Legislative documents which do not specifically relate to AI, even if they

are used to enforce protections (e.g. FTC Act).

Document focused on regulatory policies for AI in the defence sector.

We selected the above inclusion/exclusion criteria to keep the document analysis manageable within the timeframe of this project, while not excluding any key documents. However, we acknowledge that this inclusion/exclusion criteria still creates some issues, for instance,

The chances of some draft regulations being passed is higher than others (e.g. the EU AI Act vs. the USA’s Algorithmic Accountability Act);

Some documents which do not explicitly mention AI technologies are necessary for understanding a jurisdiction’s approach to governing these technologies.

To overcome these issues, we considered the wider context of the documents in our analysis; for instance, by specifying that some legislation are more likely to pass than others and linking specific regulatory documents that act as reference points for a country’s approach to regulatory policy.
------------
Given the new context, refine the original answer to better answer the question. If the context isn't useful, output the original answer again.</p>
                    <p style="text-align: left;"><strong>LLM output:&nbsp;</strong>A fragmented regulatory environment for artificial intelligence could create barriers for interoperability and trade, leading to negative outcomes. If distinct or mutually-exclusive regulatory requirements are introduced, it could result in a landscape that lacks a high degree of mutual recognition. This could lead to potential risks such as hindering the development and deployment of AI technologies, reducing competitiveness in the global market, and limiting the potential benefits that AI can bring to society. Therefore, it is crucial for policymakers and other key stakeholders to leverage contextual understanding to promote regulatory cooperation, coordination, and alignment.</p>
    
                    
                    <p style="text-align: left;">&nbsp;</p>
                    <p style="text-align: left;"><strong>LLM input:&nbsp;</strong>Context information is below. 
---------------------
Document Name: UK Gov guide to using artificial intelligence in the public sector


Current applications of AI focus on performing narrowly defined tasks. AI generally cannot be imaginative.

perform well without a large quantity of relevant, high quality data.

infer additional context if the information is not present in the data.

Even if AI can help you meet some user needs, simpler solutions may be more effective and less expensive. For example, optical character recognition technology can extract information from scans of passports. However, a digital form requiring manual input might be more accurate, quicker to build, and cheaper. You’ll need to investigate alternative mature technology solutions thoroughly to check if this is the case.

With an AI project you should consider a number of factors, including AI ethics and safety. These factors span safety, ethical, legal and administrative concerns and include data quality - the success of your AI project depends on the quality of your data.

fairness - are the models trained and tested on relevant, accurate, and generalisable datasets and is the AI system deployed by users trained to implement them responsibly and without bias.

accountability - consider who is responsible for each element of the model’s output and how the designers and implementers of AI systems will be held accountable.

privacy - complying with appropriate data policies, for example the General Data Protection Regulations (GDPR) and the Data Protection Act 2018.

explainability and transparency - so the affected stakeholders can know how the AI model reached its decision.

costs - consider how much it will cost to build, run and maintain an AI infrastructure, train and educate staff and if the work to install AI may outweigh any potential savings.

You’ll need to make sure your AI system is compliant with General Data Protection Regulation (GDPR) and the Data Protection Act 2018 (DPA 2018), including the points which relate to automated decision making. We recommend discussing this with legal advisors.

Automated decisions in this context are decisions made without human intervention, which have legal or similarly significant effects on ‘data subjects’. For example, an online decision to award a loan, or a recruitment aptitude test which uses pre-programmed algorithms.

If you want to use automated processes to make decisions with legal or similarly significant effects on individuals you must follow the safeguards laid out in the GDPR and DPA 2018. This includes making sure you provide users with specific and easily accessible information about the automated decision-making process.

a simple way to obtain human intervention to review, and potentially change the decision. Remember to make sure your use of automated decision-making does not conflict with any other laws or regulations.

You should consider both the final decision and any automated decisions which significantly affected the decision-making process.
---------------------
Given the context information and not prior knowledge, answer the question: What ethical and safety concerns are highlighted in the UK Gov guide to using artificial intelligence in the public sector?
</p>
                    <p style="text-align: left;"><strong>LLM output:&nbsp;</strong>The UK Gov guide highlights a range of ethical and safety concerns related to the use of artificial intelligence in the public sector, including data quality, fairness, accountability, privacy, explainability and transparency, as well as potential costs. The guide also emphasizes the need to comply with GDPR and DPA 2018 when using automated decision-making processes, and to provide users with accessible information about the decision-making process, as well as a way to obtain human intervention to review and potentially change the decision.</p>
    
                    
                    <p style="text-align: left;">&nbsp;</p>
                    <p style="text-align: left;"><strong>LLM input:&nbsp;</strong>Human: What ethical and safety concerns are highlighted in the UK Gov guide to using artificial intelligence in the public sector?
AI: The UK Gov guide highlights a range of ethical and safety concerns related to the use of artificial intelligence in the public sector, including data quality, fairness, accountability, privacy, explainability and transparency, as well as potential costs. The guide also emphasizes the need to comply with GDPR and DPA 2018 when using automated decision-making processes, and to provide users with accessible information about the decision-making process, as well as a way to obtain human intervention to review and potentially change the decision.
Human: We have the opportunity to refine the above answer (only if needed) with some more context below.
------------
Document Name: UK Gov guide to using artificial intelligence in the public sector

AI has the potential to change the way we live and work. Embedding AI across all sectors has the potential to create thousands of jobs and drive economic growth. By one estimate, AI’s contribution to the United Kingdom could be as large as 5% of GDP by 2030.

A number of public sector organisations are already successfully using AI for tasks ranging from fraud detection to answering customer queries.

The potential uses for AI in the public sector are significant, but have to be balanced with ethical, fairness and safety considerations.

organisation leads who want to understand the best ways to use AI.

delivery leads who want to evaluate if AI can meet user needs. Recognising AI’s potential, the government’s Industrial Strategy White Paper placed Artificial Intelligence and Data as one of 4 Grand Challenges, supported by the £950m AI Sector Deal.

The government has set up 3 new bodies to support the use of AI, build the right infrastructure and facilitate public and private sector adoption of these technologies. These 3 new bodies are the AI Council which will be an expert committee providing high-level leadership on implementing the AI Sector Deal.

Office for AI which works with industry, academia and the third sector to coordinate and oversee the implementation of the UK’s AI strategy. Centre for Data Ethics and Innovation which identifies the measures needed to make sure the development of AI is safe, ethical and innovative. The government has also set up 2 funds to support the development and uptake of AI, the GovTech Catalyst to help public sector bodies take advantage of emerging technologies

Regulators’. Pioneer Fund to help regulators promote cutting-edge regulatory practices when developing emerging technologies. At its core, AI is a research field spanning philosophy, logic, statistics, computer science, mathematics, neuroscience, linguistics, cognitive psychology and economics.

AI can be defined as the use of digital technology to create systems capable of performing tasks commonly thought to require intelligence.

AI is constantly evolving, but generally it involves machines using statistics to find patterns in large amounts of data.

is the ability to perform repetitive tasks with data without the need for constant human guidance. This guidance mostly discusses machine learning. Machine learning is a subset of AI, and refers to the development of digital systems that improve their performance on a given task over time through experience.

Machine learning is the most widely-used form of AI, and has contributed to innovations like self-driving cars, speech recognition and machine translation. Recent advances in machine learning are the result of improvements to algorithms.

increases in funding.

huge growth in the amount of data created and stored by digital systems.

increased access to computational power and the expansion of cloud computing. Machine learning can be supervised learning which allows an AI model to learn from labelled training data, for example training an AI model to help tag content on GOV uk.

unsupervised learning which is training an AI algorithm to use unlabelled and unclassified information.

reinforcement learning which allows an AI model to learn as it performs a task.

AI can benefit the public sector in a number of ways. For example, it can provide more accurate information, forecasts and predictions leading to better outcomes - for example more accurate medical diagnoses.

produce a positive social impact by using AI to provide solutions for some of the world’s most challenging social problems.

simulate complex systems to experiment with different policy options and spot unintended consequences before committing to a measure.

improve public services - for example personalising public services to adapt to individual circumstances.

automate simple, manual tasks which frees staff up to do more interesting work.

AI is not a general purpose solution which can solve every problem.

Current applications of AI focus on performing narrowly defined tasks. AI generally cannot be imaginative.


------------
Given the new context, refine the original answer to better answer the question. If the context isn't useful, output the original answer again.</p>
                    <p style="text-align: left;"><strong>LLM output:&nbsp;</strong>The UK Gov guide highlights a range of ethical and safety concerns related to the use of artificial intelligence in the public sector. The guide emphasizes the need to balance the potential uses of AI with ethical, fairness and safety considerations. These include data quality, fairness, accountability, privacy, explainability, and transparency, as well as potential costs. The guide also emphasizes the need to comply with GDPR and DPA 2018 when using automated decision-making processes, and to provide users with accessible information about the decision-making process, as well as a way to obtain human intervention to review and potentially change the decision. Additionally, the document discusses the government's establishment of new bodies to support the use of AI, build the right infrastructure and facilitate public and private sector adoption of these technologies, and has set up 2 funds to support the development and uptake of AI.</p>
    
                    
                    <p style="text-align: left;">&nbsp;</p>
                    <p style="text-align: left;"><strong>LLM input:&nbsp;</strong>Context information is below. 
---------------------
Document Name: bank of england discussion paper

, and financial stability. One of the most significant questions is whether AI can be managed through clarifications of the existing regulatory framework, or whether a new approach is needed. How to regulate AI to ensure it delivers in the best interests of consumers, firms, and markets is the subject of a wide-ranging debate, both here in the UK and in other jurisdictions around the world.

This Discussion Paper (DP) sits within the context of this wider debate and focuses on the regulation of AI in UK financial services. The Bank, PRA, and the FCA seek to encourage a broad-based and structured discussion with stakeholders on the challenges associated with the use and regulation of AI. We are keen to explore how best to address these issues in a way that is aligned with our statutory objectives, provides clarity, is actionable, and makes a practical difference for consumers, firms, and markets.

Beginning with our current regulatory framework, we have considered how key existing sectoral legal requirements and guidance in UK financial services apply to AI. This evaluation will allow us to consider which ones are most relevant, explore whether they are sufficient, and identify gaps. The DP considers how such legal requirements and guidance apply to the use of AI in UK financial services to support consumer protection, competition, the safety and soundness of individual firms, market integrity, and financial stability. How can policy mitigate AI risks while facilitating beneficial innovation? Is there a role for technical and, indeed, for global standards? If so, what?

Given the extent of overlaps within the existing sectoral rules, policies and principles in UK financial services that apply to AI, the supervisory authorities’ approach is largely limited to clarifying how the existing regulatory framework applies to AI and addressing any identified gaps in the regulatory framework. In particular, the supervisory authorities are interested in the additional challenges and risks that AI brings to firms’ decision-making and governance processes, and how those may be addressed through the Senior Managers and Certification Regime (SM&CR) and other existing regulatory tools.

Given the wide-ranging implications of AI, we are keen to hear from a broad range of stakeholders. This includes firms regulated by the Bank, PRA and/or FCA, as well as non-regulated financial services firms, professional services firms (such as accounting and auditing firms), law firms, third parties (such as technology companies), trade associations and industry bodies, standard setting organisations, academics, and civil society organisations.

We note the importance of building, maintaining, and reinforcing the trust of all stakeholders, including consumers in AI. Engagement between the public and private sectors will facilitate the creation of a regulatory framework that enables innovation and mitigates potential risks.

We hope that this DP contributes to this process and look forward to hearing from you.

Victoria Saporta

Executive Director,

Prudential Policy Directorate,

Bank of England

Sheldon Mills

Executive Director,

Consumers and Competition,

Financial Conduct Authority

cid:image003.png@01D8DA36.85FC6730

Jessica Rusu

Chief Data, Information and Intelligence Officer (CDIIO),

Financial Conduct Authority

Executive summary

Artificial intelligence (AI) and machine learning (ML) are rapidly developing technologies that have the potential to transform financial services. The promise of this technology is to make financial services and markets more efficient, accessible, and tailored to consumer needs. This may bring important benefits to consumers, financial services firms, financial markets, and the wider economy.

However, AI can pose novel challenges, as well as create new regulatory risks, or amplify existing ones. The Bank of England (the Bank), the Prudential Regulation Authority (PRA) and the Financial Conduct Authority (FCA) therefore have a close interest in the safe and responsible adoption
---------------------
Given the context information and not prior knowledge, answer the question: What are some potential risks associated with the use of artificial intelligence in UK financial markets, according to the Bank of England's discussion paper?
</p>
                    <p style="text-align: left;"><strong>LLM output:&nbsp;</strong>The use of artificial intelligence in UK financial markets can pose novel challenges and create new regulatory risks or amplify existing ones, according to the Bank of England's discussion paper. The potential risks are not specified in detail in the given context.</p>
    
                    
                    <p style="text-align: left;">&nbsp;</p>
                    <p style="text-align: left;"><strong>LLM input:&nbsp;</strong>Human: What are some potential risks associated with the use of artificial intelligence in UK financial markets, according to the Bank of England's discussion paper?
AI: The use of artificial intelligence in UK financial markets can pose novel challenges and create new regulatory risks or amplify existing ones, according to the Bank of England's discussion paper. The potential risks are not specified in detail in the given context.
Human: We have the opportunity to refine the above answer (only if needed) with some more context below.
------------
Document Name: bank of england discussion paper

 and the Financial Conduct Authority (FCA) therefore have a close interest in the safe and responsible adoption of AI in UK financial services, including considering how policy and regulation can best support this.

The supervisory authorities are publishing this DP to further our understanding and to deepen dialogue on how AI may affect our respective objectives. This is part of the supervisory authorities’ wider programme of work related to AI, including the AI Public Private Forum, the final report of which was published in February 2022. This DP should also be considered within the context of the evolving wider national and international policy debate on AI, including the UK government’s policy paper ‘Establishing a pro-innovation approach to regulating AI’Opens in a new window, joint working between UK regulators through the Digital Regulation Cooperation Forum (DRCF), and international developments from other regulators and authorities, such as the proposed AI regulation for the EU.

Benefits and risks related to the use of AI in financial services

AI offers potential benefits for consumers, businesses, and markets. However, AI also has the potential to create new or increased risks and challenges. The benefits, risks, and harms discussed in this DP are neither exhaustive nor applicable to every AI use case.

The primary drivers of AI risk in financial services relate to three key stages of the AI lifecycle: (i) data; (ii) models; and (iii) governance. Interconnected risks at the data level can feed into the model level, and then raise broader challenges at the level of the firm and its overall governance of AI systems. Depending on how AI is used in financial services, issues at each of the three stages (data, models, and governance) can result in a range of outcomes and risks that are relevant to the supervisory authorities’ remits.

Consumers

AI may benefit consumers in important ways – from improved outcomes through more effective matching to products and services, to an enhanced ability to identify and support consumers with characteristics of vulnerability, as well as increasing financial access. However, if misused, these technologies may potentially lead to harmful targeting of consumers’ behavioural biases or characteristics of vulnerability, discriminatory decisions, financial exclusion, and reduced trust.

Competition

There may be substantial benefits to competition from the use of AI in financial services, where these technologies may enable consumers to access, assess, and act on information more effectively. But risks to competition may also arise where AI is used to implement or facilitate further harmful strategic behaviour such as collusion, or creating or exacerbating market features that hinder competition, such as barriers to entry or to leverage a dominant position.

Firms

There are also many potential benefits for financial services firms including enhanced data and analytical insights, increased revenue generation, increased operational efficiency and productivity, enhanced risk management and controls, and better combatting of fraud and money laundering. Equally, the use of AI can translate into a range of prudential risks to the safety and soundness of firms, which may differ depending on how the technology is used by firms.

Financial markets

AI may benefit the broader financial system and markets in general through more responsive pricing and more accurate decision-making, which can, in turn, lead to increased allocative efficiency. However, AI may also lead to risks to system resilience and efficiency. For example, models may become correlated in subtle ways and add to risks of herding, or procyclical behaviour at times of market stress.

How existing legal requirements and guidance apply to the use of AI

In line with their statutory objectives and to support the safe and responsible adoption of AI in UK financial services, the supervisory authorities may need to intervene further to manage and mitigate the potential risks and harms AI may have on consumers, firms, and the stability and integrity of the UK financial system and markets.

It is important that the regulatory environment is proportion
------------
Given the new context, refine the original answer to better answer the question. If the context isn't useful, output the original answer again.</p>
                    <p style="text-align: left;"><strong>LLM output:&nbsp;</strong>The Bank of England's discussion paper highlights that while the use of artificial intelligence in UK financial markets offers potential benefits for consumers, businesses, and markets, it can also create new or increased risks and challenges that are not applicable to every AI use case. The risks associated with AI in financial services relate to data, models, and governance, which can result in a range of outcomes and risks such as harmful targeting of consumers' biases or vulnerabilities, discriminatory decisions, financial exclusion, and reduced trust. The use of AI may also pose risks to competition, create prudential risks to the safety and soundness of firms, and pose risks to system resilience and efficiency. The regulatory environment needs to be proportionate to manage and mitigate potential risks and harms that AI may have on consumers, firms, and the stability and integrity of the UK financial system and markets.</p>
    
                    
                    <p style="text-align: left;">&nbsp;</p>
                    <p style="text-align: left;"><strong>LLM input:&nbsp;</strong>Context information is below. 
---------------------
A fragmented regulatory environment for artificial intelligence could create barriers for interoperability and trade, leading to negative outcomes. If distinct or mutually-exclusive regulatory requirements are introduced, it could result in a landscape that lacks a high degree of mutual recognition. This could lead to potential risks such as hindering the development and deployment of AI technologies, reducing competitiveness in the global market, and limiting the potential benefits that AI can bring to society. Therefore, it is crucial for policymakers and other key stakeholders to leverage contextual understanding to promote regulatory cooperation, coordination, and alignment.

The UK Gov guide highlights a range of ethical and safety concerns related to the use of artificial intelligence in the public sector. The guide emphasizes the need to balance the potential uses of AI with ethical, fairness and safety considerations. These include data quality, fairness, accountability, privacy, explainability, and transparency, as well as potential costs. The guide also emphasizes the need to comply with GDPR and DPA 2018 when using automated decision-making processes, and to provide users with accessible information about the decision-making process, as well as a way to obtain human intervention to review and potentially change the decision. Additionally, the document discusses the government's establishment of new bodies to support the use of AI, build the right infrastructure and facilitate public and private sector adoption of these technologies, and has set up 2 funds to support the development and uptake of AI.

The Bank of England's discussion paper highlights that while the use of artificial intelligence in UK financial markets offers potential benefits for consumers, businesses, and markets, it can also create new or increased risks and challenges that are not applicable to every AI use case. The risks associated with AI in financial services relate to data, models, and governance, which can result in a range of outcomes and risks such as harmful targeting of consumers' biases or vulnerabilities, discriminatory decisions, financial exclusion, and reduced trust. The use of AI may also pose risks to competition, create prudential risks to the safety and soundness of firms, and pose risks to system resilience and efficiency. The regulatory environment needs to be proportionate to manage and mitigate potential risks and harms that AI may have on consumers, firms, and the stability and integrity of the UK financial system and markets.
---------------------
Given the context information and not prior knowledge, answer the question: "What can we expect in terms of regulations for artificial intelligence in the future?"
</p>
                    <p style="text-align: left;"><strong>LLM output:&nbsp;</strong>It is important for policymakers and other key stakeholders to leverage contextual understanding to promote regulatory cooperation, coordination, and alignment in order to avoid a fragmented regulatory environment for artificial intelligence that could create barriers for interoperability and trade. There may be risks associated with introducing distinct or mutually-exclusive regulatory requirements, including hindering the development and deployment of AI technologies, reducing competitiveness in the global market, and limiting the potential benefits that AI can bring to society. Specific regulatory concerns highlighted include ethical and safety considerations in the public sector and risks relating to data, models, and governance in UK financial markets. It is important for the regulatory environment to be proportionate to manage and mitigate potential risks and harms that AI may have on consumers, firms, and the stability and integrity of the UK financial system and markets. The context information does not provide a clear answer about specific future regulations for artificial intelligence.</p>
    
                    
                    <p style="text-align: left;">&nbsp;</p>
                    <p style="text-align: left;"><strong>---------------------------------------------------------------------------------------------</strong></p>
                    